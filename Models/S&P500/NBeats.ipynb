{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Prerequisite </h2>\n",
    "- pip install torch    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b96be4f5deceda"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T10:15:06.537504Z",
     "start_time": "2024-03-26T10:15:05.609057Z"
    }
   },
   "id": "45f8cd0c6bac992a",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/keremsmacbook/DataspellProjects/MultivariateForecasting/Data/SP500_Cleaned.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T10:15:07.227711Z",
     "start_time": "2024-03-26T10:15:07.092300Z"
    }
   },
   "id": "7fc4c59fc0fb04e1",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T10:15:07.782152Z",
     "start_time": "2024-03-26T10:15:07.777649Z"
    }
   },
   "id": "f53b8fd8a4597732",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(2764, 461)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T10:15:08.496169Z",
     "start_time": "2024-03-26T10:15:08.492735Z"
    }
   },
   "id": "340643e748d40bdd",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "            Unnamed: 0          A        AAL       AAPL       ABBV        ABT  \\\nDate                                                                            \n2013-01-02           0  27.283405  13.179525  16.769093  22.419830  26.000973   \n2013-01-03           1  27.381128  12.877850  16.557428  22.234699  26.990719   \n2013-01-04           2  27.921843  13.886581  16.096222  21.953815  26.828459   \n2013-01-07           3  27.719885  13.990280  16.001547  21.998503  27.047497   \n2013-01-08           4  27.498386  14.291961  16.044611  21.519714  27.055613   \n2013-01-09           5  28.241058  14.263677  15.793853  21.641014  27.234089   \n2013-01-10           6  28.449533  14.273106  15.989633  21.704851  27.461245   \n2013-01-11           7  28.293179  13.933719  15.891588  21.866341  27.184271   \n2013-01-14           8  28.364840  13.830017  15.325013  22.021378  27.176128   \n2013-01-15           9  28.169407  13.490630  14.841516  22.350830  26.899151   \n\n                 ACGL        ACN       ADBE        ADI  ...         WY  \\\nDate                                                    ...              \n2013-01-02  14.793333  56.624992  38.340000  34.286098  ...  19.033312   \n2013-01-03  14.750000  56.419998  37.750000  33.732830  ...  19.033312   \n2013-01-04  14.876667  56.731575  38.130001  33.132832  ...  19.190020   \n2013-01-07  14.730000  56.485600  37.939999  33.234123  ...  19.248785   \n2013-01-08  14.750000  56.813564  38.139999  32.891258  ...  19.379377   \n2013-01-09  14.803333  57.215347  38.660000  32.805550  ...  19.823374   \n2013-01-10  14.836667  56.715172  38.619999  33.202961  ...  19.940905   \n2013-01-11  14.926667  57.297321  38.090000  32.930241  ...  20.110674   \n2013-01-14  15.026667  57.280918  38.160000  32.914646  ...  20.110674   \n2013-01-15  15.000000  57.600704  38.150002  32.743206  ...  19.967022   \n\n                  WYNN        XEL        XOM       XRAY        XYL        YUM  \\\nDate                                                                            \n2013-01-02   95.529976  19.385836  56.165749  37.213242  23.749273  39.127857   \n2013-01-03   96.557526  19.329191  56.064457  37.112217  23.671537  39.372551   \n2013-01-04   97.722618  19.400000  56.324043  37.433651  23.637007  39.803684   \n2013-01-07   98.054352  19.194668  55.671913  37.562252  23.326099  39.553158   \n2013-01-08   97.973450  19.230074  56.020145  37.130569  23.041113  37.892735   \n2013-01-09   98.321350  19.258394  55.804855  37.112217  23.403828  37.960068   \n2013-01-10   99.437920  19.258394  56.412678  37.038750  23.731987  38.797516   \n2013-01-11   99.510735  19.237150  56.735592  36.671391  23.472908  39.160606   \n2013-01-14   99.356995  19.173429  56.716572  36.561184  23.498819  38.738945   \n2013-01-15  100.166100  19.152187  56.684917  36.359127  23.429733  38.867794   \n\n                  ZBH       ZBRA       ZION  \nDate                                         \n2013-01-02  60.207806  40.959999  17.587208  \n2013-01-03  61.070477  41.000000  17.611206  \n2013-01-04  61.381714  40.669998  18.115057  \n2013-01-07  61.532909  40.900002  18.027084  \n2013-01-08  61.621864  40.930000  17.667183  \n2013-01-09  62.760197  41.369999  17.475239  \n2013-01-10  62.884705  41.150002  17.555218  \n2013-01-11  63.293781  41.299999  17.395258  \n2013-01-14  63.187073  40.980000  17.483234  \n2013-01-15  63.195961  41.139999  17.635189  \n\n[10 rows x 461 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>A</th>\n      <th>AAL</th>\n      <th>AAPL</th>\n      <th>ABBV</th>\n      <th>ABT</th>\n      <th>ACGL</th>\n      <th>ACN</th>\n      <th>ADBE</th>\n      <th>ADI</th>\n      <th>...</th>\n      <th>WY</th>\n      <th>WYNN</th>\n      <th>XEL</th>\n      <th>XOM</th>\n      <th>XRAY</th>\n      <th>XYL</th>\n      <th>YUM</th>\n      <th>ZBH</th>\n      <th>ZBRA</th>\n      <th>ZION</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-01-02</th>\n      <td>0</td>\n      <td>27.283405</td>\n      <td>13.179525</td>\n      <td>16.769093</td>\n      <td>22.419830</td>\n      <td>26.000973</td>\n      <td>14.793333</td>\n      <td>56.624992</td>\n      <td>38.340000</td>\n      <td>34.286098</td>\n      <td>...</td>\n      <td>19.033312</td>\n      <td>95.529976</td>\n      <td>19.385836</td>\n      <td>56.165749</td>\n      <td>37.213242</td>\n      <td>23.749273</td>\n      <td>39.127857</td>\n      <td>60.207806</td>\n      <td>40.959999</td>\n      <td>17.587208</td>\n    </tr>\n    <tr>\n      <th>2013-01-03</th>\n      <td>1</td>\n      <td>27.381128</td>\n      <td>12.877850</td>\n      <td>16.557428</td>\n      <td>22.234699</td>\n      <td>26.990719</td>\n      <td>14.750000</td>\n      <td>56.419998</td>\n      <td>37.750000</td>\n      <td>33.732830</td>\n      <td>...</td>\n      <td>19.033312</td>\n      <td>96.557526</td>\n      <td>19.329191</td>\n      <td>56.064457</td>\n      <td>37.112217</td>\n      <td>23.671537</td>\n      <td>39.372551</td>\n      <td>61.070477</td>\n      <td>41.000000</td>\n      <td>17.611206</td>\n    </tr>\n    <tr>\n      <th>2013-01-04</th>\n      <td>2</td>\n      <td>27.921843</td>\n      <td>13.886581</td>\n      <td>16.096222</td>\n      <td>21.953815</td>\n      <td>26.828459</td>\n      <td>14.876667</td>\n      <td>56.731575</td>\n      <td>38.130001</td>\n      <td>33.132832</td>\n      <td>...</td>\n      <td>19.190020</td>\n      <td>97.722618</td>\n      <td>19.400000</td>\n      <td>56.324043</td>\n      <td>37.433651</td>\n      <td>23.637007</td>\n      <td>39.803684</td>\n      <td>61.381714</td>\n      <td>40.669998</td>\n      <td>18.115057</td>\n    </tr>\n    <tr>\n      <th>2013-01-07</th>\n      <td>3</td>\n      <td>27.719885</td>\n      <td>13.990280</td>\n      <td>16.001547</td>\n      <td>21.998503</td>\n      <td>27.047497</td>\n      <td>14.730000</td>\n      <td>56.485600</td>\n      <td>37.939999</td>\n      <td>33.234123</td>\n      <td>...</td>\n      <td>19.248785</td>\n      <td>98.054352</td>\n      <td>19.194668</td>\n      <td>55.671913</td>\n      <td>37.562252</td>\n      <td>23.326099</td>\n      <td>39.553158</td>\n      <td>61.532909</td>\n      <td>40.900002</td>\n      <td>18.027084</td>\n    </tr>\n    <tr>\n      <th>2013-01-08</th>\n      <td>4</td>\n      <td>27.498386</td>\n      <td>14.291961</td>\n      <td>16.044611</td>\n      <td>21.519714</td>\n      <td>27.055613</td>\n      <td>14.750000</td>\n      <td>56.813564</td>\n      <td>38.139999</td>\n      <td>32.891258</td>\n      <td>...</td>\n      <td>19.379377</td>\n      <td>97.973450</td>\n      <td>19.230074</td>\n      <td>56.020145</td>\n      <td>37.130569</td>\n      <td>23.041113</td>\n      <td>37.892735</td>\n      <td>61.621864</td>\n      <td>40.930000</td>\n      <td>17.667183</td>\n    </tr>\n    <tr>\n      <th>2013-01-09</th>\n      <td>5</td>\n      <td>28.241058</td>\n      <td>14.263677</td>\n      <td>15.793853</td>\n      <td>21.641014</td>\n      <td>27.234089</td>\n      <td>14.803333</td>\n      <td>57.215347</td>\n      <td>38.660000</td>\n      <td>32.805550</td>\n      <td>...</td>\n      <td>19.823374</td>\n      <td>98.321350</td>\n      <td>19.258394</td>\n      <td>55.804855</td>\n      <td>37.112217</td>\n      <td>23.403828</td>\n      <td>37.960068</td>\n      <td>62.760197</td>\n      <td>41.369999</td>\n      <td>17.475239</td>\n    </tr>\n    <tr>\n      <th>2013-01-10</th>\n      <td>6</td>\n      <td>28.449533</td>\n      <td>14.273106</td>\n      <td>15.989633</td>\n      <td>21.704851</td>\n      <td>27.461245</td>\n      <td>14.836667</td>\n      <td>56.715172</td>\n      <td>38.619999</td>\n      <td>33.202961</td>\n      <td>...</td>\n      <td>19.940905</td>\n      <td>99.437920</td>\n      <td>19.258394</td>\n      <td>56.412678</td>\n      <td>37.038750</td>\n      <td>23.731987</td>\n      <td>38.797516</td>\n      <td>62.884705</td>\n      <td>41.150002</td>\n      <td>17.555218</td>\n    </tr>\n    <tr>\n      <th>2013-01-11</th>\n      <td>7</td>\n      <td>28.293179</td>\n      <td>13.933719</td>\n      <td>15.891588</td>\n      <td>21.866341</td>\n      <td>27.184271</td>\n      <td>14.926667</td>\n      <td>57.297321</td>\n      <td>38.090000</td>\n      <td>32.930241</td>\n      <td>...</td>\n      <td>20.110674</td>\n      <td>99.510735</td>\n      <td>19.237150</td>\n      <td>56.735592</td>\n      <td>36.671391</td>\n      <td>23.472908</td>\n      <td>39.160606</td>\n      <td>63.293781</td>\n      <td>41.299999</td>\n      <td>17.395258</td>\n    </tr>\n    <tr>\n      <th>2013-01-14</th>\n      <td>8</td>\n      <td>28.364840</td>\n      <td>13.830017</td>\n      <td>15.325013</td>\n      <td>22.021378</td>\n      <td>27.176128</td>\n      <td>15.026667</td>\n      <td>57.280918</td>\n      <td>38.160000</td>\n      <td>32.914646</td>\n      <td>...</td>\n      <td>20.110674</td>\n      <td>99.356995</td>\n      <td>19.173429</td>\n      <td>56.716572</td>\n      <td>36.561184</td>\n      <td>23.498819</td>\n      <td>38.738945</td>\n      <td>63.187073</td>\n      <td>40.980000</td>\n      <td>17.483234</td>\n    </tr>\n    <tr>\n      <th>2013-01-15</th>\n      <td>9</td>\n      <td>28.169407</td>\n      <td>13.490630</td>\n      <td>14.841516</td>\n      <td>22.350830</td>\n      <td>26.899151</td>\n      <td>15.000000</td>\n      <td>57.600704</td>\n      <td>38.150002</td>\n      <td>32.743206</td>\n      <td>...</td>\n      <td>19.967022</td>\n      <td>100.166100</td>\n      <td>19.152187</td>\n      <td>56.684917</td>\n      <td>36.359127</td>\n      <td>23.429733</td>\n      <td>38.867794</td>\n      <td>63.195961</td>\n      <td>41.139999</td>\n      <td>17.635189</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 461 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T10:15:09.295295Z",
     "start_time": "2024-03-26T10:15:09.282656Z"
    }
   },
   "id": "3b7b7417b0bdba39",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.drop(\"Unnamed: 0\", inplace = True, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T10:15:10.517006Z",
     "start_time": "2024-03-26T10:15:10.513917Z"
    }
   },
   "id": "bb590d8e42618f28",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                    A        AAL       AAPL       ABBV        ABT       ACGL  \\\nDate                                                                           \n2013-01-02  27.283405  13.179525  16.769093  22.419830  26.000973  14.793333   \n2013-01-03  27.381128  12.877850  16.557428  22.234699  26.990719  14.750000   \n2013-01-04  27.921843  13.886581  16.096222  21.953815  26.828459  14.876667   \n2013-01-07  27.719885  13.990280  16.001547  21.998503  27.047497  14.730000   \n2013-01-08  27.498386  14.291961  16.044611  21.519714  27.055613  14.750000   \n2013-01-09  28.241058  14.263677  15.793853  21.641014  27.234089  14.803333   \n2013-01-10  28.449533  14.273106  15.989633  21.704851  27.461245  14.836667   \n2013-01-11  28.293179  13.933719  15.891588  21.866341  27.184271  14.926667   \n2013-01-14  28.364840  13.830017  15.325013  22.021378  27.176128  15.026667   \n2013-01-15  28.169407  13.490630  14.841516  22.350830  26.899151  15.000000   \n\n                  ACN       ADBE        ADI        ADM  ...         WY  \\\nDate                                                    ...              \n2013-01-02  56.624992  38.340000  34.286098  21.421146  ...  19.033312   \n2013-01-03  56.419998  37.750000  33.732830  21.249302  ...  19.033312   \n2013-01-04  56.731575  38.130001  33.132832  21.832092  ...  19.190020   \n2013-01-07  56.485600  37.939999  33.234123  20.928026  ...  19.248785   \n2013-01-08  56.813564  38.139999  32.891258  21.174585  ...  19.379377   \n2013-01-09  57.215347  38.660000  32.805550  21.279190  ...  19.823374   \n2013-01-10  56.715172  38.619999  33.202961  21.174585  ...  19.940905   \n2013-01-11  57.297321  38.090000  32.930241  21.174585  ...  20.110674   \n2013-01-14  57.280918  38.160000  32.914646  21.338957  ...  20.110674   \n2013-01-15  57.600704  38.150002  32.743206  21.234358  ...  19.967022   \n\n                  WYNN        XEL        XOM       XRAY        XYL        YUM  \\\nDate                                                                            \n2013-01-02   95.529976  19.385836  56.165749  37.213242  23.749273  39.127857   \n2013-01-03   96.557526  19.329191  56.064457  37.112217  23.671537  39.372551   \n2013-01-04   97.722618  19.400000  56.324043  37.433651  23.637007  39.803684   \n2013-01-07   98.054352  19.194668  55.671913  37.562252  23.326099  39.553158   \n2013-01-08   97.973450  19.230074  56.020145  37.130569  23.041113  37.892735   \n2013-01-09   98.321350  19.258394  55.804855  37.112217  23.403828  37.960068   \n2013-01-10   99.437920  19.258394  56.412678  37.038750  23.731987  38.797516   \n2013-01-11   99.510735  19.237150  56.735592  36.671391  23.472908  39.160606   \n2013-01-14   99.356995  19.173429  56.716572  36.561184  23.498819  38.738945   \n2013-01-15  100.166100  19.152187  56.684917  36.359127  23.429733  38.867794   \n\n                  ZBH       ZBRA       ZION  \nDate                                         \n2013-01-02  60.207806  40.959999  17.587208  \n2013-01-03  61.070477  41.000000  17.611206  \n2013-01-04  61.381714  40.669998  18.115057  \n2013-01-07  61.532909  40.900002  18.027084  \n2013-01-08  61.621864  40.930000  17.667183  \n2013-01-09  62.760197  41.369999  17.475239  \n2013-01-10  62.884705  41.150002  17.555218  \n2013-01-11  63.293781  41.299999  17.395258  \n2013-01-14  63.187073  40.980000  17.483234  \n2013-01-15  63.195961  41.139999  17.635189  \n\n[10 rows x 460 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>AAL</th>\n      <th>AAPL</th>\n      <th>ABBV</th>\n      <th>ABT</th>\n      <th>ACGL</th>\n      <th>ACN</th>\n      <th>ADBE</th>\n      <th>ADI</th>\n      <th>ADM</th>\n      <th>...</th>\n      <th>WY</th>\n      <th>WYNN</th>\n      <th>XEL</th>\n      <th>XOM</th>\n      <th>XRAY</th>\n      <th>XYL</th>\n      <th>YUM</th>\n      <th>ZBH</th>\n      <th>ZBRA</th>\n      <th>ZION</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-01-02</th>\n      <td>27.283405</td>\n      <td>13.179525</td>\n      <td>16.769093</td>\n      <td>22.419830</td>\n      <td>26.000973</td>\n      <td>14.793333</td>\n      <td>56.624992</td>\n      <td>38.340000</td>\n      <td>34.286098</td>\n      <td>21.421146</td>\n      <td>...</td>\n      <td>19.033312</td>\n      <td>95.529976</td>\n      <td>19.385836</td>\n      <td>56.165749</td>\n      <td>37.213242</td>\n      <td>23.749273</td>\n      <td>39.127857</td>\n      <td>60.207806</td>\n      <td>40.959999</td>\n      <td>17.587208</td>\n    </tr>\n    <tr>\n      <th>2013-01-03</th>\n      <td>27.381128</td>\n      <td>12.877850</td>\n      <td>16.557428</td>\n      <td>22.234699</td>\n      <td>26.990719</td>\n      <td>14.750000</td>\n      <td>56.419998</td>\n      <td>37.750000</td>\n      <td>33.732830</td>\n      <td>21.249302</td>\n      <td>...</td>\n      <td>19.033312</td>\n      <td>96.557526</td>\n      <td>19.329191</td>\n      <td>56.064457</td>\n      <td>37.112217</td>\n      <td>23.671537</td>\n      <td>39.372551</td>\n      <td>61.070477</td>\n      <td>41.000000</td>\n      <td>17.611206</td>\n    </tr>\n    <tr>\n      <th>2013-01-04</th>\n      <td>27.921843</td>\n      <td>13.886581</td>\n      <td>16.096222</td>\n      <td>21.953815</td>\n      <td>26.828459</td>\n      <td>14.876667</td>\n      <td>56.731575</td>\n      <td>38.130001</td>\n      <td>33.132832</td>\n      <td>21.832092</td>\n      <td>...</td>\n      <td>19.190020</td>\n      <td>97.722618</td>\n      <td>19.400000</td>\n      <td>56.324043</td>\n      <td>37.433651</td>\n      <td>23.637007</td>\n      <td>39.803684</td>\n      <td>61.381714</td>\n      <td>40.669998</td>\n      <td>18.115057</td>\n    </tr>\n    <tr>\n      <th>2013-01-07</th>\n      <td>27.719885</td>\n      <td>13.990280</td>\n      <td>16.001547</td>\n      <td>21.998503</td>\n      <td>27.047497</td>\n      <td>14.730000</td>\n      <td>56.485600</td>\n      <td>37.939999</td>\n      <td>33.234123</td>\n      <td>20.928026</td>\n      <td>...</td>\n      <td>19.248785</td>\n      <td>98.054352</td>\n      <td>19.194668</td>\n      <td>55.671913</td>\n      <td>37.562252</td>\n      <td>23.326099</td>\n      <td>39.553158</td>\n      <td>61.532909</td>\n      <td>40.900002</td>\n      <td>18.027084</td>\n    </tr>\n    <tr>\n      <th>2013-01-08</th>\n      <td>27.498386</td>\n      <td>14.291961</td>\n      <td>16.044611</td>\n      <td>21.519714</td>\n      <td>27.055613</td>\n      <td>14.750000</td>\n      <td>56.813564</td>\n      <td>38.139999</td>\n      <td>32.891258</td>\n      <td>21.174585</td>\n      <td>...</td>\n      <td>19.379377</td>\n      <td>97.973450</td>\n      <td>19.230074</td>\n      <td>56.020145</td>\n      <td>37.130569</td>\n      <td>23.041113</td>\n      <td>37.892735</td>\n      <td>61.621864</td>\n      <td>40.930000</td>\n      <td>17.667183</td>\n    </tr>\n    <tr>\n      <th>2013-01-09</th>\n      <td>28.241058</td>\n      <td>14.263677</td>\n      <td>15.793853</td>\n      <td>21.641014</td>\n      <td>27.234089</td>\n      <td>14.803333</td>\n      <td>57.215347</td>\n      <td>38.660000</td>\n      <td>32.805550</td>\n      <td>21.279190</td>\n      <td>...</td>\n      <td>19.823374</td>\n      <td>98.321350</td>\n      <td>19.258394</td>\n      <td>55.804855</td>\n      <td>37.112217</td>\n      <td>23.403828</td>\n      <td>37.960068</td>\n      <td>62.760197</td>\n      <td>41.369999</td>\n      <td>17.475239</td>\n    </tr>\n    <tr>\n      <th>2013-01-10</th>\n      <td>28.449533</td>\n      <td>14.273106</td>\n      <td>15.989633</td>\n      <td>21.704851</td>\n      <td>27.461245</td>\n      <td>14.836667</td>\n      <td>56.715172</td>\n      <td>38.619999</td>\n      <td>33.202961</td>\n      <td>21.174585</td>\n      <td>...</td>\n      <td>19.940905</td>\n      <td>99.437920</td>\n      <td>19.258394</td>\n      <td>56.412678</td>\n      <td>37.038750</td>\n      <td>23.731987</td>\n      <td>38.797516</td>\n      <td>62.884705</td>\n      <td>41.150002</td>\n      <td>17.555218</td>\n    </tr>\n    <tr>\n      <th>2013-01-11</th>\n      <td>28.293179</td>\n      <td>13.933719</td>\n      <td>15.891588</td>\n      <td>21.866341</td>\n      <td>27.184271</td>\n      <td>14.926667</td>\n      <td>57.297321</td>\n      <td>38.090000</td>\n      <td>32.930241</td>\n      <td>21.174585</td>\n      <td>...</td>\n      <td>20.110674</td>\n      <td>99.510735</td>\n      <td>19.237150</td>\n      <td>56.735592</td>\n      <td>36.671391</td>\n      <td>23.472908</td>\n      <td>39.160606</td>\n      <td>63.293781</td>\n      <td>41.299999</td>\n      <td>17.395258</td>\n    </tr>\n    <tr>\n      <th>2013-01-14</th>\n      <td>28.364840</td>\n      <td>13.830017</td>\n      <td>15.325013</td>\n      <td>22.021378</td>\n      <td>27.176128</td>\n      <td>15.026667</td>\n      <td>57.280918</td>\n      <td>38.160000</td>\n      <td>32.914646</td>\n      <td>21.338957</td>\n      <td>...</td>\n      <td>20.110674</td>\n      <td>99.356995</td>\n      <td>19.173429</td>\n      <td>56.716572</td>\n      <td>36.561184</td>\n      <td>23.498819</td>\n      <td>38.738945</td>\n      <td>63.187073</td>\n      <td>40.980000</td>\n      <td>17.483234</td>\n    </tr>\n    <tr>\n      <th>2013-01-15</th>\n      <td>28.169407</td>\n      <td>13.490630</td>\n      <td>14.841516</td>\n      <td>22.350830</td>\n      <td>26.899151</td>\n      <td>15.000000</td>\n      <td>57.600704</td>\n      <td>38.150002</td>\n      <td>32.743206</td>\n      <td>21.234358</td>\n      <td>...</td>\n      <td>19.967022</td>\n      <td>100.166100</td>\n      <td>19.152187</td>\n      <td>56.684917</td>\n      <td>36.359127</td>\n      <td>23.429733</td>\n      <td>38.867794</td>\n      <td>63.195961</td>\n      <td>41.139999</td>\n      <td>17.635189</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 460 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T10:15:11.188292Z",
     "start_time": "2024-03-26T10:15:11.178598Z"
    }
   },
   "id": "aca485f175fd7867",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "all_data_normalized = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T10:15:12.884657Z",
     "start_time": "2024-03-26T10:15:12.881964Z"
    }
   },
   "id": "f33d8a0cdc42689e",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, input_size, layer_size=64, output_size=None):\n",
    "        super(NBeatsBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, layer_size)\n",
    "        self.fc2 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc3 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc4 = nn.Linear(layer_size, output_size if output_size is not None else input_size)\n",
    "        self.output_size = output_size if output_size is not None else input_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        forecast = self.fc4(x)\n",
    "        backcast = x[:, :self.output_size]\n",
    "        return backcast, forecast\n",
    "\n",
    "class NBeatsNet(nn.Module):\n",
    "    def __init__(self, input_size, forecast_length, stack_types=[1, 1], nb_blocks_per_stack=4, layer_size=64, thetas_dim=[4, 8], share_weights_in_stack=False):\n",
    "        super(NBeatsNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.forecast_length = forecast_length\n",
    "        self.stack_types = stack_types\n",
    "        self.nb_blocks_per_stack = nb_blocks_per_stack\n",
    "\n",
    "        blocks = []\n",
    "        for stack_id in range(len(stack_types)):\n",
    "            for block_id in range(nb_blocks_per_stack):\n",
    "                block_init = NBeatsBlock(input_size=input_size, layer_size=layer_size, output_size=forecast_length) # Adjusted to use 'input_size'\n",
    "                blocks.append(block_init)\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        forecast = torch.zeros(x.size(0), self.forecast_length, device=x.device)  # Adjusted to properly initialize the forecast tensor\n",
    "        for block in self.blocks:\n",
    "            backcast, block_forecast = block(x.view(x.size(0), -1))\n",
    "            forecast += block_forecast.view(forecast.size())\n",
    "        return forecast"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T10:15:13.546465Z",
     "start_time": "2024-03-26T10:15:13.539245Z"
    }
   },
   "id": "3a92397e0b44f4bf",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length, label):\n",
    "    X, y, labels = [], [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "        labels.append(label)\n",
    "    return np.array(X), np.array(y), np.array(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T10:15:14.615743Z",
     "start_time": "2024-03-26T10:15:14.612805Z"
    }
   },
   "id": "bd369e0cbe661dd2",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_combined shape: (4947300, 30, 1)\n",
      "y_train_combined shape: (4947300, 1)\n",
      "labels_train_combined shape: (4947300,)\n",
      "X_val_combined shape: (786600, 30, 1)\n",
      "y_val_combined shape: (786600, 1)\n",
      "labels_val_combined shape: (786600,)\n",
      "X_test_combined shape: (254380, 30, 1)\n",
      "y_test_combined shape: (254380, 1)\n",
      "labels_test_combined shape: (254380,)\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 30\n",
    "test_size = 0.2\n",
    "validation_fraction = 0.1  \n",
    "\n",
    "X_train_combined, y_train_combined, labels_train_combined = [], [], []\n",
    "X_val_combined, y_val_combined, labels_val_combined = [], [], []\n",
    "X_test_combined, y_test_combined, labels_test_combined = [], [], []\n",
    "\n",
    "def create_sequences(data, sequence_length, label):\n",
    "    X, y, labels = [], [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "        labels.append(label)\n",
    "    return np.array(X), np.array(y), np.array(labels)\n",
    "\n",
    "for idx, company in enumerate(df.columns):\n",
    "    company_data = df[company].dropna().values.reshape(-1, 1)  \n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    normalized_data = scaler.fit_transform(company_data)\n",
    "\n",
    "    if len(normalized_data) <= sequence_length:\n",
    "        print(f\"Skipping {company}: Not enough data for sequence creation.\")\n",
    "        continue\n",
    "\n",
    "    train_size = int(len(normalized_data) * (1 - test_size))\n",
    "    train_data = normalized_data[:train_size]\n",
    "    test_data = normalized_data[train_size - sequence_length:]  \n",
    "\n",
    "    X_test_tmp, y_test_tmp, labels_test_tmp = create_sequences(test_data, sequence_length, idx)\n",
    "    X_test_combined.extend(X_test_tmp)\n",
    "    y_test_combined.extend(y_test_tmp)\n",
    "    labels_test_combined.extend(labels_test_tmp)\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=int(1 / validation_fraction))\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(train_data):\n",
    "        X_train_tmp, y_train_tmp, labels_train_tmp = create_sequences(train_data[train_idx], sequence_length, idx)\n",
    "        X_val_tmp, y_val_tmp, labels_val_tmp = create_sequences(train_data[val_idx], sequence_length, idx)\n",
    "\n",
    "        X_train_combined.extend(X_train_tmp)\n",
    "        y_train_combined.extend(y_train_tmp)\n",
    "        labels_train_combined.extend(labels_train_tmp)\n",
    "\n",
    "        X_val_combined.extend(X_val_tmp)\n",
    "        y_val_combined.extend(y_val_tmp)\n",
    "        labels_val_combined.extend(labels_val_tmp)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train_combined = np.array(X_train_combined)\n",
    "y_train_combined = np.array(y_train_combined).reshape(-1, 1)\n",
    "labels_train_combined = np.array(labels_train_combined)\n",
    "\n",
    "X_val_combined = np.array(X_val_combined)\n",
    "y_val_combined = np.array(y_val_combined).reshape(-1, 1)\n",
    "labels_val_combined = np.array(labels_val_combined)\n",
    "\n",
    "X_test_combined = np.array(X_test_combined)\n",
    "y_test_combined = np.array(y_test_combined).reshape(-1, 1)\n",
    "labels_test_combined = np.array(labels_test_combined)\n",
    "\n",
    "# Print the shapes\n",
    "print(f\"X_train_combined shape: {X_train_combined.shape}\")\n",
    "print(f\"y_train_combined shape: {y_train_combined.shape}\")\n",
    "print(f\"labels_train_combined shape: {labels_train_combined.shape}\")\n",
    "print(f\"X_val_combined shape: {X_val_combined.shape}\")\n",
    "print(f\"y_val_combined shape: {y_val_combined.shape}\")\n",
    "print(f\"labels_val_combined shape: {labels_val_combined.shape}\")\n",
    "print(f\"X_test_combined shape: {X_test_combined.shape}\")\n",
    "print(f\"y_test_combined shape: {y_test_combined.shape}\")\n",
    "print(f\"labels_test_combined shape: {labels_test_combined.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T10:15:23.567057Z",
     "start_time": "2024-03-26T10:15:15.391083Z"
    }
   },
   "id": "ac9d5d1b69441008",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4948/4948 [00:00<00:00, 14208.39it/s]\n",
      "100%|██████████| 4948/4948 [00:00<00:00, 220844.46it/s]\n",
      "100%|██████████| 787/787 [00:00<00:00, 15141.06it/s]\n",
      "100%|██████████| 787/787 [00:00<00:00, 226400.36it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_convert_to_tensor(data, batch_size=1000, dtype=torch.float32):\n",
    "    tensor_chunks = []\n",
    "    for i in tqdm(range(0, len(data), batch_size)):\n",
    "        batch = data[i:i+batch_size]\n",
    "        tensor = torch.tensor(batch, dtype=dtype)\n",
    "        tensor_chunks.append(tensor)\n",
    "    return torch.cat(tensor_chunks, dim=0)\n",
    "\n",
    "# Convert training and validation data to tensors\n",
    "X_train_tensor = batch_convert_to_tensor(X_train_combined, batch_size=1000)\n",
    "y_train_tensor = batch_convert_to_tensor(y_train_combined, batch_size=1000)\n",
    "X_val_tensor = batch_convert_to_tensor(X_val_combined, batch_size=1000)\n",
    "y_val_tensor = batch_convert_to_tensor(y_val_combined, batch_size=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T10:15:28.425169Z",
     "start_time": "2024-03-26T10:15:27.675225Z"
    }
   },
   "id": "9e5a043431cfc56e",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the model training\n",
      "\n",
      "Epoch 1/100\n",
      "Epoch 1, Batch 100/4948, Batch loss: 0.001413231366313994\n",
      "Memory Usage: 2543.59 MB\n",
      "Epoch 1, Batch 200/4948, Batch loss: 0.00022741266002412885\n",
      "Memory Usage: 2403.69 MB\n",
      "Epoch 1, Batch 300/4948, Batch loss: 0.001555035007186234\n",
      "Memory Usage: 2421.95 MB\n",
      "Epoch 1, Batch 400/4948, Batch loss: 0.00035302885225974023\n",
      "Memory Usage: 2434.70 MB\n",
      "Epoch 1, Batch 500/4948, Batch loss: 0.0001084524265024811\n",
      "Memory Usage: 2447.03 MB\n",
      "Epoch 1, Batch 600/4948, Batch loss: 0.0009141748305410147\n",
      "Memory Usage: 2459.36 MB\n",
      "Epoch 1, Batch 700/4948, Batch loss: 0.00011800937500083819\n",
      "Memory Usage: 2451.95 MB\n",
      "Epoch 1, Batch 800/4948, Batch loss: 0.00018084245675709099\n",
      "Memory Usage: 2283.03 MB\n",
      "Epoch 1, Batch 900/4948, Batch loss: 0.0009326428989879787\n",
      "Memory Usage: 2243.47 MB\n",
      "Epoch 1, Batch 1000/4948, Batch loss: 0.00011829907452920452\n",
      "Memory Usage: 2256.67 MB\n",
      "Epoch 1, Batch 1100/4948, Batch loss: 0.0012377171078696847\n",
      "Memory Usage: 2268.50 MB\n",
      "Epoch 1, Batch 1200/4948, Batch loss: 0.002258430700749159\n",
      "Memory Usage: 2281.30 MB\n",
      "Epoch 1, Batch 1300/4948, Batch loss: 0.0001781697355909273\n",
      "Memory Usage: 2293.14 MB\n",
      "Epoch 1, Batch 1400/4948, Batch loss: 0.0001450647396268323\n",
      "Memory Usage: 2243.48 MB\n",
      "Epoch 1, Batch 1500/4948, Batch loss: 0.0002715503505896777\n",
      "Memory Usage: 2256.81 MB\n",
      "Epoch 1, Batch 1600/4948, Batch loss: 1.6360658264602534e-05\n",
      "Memory Usage: 2268.61 MB\n",
      "Epoch 1, Batch 1700/4948, Batch loss: 0.000568552000913769\n",
      "Memory Usage: 2280.95 MB\n",
      "Epoch 1, Batch 1800/4948, Batch loss: 0.0003086670767515898\n",
      "Memory Usage: 2292.80 MB\n",
      "Epoch 1, Batch 1900/4948, Batch loss: 0.011590095236897469\n",
      "Memory Usage: 2304.61 MB\n",
      "Epoch 1, Batch 2000/4948, Batch loss: 0.0006247792625799775\n",
      "Memory Usage: 2316.44 MB\n",
      "Epoch 1, Batch 2100/4948, Batch loss: 0.0011522687273100019\n",
      "Memory Usage: 2328.31 MB\n",
      "Epoch 1, Batch 2200/4948, Batch loss: 0.00012649189739022404\n",
      "Memory Usage: 2340.16 MB\n",
      "Epoch 1, Batch 2300/4948, Batch loss: 0.00029291579267010093\n",
      "Memory Usage: 2351.97 MB\n",
      "Epoch 1, Batch 2400/4948, Batch loss: 0.00012679096835199744\n",
      "Memory Usage: 2363.80 MB\n",
      "Epoch 1, Batch 2500/4948, Batch loss: 5.9221118135610595e-05\n",
      "Memory Usage: 2375.62 MB\n",
      "Epoch 1, Batch 2600/4948, Batch loss: 0.0004617935628630221\n",
      "Memory Usage: 2388.44 MB\n",
      "Epoch 1, Batch 2700/4948, Batch loss: 0.0005651303217746317\n",
      "Memory Usage: 2400.28 MB\n",
      "Epoch 1, Batch 2800/4948, Batch loss: 0.0019313811790198088\n",
      "Memory Usage: 2412.09 MB\n",
      "Epoch 1, Batch 2900/4948, Batch loss: 0.00022313944646157324\n",
      "Memory Usage: 2423.94 MB\n",
      "Epoch 1, Batch 3000/4948, Batch loss: 6.393348303390667e-05\n",
      "Memory Usage: 2435.75 MB\n",
      "Epoch 1, Batch 3100/4948, Batch loss: 0.00013854344433639199\n",
      "Memory Usage: 2447.56 MB\n",
      "Epoch 1, Batch 3200/4948, Batch loss: 0.0003471710078883916\n",
      "Memory Usage: 2459.34 MB\n",
      "Epoch 1, Batch 3300/4948, Batch loss: 0.00014695525169372559\n",
      "Memory Usage: 2471.16 MB\n",
      "Epoch 1, Batch 3400/4948, Batch loss: 1.1084110155934468e-05\n",
      "Memory Usage: 2483.00 MB\n",
      "Epoch 1, Batch 3500/4948, Batch loss: 0.0009409557678736746\n",
      "Memory Usage: 2494.81 MB\n",
      "Epoch 1, Batch 3600/4948, Batch loss: 4.3677286157617345e-05\n",
      "Memory Usage: 2506.64 MB\n",
      "Epoch 1, Batch 3700/4948, Batch loss: 0.0006244454416446388\n",
      "Memory Usage: 2518.47 MB\n",
      "Epoch 1, Batch 3800/4948, Batch loss: 0.00011631540110101923\n",
      "Memory Usage: 2530.30 MB\n",
      "Epoch 1, Batch 3900/4948, Batch loss: 0.00015700749645475298\n",
      "Memory Usage: 2542.12 MB\n",
      "Epoch 1, Batch 4000/4948, Batch loss: 0.00015702635573688895\n",
      "Memory Usage: 2553.92 MB\n",
      "Epoch 1, Batch 4100/4948, Batch loss: 4.854897269979119e-05\n",
      "Memory Usage: 2565.64 MB\n",
      "Epoch 1, Batch 4200/4948, Batch loss: 0.0011870893649756908\n",
      "Memory Usage: 2577.48 MB\n",
      "Epoch 1, Batch 4300/4948, Batch loss: 0.00015581809566356242\n",
      "Memory Usage: 2589.28 MB\n",
      "Epoch 1, Batch 4400/4948, Batch loss: 1.0788997315103188e-05\n",
      "Memory Usage: 2601.09 MB\n",
      "Epoch 1, Batch 4500/4948, Batch loss: 0.0010936547769233584\n",
      "Memory Usage: 2613.41 MB\n",
      "Epoch 1, Batch 4600/4948, Batch loss: 0.00025377009296789765\n",
      "Memory Usage: 2625.25 MB\n",
      "Epoch 1, Batch 4700/4948, Batch loss: 0.00046066136565059423\n",
      "Memory Usage: 2637.06 MB\n",
      "Epoch 1, Batch 4800/4948, Batch loss: 0.00017901649698615074\n",
      "Memory Usage: 2648.89 MB\n",
      "Epoch 1, Batch 4900/4948, Batch loss: 0.00023152146604843438\n",
      "Memory Usage: 2660.72 MB\n",
      "Epoch 1, Batch 4948/4948, Batch loss: 0.0015159270260483027\n",
      "Memory Usage: 2666.28 MB\n",
      "Epoch 1 completed in 82.61 seconds, Total Training Loss: 0.001562528224358073\n",
      "Validation completed in 3.92 seconds, Average Validation Loss: 0.0005460450234678519\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 2/100\n",
      "Epoch 2, Batch 100/4948, Batch loss: 0.00017795542953535914\n",
      "Memory Usage: 2788.61 MB\n",
      "Epoch 2, Batch 200/4948, Batch loss: 1.4121319509285968e-05\n",
      "Memory Usage: 2788.62 MB\n",
      "Epoch 2, Batch 300/4948, Batch loss: 0.0003671476151794195\n",
      "Memory Usage: 2788.62 MB\n",
      "Epoch 2, Batch 400/4948, Batch loss: 0.0001245488238055259\n",
      "Memory Usage: 2788.62 MB\n",
      "Epoch 2, Batch 500/4948, Batch loss: 5.20512112416327e-05\n",
      "Memory Usage: 2788.66 MB\n",
      "Epoch 2, Batch 600/4948, Batch loss: 0.0004843273782171309\n",
      "Memory Usage: 2788.66 MB\n",
      "Epoch 2, Batch 700/4948, Batch loss: 0.002459949115291238\n",
      "Memory Usage: 2788.66 MB\n",
      "Epoch 2, Batch 800/4948, Batch loss: 0.00018067889322992414\n",
      "Memory Usage: 2788.66 MB\n",
      "Epoch 2, Batch 900/4948, Batch loss: 0.0007444778457283974\n",
      "Memory Usage: 2788.66 MB\n",
      "Epoch 2, Batch 1000/4948, Batch loss: 9.248963760910556e-05\n",
      "Memory Usage: 2788.67 MB\n",
      "Epoch 2, Batch 1100/4948, Batch loss: 0.0009275873308070004\n",
      "Memory Usage: 2788.67 MB\n",
      "Epoch 2, Batch 1200/4948, Batch loss: 0.0018309243023395538\n",
      "Memory Usage: 2788.69 MB\n",
      "Epoch 2, Batch 1300/4948, Batch loss: 0.00015880131104495376\n",
      "Memory Usage: 2788.69 MB\n",
      "Epoch 2, Batch 1400/4948, Batch loss: 0.0001267209299840033\n",
      "Memory Usage: 2788.69 MB\n",
      "Epoch 2, Batch 1500/4948, Batch loss: 0.00014607099001295865\n",
      "Memory Usage: 2788.69 MB\n",
      "Epoch 2, Batch 1600/4948, Batch loss: 0.000228858130867593\n",
      "Memory Usage: 2789.20 MB\n",
      "Epoch 2, Batch 1700/4948, Batch loss: 0.004535290412604809\n",
      "Memory Usage: 2789.20 MB\n",
      "Epoch 2, Batch 1800/4948, Batch loss: 0.0004416669544298202\n",
      "Memory Usage: 2789.20 MB\n",
      "Epoch 2, Batch 1900/4948, Batch loss: 0.0181854497641325\n",
      "Memory Usage: 2789.20 MB\n",
      "Epoch 2, Batch 2000/4948, Batch loss: 0.0008469998720102012\n",
      "Memory Usage: 2789.20 MB\n",
      "Epoch 2, Batch 2100/4948, Batch loss: 0.0012377508683130145\n",
      "Memory Usage: 2789.20 MB\n",
      "Epoch 2, Batch 2200/4948, Batch loss: 0.00012066031922586262\n",
      "Memory Usage: 2789.22 MB\n",
      "Epoch 2, Batch 2300/4948, Batch loss: 0.00024302996462211013\n",
      "Memory Usage: 2789.22 MB\n",
      "Epoch 2, Batch 2400/4948, Batch loss: 0.00012391069321893156\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 2500/4948, Batch loss: 5.404818875831552e-05\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 2600/4948, Batch loss: 0.0004269801138434559\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 2700/4948, Batch loss: 0.0005262845661491156\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 2800/4948, Batch loss: 0.0017749686958268285\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 2900/4948, Batch loss: 0.000212885468499735\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 3000/4948, Batch loss: 6.29260393907316e-05\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 3100/4948, Batch loss: 0.0001304027537116781\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 3200/4948, Batch loss: 0.00034345907624810934\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 3300/4948, Batch loss: 0.0001453749864595011\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 3400/4948, Batch loss: 9.916778253682423e-06\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 3500/4948, Batch loss: 6.0244805354159325e-05\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 3600/4948, Batch loss: 4.539808287518099e-05\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 3700/4948, Batch loss: 0.0005999261629767716\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 3800/4948, Batch loss: 0.0001212317292811349\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 3900/4948, Batch loss: 0.00014333603030536324\n",
      "Memory Usage: 2789.23 MB\n",
      "Epoch 2, Batch 4000/4948, Batch loss: 0.00015629061090294272\n",
      "Memory Usage: 2789.27 MB\n",
      "Epoch 2, Batch 4100/4948, Batch loss: 5.398471330408938e-05\n",
      "Memory Usage: 2789.27 MB\n",
      "Epoch 2, Batch 4200/4948, Batch loss: 0.0011206272756680846\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 2, Batch 4300/4948, Batch loss: 0.0001477705518482253\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 2, Batch 4400/4948, Batch loss: 2.3621607397217304e-05\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 2, Batch 4500/4948, Batch loss: 0.001069395337253809\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 2, Batch 4600/4948, Batch loss: 0.0002640275633893907\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 2, Batch 4700/4948, Batch loss: 0.000452678301371634\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 2, Batch 4800/4948, Batch loss: 0.00012790417531505227\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 2, Batch 4900/4948, Batch loss: 0.00010686380119295791\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 2, Batch 4948/4948, Batch loss: 0.001482651219703257\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 2 completed in 76.54 seconds, Total Training Loss: 0.0005371061907672863\n",
      "\n",
      "Epoch 3/100\n",
      "Epoch 3, Batch 100/4948, Batch loss: 0.0001736589038046077\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 3, Batch 200/4948, Batch loss: 1.9385510313441046e-05\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 3, Batch 300/4948, Batch loss: 0.0003509621601551771\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 3, Batch 400/4948, Batch loss: 0.00012265128316357732\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 3, Batch 500/4948, Batch loss: 5.209097798797302e-05\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 3, Batch 600/4948, Batch loss: 0.000491925107780844\n",
      "Memory Usage: 2789.78 MB\n",
      "Epoch 3, Batch 700/4948, Batch loss: 0.0005385350086726248\n",
      "Memory Usage: 2790.28 MB\n",
      "Epoch 3, Batch 800/4948, Batch loss: 0.00013523301458917558\n",
      "Memory Usage: 2790.28 MB\n",
      "Epoch 3, Batch 900/4948, Batch loss: 0.0005824182881042361\n",
      "Memory Usage: 2790.28 MB\n",
      "Epoch 3, Batch 1000/4948, Batch loss: 8.687394438311458e-05\n",
      "Memory Usage: 2790.28 MB\n",
      "Epoch 3, Batch 1100/4948, Batch loss: 0.0009033234673552215\n",
      "Memory Usage: 2790.28 MB\n",
      "Epoch 3, Batch 1200/4948, Batch loss: 0.0018026314210146666\n",
      "Memory Usage: 2790.28 MB\n",
      "Epoch 3, Batch 1300/4948, Batch loss: 0.00015045121836010367\n",
      "Memory Usage: 2790.28 MB\n",
      "Epoch 3, Batch 1400/4948, Batch loss: 0.00012422770669218153\n",
      "Memory Usage: 2790.28 MB\n",
      "Epoch 3, Batch 1500/4948, Batch loss: 0.00011823074601124972\n",
      "Memory Usage: 2790.28 MB\n",
      "Epoch 3, Batch 1600/4948, Batch loss: 9.598428732715547e-05\n",
      "Memory Usage: 2790.28 MB\n",
      "Epoch 3, Batch 1700/4948, Batch loss: 0.0025504259392619133\n",
      "Memory Usage: 2790.28 MB\n",
      "Epoch 3, Batch 1800/4948, Batch loss: 0.00032139860559254885\n",
      "Memory Usage: 2790.31 MB\n",
      "Epoch 3, Batch 1900/4948, Batch loss: 0.015123601071536541\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 2000/4948, Batch loss: 0.0012434310046955943\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 2100/4948, Batch loss: 0.001241400372236967\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 2200/4948, Batch loss: 0.00011387943959562108\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 2300/4948, Batch loss: 0.0002406088897259906\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 2400/4948, Batch loss: 0.00011576975521165878\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 2500/4948, Batch loss: 5.1420662202872336e-05\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 2600/4948, Batch loss: 0.00041192088974639773\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 2700/4948, Batch loss: 0.0004946835106238723\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 2800/4948, Batch loss: 0.001713440171442926\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 2900/4948, Batch loss: 0.00020374087034724653\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 3000/4948, Batch loss: 6.0417081840569153e-05\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 3100/4948, Batch loss: 0.00012705757399089634\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 3200/4948, Batch loss: 0.0003455989935901016\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 3300/4948, Batch loss: 0.00014619011199101806\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 3400/4948, Batch loss: 9.599999430065509e-06\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 3500/4948, Batch loss: 5.565604078583419e-05\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 3600/4948, Batch loss: 4.999062366550788e-05\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 3700/4948, Batch loss: 0.000585885951295495\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 3800/4948, Batch loss: 0.0001146584254456684\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 3900/4948, Batch loss: 0.00013846986985299736\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 4000/4948, Batch loss: 0.0001541225065011531\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 4100/4948, Batch loss: 5.019391392124817e-05\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 4200/4948, Batch loss: 0.0010894613806158304\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 4300/4948, Batch loss: 0.00014829152496531606\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 4400/4948, Batch loss: 1.362468447041465e-05\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 4500/4948, Batch loss: 0.0010611696634441614\n",
      "Memory Usage: 2790.81 MB\n",
      "Epoch 3, Batch 4600/4948, Batch loss: 0.000273823767201975\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 3, Batch 4700/4948, Batch loss: 0.00045297935139387846\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 3, Batch 4800/4948, Batch loss: 0.00011359396739862859\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 3, Batch 4900/4948, Batch loss: 7.392227416858077e-05\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 3, Batch 4948/4948, Batch loss: 0.0014829201390966773\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 3 completed in 76.76 seconds, Total Training Loss: 0.0004683417238843581\n",
      "\n",
      "Epoch 4/100\n",
      "Epoch 4, Batch 100/4948, Batch loss: 0.0001789496309356764\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 4, Batch 200/4948, Batch loss: 4.969122164766304e-05\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 4, Batch 300/4948, Batch loss: 0.00034744470031000674\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 4, Batch 400/4948, Batch loss: 0.0001220606136485003\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 4, Batch 500/4948, Batch loss: 5.4783147788839415e-05\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 4, Batch 600/4948, Batch loss: 0.0004730905930045992\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 4, Batch 700/4948, Batch loss: 5.7779430790105835e-05\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 4, Batch 800/4948, Batch loss: 0.0001319890725426376\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 4, Batch 900/4948, Batch loss: 0.0005383347161114216\n",
      "Memory Usage: 2790.86 MB\n",
      "Epoch 4, Batch 1000/4948, Batch loss: 8.729853288969025e-05\n",
      "Memory Usage: 2791.36 MB\n",
      "Epoch 4, Batch 1100/4948, Batch loss: 0.0008885581628419459\n",
      "Memory Usage: 2791.36 MB\n",
      "Epoch 4, Batch 1200/4948, Batch loss: 0.0018224661471322179\n",
      "Memory Usage: 2791.36 MB\n",
      "Epoch 4, Batch 1300/4948, Batch loss: 0.0001735577534418553\n",
      "Memory Usage: 2791.86 MB\n",
      "Epoch 4, Batch 1400/4948, Batch loss: 0.00012360229447949678\n",
      "Memory Usage: 2791.86 MB\n",
      "Epoch 4, Batch 1500/4948, Batch loss: 9.95259324554354e-05\n",
      "Memory Usage: 2791.86 MB\n",
      "Epoch 4, Batch 1600/4948, Batch loss: 6.552569539053366e-05\n",
      "Memory Usage: 2791.86 MB\n",
      "Epoch 4, Batch 1700/4948, Batch loss: 0.0007482828223146498\n",
      "Memory Usage: 2791.86 MB\n",
      "Epoch 4, Batch 1800/4948, Batch loss: 0.00027479015989229083\n",
      "Memory Usage: 2791.86 MB\n",
      "Epoch 4, Batch 1900/4948, Batch loss: 0.0015256518963724375\n",
      "Memory Usage: 2791.86 MB\n",
      "Epoch 4, Batch 2000/4948, Batch loss: 0.0004975541960448027\n",
      "Memory Usage: 2791.86 MB\n",
      "Epoch 4, Batch 2100/4948, Batch loss: 0.0008892972837202251\n",
      "Memory Usage: 2791.86 MB\n",
      "Epoch 4, Batch 2200/4948, Batch loss: 0.00010631239274516702\n",
      "Memory Usage: 2791.86 MB\n",
      "Epoch 4, Batch 2300/4948, Batch loss: 0.0002349722053622827\n",
      "Memory Usage: 2792.36 MB\n",
      "Epoch 4, Batch 2400/4948, Batch loss: 0.00010751625814009458\n",
      "Memory Usage: 2793.38 MB\n",
      "Epoch 4, Batch 2500/4948, Batch loss: 5.220071034273133e-05\n",
      "Memory Usage: 2794.38 MB\n",
      "Epoch 4, Batch 2600/4948, Batch loss: 0.00040555858868174255\n",
      "Memory Usage: 2794.42 MB\n",
      "Epoch 4, Batch 2700/4948, Batch loss: 0.0005033083725720644\n",
      "Memory Usage: 2794.44 MB\n",
      "Epoch 4, Batch 2800/4948, Batch loss: 0.001632716041058302\n",
      "Memory Usage: 2794.94 MB\n",
      "Epoch 4, Batch 2900/4948, Batch loss: 0.0002040370018221438\n",
      "Memory Usage: 2794.94 MB\n",
      "Epoch 4, Batch 3000/4948, Batch loss: 8.281136251753196e-05\n",
      "Memory Usage: 2794.94 MB\n",
      "Epoch 4, Batch 3100/4948, Batch loss: 0.00015161902410909534\n",
      "Memory Usage: 2795.44 MB\n",
      "Epoch 4, Batch 3200/4948, Batch loss: 0.0003234113100916147\n",
      "Memory Usage: 2795.44 MB\n",
      "Epoch 4, Batch 3300/4948, Batch loss: 0.00014389929128810763\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 3400/4948, Batch loss: 5.166200025996659e-06\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 3500/4948, Batch loss: 0.0004064415115863085\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 3600/4948, Batch loss: 4.832959166378714e-05\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 3700/4948, Batch loss: 0.0005424306727945805\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 3800/4948, Batch loss: 0.0001080906149582006\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 3900/4948, Batch loss: 0.00014868094876874238\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 4000/4948, Batch loss: 0.00015432789223268628\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 4100/4948, Batch loss: 0.00010446884698467329\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 4200/4948, Batch loss: 0.0010751589434221387\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 4300/4948, Batch loss: 0.00014734399155713618\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 4400/4948, Batch loss: 1.3572344869317021e-05\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 4500/4948, Batch loss: 0.0010442425264045596\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 4600/4948, Batch loss: 0.000259083928540349\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 4700/4948, Batch loss: 0.0004438319301698357\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 4800/4948, Batch loss: 0.00012123022315790877\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 4900/4948, Batch loss: 6.99970405548811e-05\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4, Batch 4948/4948, Batch loss: 0.0014699590392410755\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 4 completed in 78.07 seconds, Total Training Loss: 0.0003445990891674023\n",
      "\n",
      "Epoch 5/100\n",
      "Epoch 5, Batch 100/4948, Batch loss: 0.0001742304884828627\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 5, Batch 200/4948, Batch loss: 3.321221811347641e-05\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 5, Batch 300/4948, Batch loss: 0.00035157796810381114\n",
      "Memory Usage: 2795.45 MB\n",
      "Epoch 5, Batch 400/4948, Batch loss: 0.0001208569374284707\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 500/4948, Batch loss: 7.504087261622772e-05\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 600/4948, Batch loss: 0.00046903875772841275\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 700/4948, Batch loss: 9.379289986100048e-05\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 800/4948, Batch loss: 0.0001660816924413666\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 900/4948, Batch loss: 0.0005312010762281716\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 1000/4948, Batch loss: 8.760261698625982e-05\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 1100/4948, Batch loss: 0.0008982808794826269\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 1200/4948, Batch loss: 0.0017749923281371593\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 1300/4948, Batch loss: 0.0001527984713902697\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 1400/4948, Batch loss: 0.00012302208051551133\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 1500/4948, Batch loss: 0.0001227941975230351\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 1600/4948, Batch loss: 0.0002030727337114513\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 1700/4948, Batch loss: 0.0016307084588333964\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 1800/4948, Batch loss: 0.00030737894121557474\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 1900/4948, Batch loss: 0.008227470330893993\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 2000/4948, Batch loss: 0.00061908143106848\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 2100/4948, Batch loss: 0.0008158253040164709\n",
      "Memory Usage: 2795.47 MB\n",
      "Epoch 5, Batch 2200/4948, Batch loss: 0.00010975303302984685\n",
      "Memory Usage: 2796.47 MB\n",
      "Epoch 5, Batch 2300/4948, Batch loss: 0.00021564465714618564\n",
      "Memory Usage: 2796.47 MB\n",
      "Epoch 5, Batch 2400/4948, Batch loss: 0.00010901452333200723\n",
      "Memory Usage: 2796.47 MB\n",
      "Epoch 5, Batch 2500/4948, Batch loss: 5.167203926248476e-05\n",
      "Memory Usage: 2796.48 MB\n",
      "Epoch 5, Batch 2600/4948, Batch loss: 0.00040173789602704346\n",
      "Memory Usage: 1833.36 MB\n",
      "Epoch 5, Batch 2700/4948, Batch loss: 0.00046654706238768995\n",
      "Memory Usage: 1840.78 MB\n",
      "Epoch 5, Batch 2800/4948, Batch loss: 0.0016594832995906472\n",
      "Memory Usage: 1844.95 MB\n",
      "Epoch 5, Batch 2900/4948, Batch loss: 0.0002008730807574466\n",
      "Memory Usage: 1845.83 MB\n",
      "Epoch 5, Batch 3000/4948, Batch loss: 5.8781177358468994e-05\n",
      "Memory Usage: 1743.95 MB\n",
      "Epoch 5, Batch 3100/4948, Batch loss: 0.00012619492190424353\n",
      "Memory Usage: 1745.17 MB\n",
      "Epoch 5, Batch 3200/4948, Batch loss: 0.00033287040423601866\n",
      "Memory Usage: 1746.39 MB\n",
      "Epoch 5, Batch 3300/4948, Batch loss: 0.0001439544721506536\n",
      "Memory Usage: 1747.30 MB\n",
      "Epoch 5, Batch 3400/4948, Batch loss: 4.980229732609587e-06\n",
      "Memory Usage: 1747.39 MB\n",
      "Epoch 5, Batch 3500/4948, Batch loss: 0.0002221945469500497\n",
      "Memory Usage: 1748.64 MB\n",
      "Epoch 5, Batch 3600/4948, Batch loss: 4.053238808410242e-05\n",
      "Memory Usage: 1749.58 MB\n",
      "Epoch 5, Batch 3700/4948, Batch loss: 0.0005833645700477064\n",
      "Memory Usage: 1749.80 MB\n",
      "Epoch 5, Batch 3800/4948, Batch loss: 0.00011597912816796452\n",
      "Memory Usage: 1749.80 MB\n",
      "Epoch 5, Batch 3900/4948, Batch loss: 0.00013786766794510186\n",
      "Memory Usage: 1750.67 MB\n",
      "Epoch 5, Batch 4000/4948, Batch loss: 0.00015304842963814735\n",
      "Memory Usage: 1750.67 MB\n",
      "Epoch 5, Batch 4100/4948, Batch loss: 4.5715241867583245e-05\n",
      "Memory Usage: 1750.67 MB\n",
      "Epoch 5, Batch 4200/4948, Batch loss: 0.001060656737536192\n",
      "Memory Usage: 1751.42 MB\n",
      "Epoch 5, Batch 4300/4948, Batch loss: 0.0001453194854548201\n",
      "Memory Usage: 1751.42 MB\n",
      "Epoch 5, Batch 4400/4948, Batch loss: 7.687591278227046e-06\n",
      "Memory Usage: 1751.42 MB\n",
      "Epoch 5, Batch 4500/4948, Batch loss: 0.0010441356571391225\n",
      "Memory Usage: 1751.42 MB\n",
      "Epoch 5, Batch 4600/4948, Batch loss: 0.0002560630673542619\n",
      "Memory Usage: 1751.42 MB\n",
      "Epoch 5, Batch 4700/4948, Batch loss: 0.0004526170960161835\n",
      "Memory Usage: 1751.42 MB\n",
      "Epoch 5, Batch 4800/4948, Batch loss: 0.0001117176580009982\n",
      "Memory Usage: 1751.42 MB\n",
      "Epoch 5, Batch 4900/4948, Batch loss: 8.599869761383161e-05\n",
      "Memory Usage: 1751.42 MB\n",
      "Epoch 5, Batch 4948/4948, Batch loss: 0.001459525665268302\n",
      "Memory Usage: 1751.42 MB\n",
      "Epoch 5 completed in 78.13 seconds, Total Training Loss: 0.0003786210765797415\n",
      "\n",
      "Epoch 6/100\n",
      "Epoch 6, Batch 100/4948, Batch loss: 0.00017295111319981515\n",
      "Memory Usage: 1751.42 MB\n",
      "Epoch 6, Batch 200/4948, Batch loss: 8.392331073991954e-05\n",
      "Memory Usage: 1751.42 MB\n",
      "Epoch 6, Batch 300/4948, Batch loss: 0.0003407553886063397\n",
      "Memory Usage: 1751.48 MB\n",
      "Epoch 6, Batch 400/4948, Batch loss: 0.00012164033978478983\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 500/4948, Batch loss: 5.4166917834663764e-05\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 600/4948, Batch loss: 0.0004682349681388587\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 700/4948, Batch loss: 0.00015138165326789021\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 800/4948, Batch loss: 0.0001296262926189229\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 900/4948, Batch loss: 0.0005276236915960908\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 1000/4948, Batch loss: 8.648345101391897e-05\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 1100/4948, Batch loss: 0.0008880896493792534\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 1200/4948, Batch loss: 0.0017666618805378675\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 1300/4948, Batch loss: 0.00015365966828539968\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 1400/4948, Batch loss: 0.00012282152601983398\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 1500/4948, Batch loss: 0.00012499591684900224\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 1600/4948, Batch loss: 0.0002998720156028867\n",
      "Memory Usage: 1751.55 MB\n",
      "Epoch 6, Batch 1700/4948, Batch loss: 0.0005702677881345153\n",
      "Memory Usage: 1752.02 MB\n",
      "Epoch 6, Batch 1800/4948, Batch loss: 0.00027885587769560516\n",
      "Memory Usage: 1752.02 MB\n",
      "Epoch 6, Batch 1900/4948, Batch loss: 0.0015849183546379209\n",
      "Memory Usage: 1752.02 MB\n",
      "Epoch 6, Batch 2000/4948, Batch loss: 0.0006367304013110697\n",
      "Memory Usage: 1752.02 MB\n",
      "Epoch 6, Batch 2100/4948, Batch loss: 0.000875805679243058\n",
      "Memory Usage: 1752.02 MB\n",
      "Epoch 6, Batch 2200/4948, Batch loss: 0.00011168325727339834\n",
      "Memory Usage: 1752.80 MB\n",
      "Epoch 6, Batch 2300/4948, Batch loss: 0.00022391635866370052\n",
      "Memory Usage: 1752.80 MB\n",
      "Epoch 6, Batch 2400/4948, Batch loss: 0.00010580940579529852\n",
      "Memory Usage: 1752.80 MB\n",
      "Epoch 6, Batch 2500/4948, Batch loss: 4.893835284747183e-05\n",
      "Memory Usage: 1752.80 MB\n",
      "Epoch 6, Batch 2600/4948, Batch loss: 0.00039662394556216896\n",
      "Memory Usage: 1752.80 MB\n",
      "Epoch 6, Batch 2700/4948, Batch loss: 0.0004674768715631217\n",
      "Memory Usage: 1752.80 MB\n",
      "Epoch 6, Batch 2800/4948, Batch loss: 0.0016177187208086252\n",
      "Memory Usage: 1752.80 MB\n",
      "Epoch 6, Batch 2900/4948, Batch loss: 0.00021172163542360067\n",
      "Memory Usage: 1752.80 MB\n",
      "Epoch 6, Batch 3000/4948, Batch loss: 6.581303023267537e-05\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 3100/4948, Batch loss: 0.00012754410272464156\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 3200/4948, Batch loss: 0.000317361147608608\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 3300/4948, Batch loss: 0.0001424885558662936\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 3400/4948, Batch loss: 5.911176685913233e-06\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 3500/4948, Batch loss: 0.0005808277055621147\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 3600/4948, Batch loss: 4.065046232426539e-05\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 3700/4948, Batch loss: 0.0006472765235230327\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 3800/4948, Batch loss: 0.00011652393732219934\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 3900/4948, Batch loss: 0.00013717600086238235\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 4000/4948, Batch loss: 0.00015261130465660244\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 4100/4948, Batch loss: 4.5057782699586824e-05\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 4200/4948, Batch loss: 0.0010548236314207315\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 4300/4948, Batch loss: 0.00014521759294439107\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 4400/4948, Batch loss: 7.820979590178467e-06\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 4500/4948, Batch loss: 0.0010462311329320073\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 4600/4948, Batch loss: 0.0002592101227492094\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 4700/4948, Batch loss: 0.00045292210415937006\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 4800/4948, Batch loss: 0.00011545968300197273\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 4900/4948, Batch loss: 7.494456804124638e-05\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6, Batch 4948/4948, Batch loss: 0.0014518584357574582\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 6 completed in 72.64 seconds, Total Training Loss: 0.00033892459829885094\n",
      "\n",
      "Epoch 7/100\n",
      "Epoch 7, Batch 100/4948, Batch loss: 0.00017185957403853536\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 7, Batch 200/4948, Batch loss: 2.7155807401868515e-05\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 7, Batch 300/4948, Batch loss: 0.0003422092122491449\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 7, Batch 400/4948, Batch loss: 0.000121317345474381\n",
      "Memory Usage: 1752.81 MB\n",
      "Epoch 7, Batch 500/4948, Batch loss: 5.7199285947717726e-05\n",
      "Memory Usage: 1752.83 MB\n",
      "Epoch 7, Batch 600/4948, Batch loss: 0.00046764803119003773\n",
      "Memory Usage: 1752.83 MB\n",
      "Epoch 7, Batch 700/4948, Batch loss: 8.961513958638534e-05\n",
      "Memory Usage: 1752.83 MB\n",
      "Epoch 7, Batch 800/4948, Batch loss: 0.0001484233798692003\n",
      "Memory Usage: 1752.83 MB\n",
      "Epoch 7, Batch 900/4948, Batch loss: 0.0005294025759212673\n",
      "Memory Usage: 1577.84 MB\n",
      "Epoch 7, Batch 1000/4948, Batch loss: 8.949595940066501e-05\n",
      "Memory Usage: 1412.50 MB\n",
      "Epoch 7, Batch 1100/4948, Batch loss: 0.0008796199108473957\n",
      "Memory Usage: 1365.77 MB\n",
      "Epoch 7, Batch 1200/4948, Batch loss: 0.0017580637941136956\n",
      "Memory Usage: 1366.88 MB\n",
      "Epoch 7, Batch 1300/4948, Batch loss: 0.0001508867571828887\n",
      "Memory Usage: 1366.88 MB\n",
      "Epoch 7, Batch 1400/4948, Batch loss: 0.00012367722229100764\n",
      "Memory Usage: 1366.88 MB\n",
      "Epoch 7, Batch 1500/4948, Batch loss: 0.00014706363435834646\n",
      "Memory Usage: 1366.88 MB\n",
      "Epoch 7, Batch 1600/4948, Batch loss: 9.128705278271809e-05\n",
      "Memory Usage: 1366.89 MB\n",
      "Epoch 7, Batch 1700/4948, Batch loss: 0.0024760104715824127\n",
      "Memory Usage: 1367.95 MB\n",
      "Epoch 7, Batch 1800/4948, Batch loss: 0.0002734087756834924\n",
      "Memory Usage: 1367.98 MB\n",
      "Epoch 7, Batch 1900/4948, Batch loss: 0.007970882579684258\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 2000/4948, Batch loss: 0.000525407085660845\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 2100/4948, Batch loss: 0.0008794780587777495\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 2200/4948, Batch loss: 0.00010887378448387608\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 2300/4948, Batch loss: 0.00023202689772006124\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 2400/4948, Batch loss: 0.00010782143363030627\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 2500/4948, Batch loss: 4.9365484301233664e-05\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 2600/4948, Batch loss: 0.0004019110929220915\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 2700/4948, Batch loss: 0.0004521962837316096\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 2800/4948, Batch loss: 0.0016336176777258515\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 2900/4948, Batch loss: 0.00020011425658594817\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 3000/4948, Batch loss: 5.813985990243964e-05\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 3100/4948, Batch loss: 0.00012480672739911824\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 3200/4948, Batch loss: 0.0003172060241922736\n",
      "Memory Usage: 1368.02 MB\n",
      "Epoch 7, Batch 3300/4948, Batch loss: 0.00014206854393705726\n",
      "Memory Usage: 1369.73 MB\n",
      "Epoch 7, Batch 3400/4948, Batch loss: 9.957435395335779e-06\n",
      "Memory Usage: 1369.75 MB\n",
      "Epoch 7, Batch 3500/4948, Batch loss: 4.500804789131507e-05\n",
      "Memory Usage: 1369.77 MB\n",
      "Epoch 7, Batch 3600/4948, Batch loss: 4.396478834678419e-05\n",
      "Memory Usage: 1369.78 MB\n",
      "Epoch 7, Batch 3700/4948, Batch loss: 0.0005518647376447916\n",
      "Memory Usage: 1369.78 MB\n",
      "Epoch 7, Batch 3800/4948, Batch loss: 0.00010800240852404386\n",
      "Memory Usage: 1369.78 MB\n",
      "Epoch 7, Batch 3900/4948, Batch loss: 0.00013737802510149777\n",
      "Memory Usage: 1370.44 MB\n",
      "Epoch 7, Batch 4000/4948, Batch loss: 0.00015328660083469003\n",
      "Memory Usage: 1370.44 MB\n",
      "Epoch 7, Batch 4100/4948, Batch loss: 4.516097033047117e-05\n",
      "Memory Usage: 1370.44 MB\n",
      "Epoch 7, Batch 4200/4948, Batch loss: 0.0010533634340390563\n",
      "Memory Usage: 1370.91 MB\n",
      "Epoch 7, Batch 4300/4948, Batch loss: 0.0001450636045774445\n",
      "Memory Usage: 1370.92 MB\n",
      "Epoch 7, Batch 4400/4948, Batch loss: 1.0117816600541119e-05\n",
      "Memory Usage: 1370.92 MB\n",
      "Epoch 7, Batch 4500/4948, Batch loss: 0.0010536967311054468\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 7, Batch 4600/4948, Batch loss: 0.0002639796875882894\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 7, Batch 4700/4948, Batch loss: 0.0004500554932747036\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 7, Batch 4800/4948, Batch loss: 0.00012267607962712646\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 7, Batch 4900/4948, Batch loss: 7.360984454862773e-05\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 7, Batch 4948/4948, Batch loss: 0.0014319514157250524\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 7 completed in 68.67 seconds, Total Training Loss: 0.0003805879586125362\n",
      "\n",
      "Epoch 8/100\n",
      "Epoch 8, Batch 100/4948, Batch loss: 0.00017185673641506582\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 200/4948, Batch loss: 7.285689207492396e-05\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 300/4948, Batch loss: 0.000340647908160463\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 400/4948, Batch loss: 0.00012217192852403969\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 500/4948, Batch loss: 5.5562984925927594e-05\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 600/4948, Batch loss: 0.00047091810847632587\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 700/4948, Batch loss: 6.878991553094238e-05\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 800/4948, Batch loss: 0.00012536357098724693\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 900/4948, Batch loss: 0.000526779389474541\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 1000/4948, Batch loss: 8.615368278697133e-05\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 1100/4948, Batch loss: 0.0009156978339888155\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 1200/4948, Batch loss: 0.0017643403261899948\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 1300/4948, Batch loss: 0.00015035882825031877\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 1400/4948, Batch loss: 0.00012240337673574686\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 1500/4948, Batch loss: 0.00014529400505125523\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 1600/4948, Batch loss: 4.9501755711389706e-05\n",
      "Memory Usage: 1371.48 MB\n",
      "Epoch 8, Batch 1700/4948, Batch loss: 0.0003899313451256603\n",
      "Memory Usage: 1371.98 MB\n",
      "Epoch 8, Batch 1800/4948, Batch loss: 0.0002730788546614349\n",
      "Memory Usage: 1372.00 MB\n",
      "Epoch 8, Batch 1900/4948, Batch loss: 0.0010974069591611624\n",
      "Memory Usage: 1372.00 MB\n",
      "Epoch 8, Batch 2000/4948, Batch loss: 0.0005687690572813153\n",
      "Memory Usage: 1372.00 MB\n",
      "Epoch 8, Batch 2100/4948, Batch loss: 0.0007479911437258124\n",
      "Memory Usage: 1372.00 MB\n",
      "Epoch 8, Batch 2200/4948, Batch loss: 0.00010657182428985834\n",
      "Memory Usage: 1372.02 MB\n",
      "Epoch 8, Batch 2300/4948, Batch loss: 0.000223766197450459\n",
      "Memory Usage: 1372.02 MB\n",
      "Epoch 8, Batch 2400/4948, Batch loss: 0.00010661291162250564\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 2500/4948, Batch loss: 4.86252574773971e-05\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 2600/4948, Batch loss: 0.00039297586772590876\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 2700/4948, Batch loss: 0.0004821410693693906\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 2800/4948, Batch loss: 0.0016027395613491535\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 2900/4948, Batch loss: 0.0002028784656431526\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 3000/4948, Batch loss: 6.101897815824486e-05\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 3100/4948, Batch loss: 0.00013668346218764782\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 3200/4948, Batch loss: 0.000337637378834188\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 3300/4948, Batch loss: 0.0001454839075449854\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 3400/4948, Batch loss: 5.057380803918932e-06\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 3500/4948, Batch loss: 0.0003323330311104655\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 3600/4948, Batch loss: 3.984568320447579e-05\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 3700/4948, Batch loss: 0.0005983550800010562\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 3800/4948, Batch loss: 0.00011087027087341994\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 3900/4948, Batch loss: 0.00013599134399555624\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 4000/4948, Batch loss: 0.00015268981223925948\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 4100/4948, Batch loss: 4.979887307854369e-05\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 4200/4948, Batch loss: 0.0010560012888163328\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 4300/4948, Batch loss: 0.00014450695016421378\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 4400/4948, Batch loss: 6.3467255131399725e-06\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 4500/4948, Batch loss: 0.0010461005149409175\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 4600/4948, Batch loss: 0.00025920456391759217\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 4700/4948, Batch loss: 0.000449618004495278\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 4800/4948, Batch loss: 0.00012104808411095291\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 4900/4948, Batch loss: 7.561934035038576e-05\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8, Batch 4948/4948, Batch loss: 0.001406382885761559\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 8 completed in 67.78 seconds, Total Training Loss: 0.00032958961997668195\n",
      "\n",
      "Epoch 9/100\n",
      "Epoch 9, Batch 100/4948, Batch loss: 0.0001710949291009456\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 200/4948, Batch loss: 5.597733616014011e-05\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 300/4948, Batch loss: 0.0003429177450016141\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 400/4948, Batch loss: 0.00012120504106860608\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 500/4948, Batch loss: 5.489205432240851e-05\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 600/4948, Batch loss: 0.00046928442316129804\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 700/4948, Batch loss: 8.68884235387668e-05\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 800/4948, Batch loss: 0.00012487730418797582\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 900/4948, Batch loss: 0.0005304152145981789\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 1000/4948, Batch loss: 8.692465053172782e-05\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 1100/4948, Batch loss: 0.0009045946644619107\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 1200/4948, Batch loss: 0.0017469307640567422\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 1300/4948, Batch loss: 0.00014994626690167934\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 1400/4948, Batch loss: 0.00012365610746201128\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 1500/4948, Batch loss: 0.00017351467977277935\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 1600/4948, Batch loss: 1.3658241186931264e-05\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 1700/4948, Batch loss: 0.001354277366772294\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 1800/4948, Batch loss: 0.00027111906092613935\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 1900/4948, Batch loss: 0.009310727007687092\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 2000/4948, Batch loss: 0.0005835907650180161\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 2100/4948, Batch loss: 0.0009874349925667048\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 2200/4948, Batch loss: 0.00010930162534350529\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 2300/4948, Batch loss: 0.00022248698223847896\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 2400/4948, Batch loss: 0.00010789427324198186\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 2500/4948, Batch loss: 5.003910337109119e-05\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 2600/4948, Batch loss: 0.0003968293312937021\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 2700/4948, Batch loss: 0.00044633474317379296\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 2800/4948, Batch loss: 0.0016224532155320048\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 2900/4948, Batch loss: 0.00019930511189159006\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 3000/4948, Batch loss: 5.8537876611808315e-05\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 3100/4948, Batch loss: 0.00012499403965193778\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 3200/4948, Batch loss: 0.00031798449344933033\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 3300/4948, Batch loss: 0.00014126436144579202\n",
      "Memory Usage: 1372.03 MB\n",
      "Epoch 9, Batch 3400/4948, Batch loss: 8.523787073499989e-06\n",
      "Memory Usage: 1372.44 MB\n",
      "Epoch 9, Batch 3500/4948, Batch loss: 4.71432795166038e-05\n",
      "Memory Usage: 1372.44 MB\n",
      "Epoch 9, Batch 3600/4948, Batch loss: 4.1700161091284826e-05\n",
      "Memory Usage: 1372.44 MB\n",
      "Epoch 9, Batch 3700/4948, Batch loss: 0.0005457248189486563\n",
      "Memory Usage: 1372.44 MB\n",
      "Epoch 9, Batch 3800/4948, Batch loss: 0.00010643379209795967\n",
      "Memory Usage: 1372.44 MB\n",
      "Epoch 9, Batch 3900/4948, Batch loss: 0.0001354988053208217\n",
      "Memory Usage: 1372.45 MB\n",
      "Epoch 9, Batch 4000/4948, Batch loss: 0.0001531318121124059\n",
      "Memory Usage: 1372.45 MB\n",
      "Epoch 9, Batch 4100/4948, Batch loss: 4.738939969683997e-05\n",
      "Memory Usage: 1372.45 MB\n",
      "Epoch 9, Batch 4200/4948, Batch loss: 0.001051866915076971\n",
      "Memory Usage: 1372.45 MB\n",
      "Epoch 9, Batch 4300/4948, Batch loss: 0.00014414270117413253\n",
      "Memory Usage: 1372.45 MB\n",
      "Epoch 9, Batch 4400/4948, Batch loss: 5.03201272294973e-06\n",
      "Memory Usage: 1372.45 MB\n",
      "Epoch 9, Batch 4500/4948, Batch loss: 0.0010520226787775755\n",
      "Memory Usage: 1372.45 MB\n",
      "Epoch 9, Batch 4600/4948, Batch loss: 0.0002699558681342751\n",
      "Memory Usage: 1372.48 MB\n",
      "Epoch 9, Batch 4700/4948, Batch loss: 0.0004488768463488668\n",
      "Memory Usage: 1088.30 MB\n",
      "Epoch 9, Batch 4800/4948, Batch loss: 0.0001199983453261666\n",
      "Memory Usage: 1084.02 MB\n",
      "Epoch 9, Batch 4900/4948, Batch loss: 7.080748036969453e-05\n",
      "Memory Usage: 1089.38 MB\n",
      "Epoch 9, Batch 4948/4948, Batch loss: 0.0013510448625311255\n",
      "Memory Usage: 1091.27 MB\n",
      "Epoch 9 completed in 65.61 seconds, Total Training Loss: 0.0003727023565703674\n",
      "\n",
      "Epoch 10/100\n",
      "Epoch 10, Batch 100/4948, Batch loss: 0.0001712591911200434\n",
      "Memory Usage: 1045.61 MB\n",
      "Epoch 10, Batch 200/4948, Batch loss: 4.331014497438446e-05\n",
      "Memory Usage: 1048.95 MB\n",
      "Epoch 10, Batch 300/4948, Batch loss: 0.0003383622388355434\n",
      "Memory Usage: 1050.36 MB\n",
      "Epoch 10, Batch 400/4948, Batch loss: 0.00011990345228696242\n",
      "Memory Usage: 1051.02 MB\n",
      "Epoch 10, Batch 500/4948, Batch loss: 5.789001806988381e-05\n",
      "Memory Usage: 1051.05 MB\n",
      "Epoch 10, Batch 600/4948, Batch loss: 0.00047097369679249823\n",
      "Memory Usage: 1051.05 MB\n",
      "Epoch 10, Batch 700/4948, Batch loss: 6.43172170384787e-05\n",
      "Memory Usage: 1051.72 MB\n",
      "Epoch 10, Batch 800/4948, Batch loss: 0.00012636084284167737\n",
      "Memory Usage: 1053.03 MB\n",
      "Epoch 10, Batch 900/4948, Batch loss: 0.000566289178095758\n",
      "Memory Usage: 1053.78 MB\n",
      "Epoch 10, Batch 1000/4948, Batch loss: 8.665346831548959e-05\n",
      "Memory Usage: 1054.83 MB\n",
      "Epoch 10, Batch 1100/4948, Batch loss: 0.000896579003892839\n",
      "Memory Usage: 1054.83 MB\n",
      "Epoch 10, Batch 1200/4948, Batch loss: 0.0017610883805900812\n",
      "Memory Usage: 1055.33 MB\n",
      "Epoch 10, Batch 1300/4948, Batch loss: 0.00015450066712219268\n",
      "Memory Usage: 1055.33 MB\n",
      "Epoch 10, Batch 1400/4948, Batch loss: 0.000123967562103644\n",
      "Memory Usage: 1056.33 MB\n",
      "Epoch 10, Batch 1500/4948, Batch loss: 0.00022181014355737716\n",
      "Memory Usage: 1056.33 MB\n",
      "Epoch 10, Batch 1600/4948, Batch loss: 0.00013956456677988172\n",
      "Memory Usage: 1056.77 MB\n",
      "Epoch 10, Batch 1700/4948, Batch loss: 0.0006545253563672304\n",
      "Memory Usage: 1056.77 MB\n",
      "Epoch 10, Batch 1800/4948, Batch loss: 0.0002715701994020492\n",
      "Memory Usage: 1056.77 MB\n",
      "Epoch 10, Batch 1900/4948, Batch loss: 0.008170169778168201\n",
      "Memory Usage: 1056.77 MB\n",
      "Epoch 10, Batch 2000/4948, Batch loss: 0.00048657943261787295\n",
      "Memory Usage: 1056.77 MB\n",
      "Epoch 10, Batch 2100/4948, Batch loss: 0.0008603931637480855\n",
      "Memory Usage: 1056.77 MB\n",
      "Epoch 10, Batch 2200/4948, Batch loss: 0.00010835877765202895\n",
      "Memory Usage: 1056.77 MB\n",
      "Epoch 10, Batch 2300/4948, Batch loss: 0.0002276005980093032\n",
      "Memory Usage: 1056.77 MB\n",
      "Epoch 10, Batch 2400/4948, Batch loss: 0.00010519072384340689\n",
      "Memory Usage: 1056.77 MB\n",
      "Epoch 10, Batch 2500/4948, Batch loss: 4.9487396609038115e-05\n",
      "Memory Usage: 1056.77 MB\n",
      "Epoch 10, Batch 2600/4948, Batch loss: 0.0003933381231036037\n",
      "Memory Usage: 1057.11 MB\n",
      "Epoch 10, Batch 2700/4948, Batch loss: 0.00043420158908702433\n",
      "Memory Usage: 1057.11 MB\n",
      "Epoch 10, Batch 2800/4948, Batch loss: 0.0016175740165635943\n",
      "Memory Usage: 1057.11 MB\n",
      "Epoch 10, Batch 2900/4948, Batch loss: 0.0002002649416681379\n",
      "Memory Usage: 1057.19 MB\n",
      "Epoch 10, Batch 3000/4948, Batch loss: 5.80495543545112e-05\n",
      "Memory Usage: 1057.19 MB\n",
      "Epoch 10, Batch 3100/4948, Batch loss: 0.00012454381794668734\n",
      "Memory Usage: 1057.25 MB\n",
      "Epoch 10, Batch 3200/4948, Batch loss: 0.0003135906590614468\n",
      "Memory Usage: 1057.25 MB\n",
      "Epoch 10, Batch 3300/4948, Batch loss: 0.00014187714259605855\n",
      "Memory Usage: 1057.25 MB\n",
      "Epoch 10, Batch 3400/4948, Batch loss: 1.0279195521434303e-05\n",
      "Memory Usage: 1057.25 MB\n",
      "Epoch 10, Batch 3500/4948, Batch loss: 4.5490141928894445e-05\n",
      "Memory Usage: 1057.81 MB\n",
      "Epoch 10, Batch 3600/4948, Batch loss: 4.347486901679076e-05\n",
      "Memory Usage: 1057.81 MB\n",
      "Epoch 10, Batch 3700/4948, Batch loss: 0.0005508421454578638\n",
      "Memory Usage: 1058.12 MB\n",
      "Epoch 10, Batch 3800/4948, Batch loss: 0.00010657301754690707\n",
      "Memory Usage: 1058.12 MB\n",
      "Epoch 10, Batch 3900/4948, Batch loss: 0.00013723842857871205\n",
      "Memory Usage: 1058.12 MB\n",
      "Epoch 10, Batch 4000/4948, Batch loss: 0.0001539605436846614\n",
      "Memory Usage: 1058.62 MB\n",
      "Epoch 10, Batch 4100/4948, Batch loss: 4.5163793402025476e-05\n",
      "Memory Usage: 1058.62 MB\n",
      "Epoch 10, Batch 4200/4948, Batch loss: 0.0010549549479037523\n",
      "Memory Usage: 1058.62 MB\n",
      "Epoch 10, Batch 4300/4948, Batch loss: 0.00014397922495845705\n",
      "Memory Usage: 1058.62 MB\n",
      "Epoch 10, Batch 4400/4948, Batch loss: 1.0317684427718632e-05\n",
      "Memory Usage: 1058.62 MB\n",
      "Epoch 10, Batch 4500/4948, Batch loss: 0.0010430376278236508\n",
      "Memory Usage: 1058.62 MB\n",
      "Epoch 10, Batch 4600/4948, Batch loss: 0.00028746583848260343\n",
      "Memory Usage: 1058.62 MB\n",
      "Epoch 10, Batch 4700/4948, Batch loss: 0.00045223801862448454\n",
      "Memory Usage: 1059.19 MB\n",
      "Epoch 10, Batch 4800/4948, Batch loss: 0.00011671798711176962\n",
      "Memory Usage: 1059.19 MB\n",
      "Epoch 10, Batch 4900/4948, Batch loss: 7.500861829612404e-05\n",
      "Memory Usage: 1059.19 MB\n",
      "Epoch 10, Batch 4948/4948, Batch loss: 0.0013379986630752683\n",
      "Memory Usage: 1059.19 MB\n",
      "Epoch 10 completed in 65.48 seconds, Total Training Loss: 0.00035285778148490087\n",
      "\n",
      "Epoch 11/100\n",
      "Epoch 11, Batch 100/4948, Batch loss: 0.00017069523164536804\n",
      "Memory Usage: 1059.19 MB\n",
      "Epoch 11, Batch 200/4948, Batch loss: 5.8205037930747494e-05\n",
      "Memory Usage: 1059.19 MB\n",
      "Epoch 11, Batch 300/4948, Batch loss: 0.00033774264738895\n",
      "Memory Usage: 1059.19 MB\n",
      "Epoch 11, Batch 400/4948, Batch loss: 0.00012048437929479405\n",
      "Memory Usage: 1059.69 MB\n",
      "Epoch 11, Batch 500/4948, Batch loss: 5.8312176406616345e-05\n",
      "Memory Usage: 1059.69 MB\n",
      "Epoch 11, Batch 600/4948, Batch loss: 0.00047351146349683404\n",
      "Memory Usage: 1059.69 MB\n",
      "Epoch 11, Batch 700/4948, Batch loss: 6.03533408138901e-05\n",
      "Memory Usage: 1059.69 MB\n",
      "Epoch 11, Batch 800/4948, Batch loss: 0.0001259295822819695\n",
      "Memory Usage: 1059.69 MB\n",
      "Epoch 11, Batch 900/4948, Batch loss: 0.0005688099190592766\n",
      "Memory Usage: 1060.62 MB\n",
      "Epoch 11, Batch 1000/4948, Batch loss: 8.650909148855135e-05\n",
      "Memory Usage: 1060.62 MB\n",
      "Epoch 11, Batch 1100/4948, Batch loss: 0.0008943152497522533\n",
      "Memory Usage: 1060.62 MB\n",
      "Epoch 11, Batch 1200/4948, Batch loss: 0.001751045580022037\n",
      "Memory Usage: 1060.62 MB\n",
      "Epoch 11, Batch 1300/4948, Batch loss: 0.00015820653061382473\n",
      "Memory Usage: 1060.62 MB\n",
      "Epoch 11, Batch 1400/4948, Batch loss: 0.00012245008838362992\n",
      "Memory Usage: 1060.62 MB\n",
      "Epoch 11, Batch 1500/4948, Batch loss: 0.00018880292191170156\n",
      "Memory Usage: 1061.03 MB\n",
      "Epoch 11, Batch 1600/4948, Batch loss: 1.4030322745384183e-05\n",
      "Memory Usage: 1061.03 MB\n",
      "Epoch 11, Batch 1700/4948, Batch loss: 0.00093269586795941\n",
      "Memory Usage: 1061.03 MB\n",
      "Epoch 11, Batch 1800/4948, Batch loss: 0.00027144289924763143\n",
      "Memory Usage: 1061.03 MB\n",
      "Epoch 11, Batch 1900/4948, Batch loss: 0.008716688491404057\n",
      "Memory Usage: 1061.03 MB\n",
      "Epoch 11, Batch 2000/4948, Batch loss: 0.0006533597479574382\n",
      "Memory Usage: 1061.03 MB\n",
      "Epoch 11, Batch 2100/4948, Batch loss: 0.0009852233342826366\n",
      "Memory Usage: 1061.03 MB\n",
      "Epoch 11, Batch 2200/4948, Batch loss: 0.00010782048047985882\n",
      "Memory Usage: 1061.03 MB\n",
      "Epoch 11, Batch 2300/4948, Batch loss: 0.00022538956545758992\n",
      "Memory Usage: 1061.03 MB\n",
      "Epoch 11, Batch 2400/4948, Batch loss: 0.00010806502541527152\n",
      "Memory Usage: 1061.03 MB\n",
      "Epoch 11, Batch 2500/4948, Batch loss: 4.893175355391577e-05\n",
      "Memory Usage: 1061.03 MB\n",
      "Epoch 11, Batch 2600/4948, Batch loss: 0.00039573461981490254\n",
      "Memory Usage: 1061.78 MB\n",
      "Epoch 11, Batch 2700/4948, Batch loss: 0.00042941904393956065\n",
      "Memory Usage: 1061.78 MB\n",
      "Epoch 11, Batch 2800/4948, Batch loss: 0.0016164066037163138\n",
      "Memory Usage: 1061.78 MB\n",
      "Epoch 11, Batch 2900/4948, Batch loss: 0.00019878389139194041\n",
      "Memory Usage: 1061.80 MB\n",
      "Epoch 11, Batch 3000/4948, Batch loss: 5.874535781913437e-05\n",
      "Memory Usage: 1061.80 MB\n",
      "Epoch 11, Batch 3100/4948, Batch loss: 0.0001304071774939075\n",
      "Memory Usage: 1061.80 MB\n",
      "Epoch 11, Batch 3200/4948, Batch loss: 0.0003262250393163413\n",
      "Memory Usage: 1061.86 MB\n",
      "Epoch 11, Batch 3300/4948, Batch loss: 0.0001409326505381614\n",
      "Memory Usage: 1061.86 MB\n",
      "Epoch 11, Batch 3400/4948, Batch loss: 1.069408972398378e-05\n",
      "Memory Usage: 1061.86 MB\n",
      "Epoch 11, Batch 3500/4948, Batch loss: 4.7509594878647476e-05\n",
      "Memory Usage: 1061.86 MB\n",
      "Epoch 11, Batch 3600/4948, Batch loss: 4.0195278415922076e-05\n",
      "Memory Usage: 1061.86 MB\n",
      "Epoch 11, Batch 3700/4948, Batch loss: 0.0005405751289799809\n",
      "Memory Usage: 1061.86 MB\n",
      "Epoch 11, Batch 3800/4948, Batch loss: 0.0001067788471118547\n",
      "Memory Usage: 1061.86 MB\n",
      "Epoch 11, Batch 3900/4948, Batch loss: 0.00013496562314685434\n",
      "Memory Usage: 1061.86 MB\n",
      "Epoch 11, Batch 4000/4948, Batch loss: 0.0001520855148555711\n",
      "Memory Usage: 1061.86 MB\n",
      "Epoch 11, Batch 4100/4948, Batch loss: 5.722607602365315e-05\n",
      "Memory Usage: 1061.86 MB\n",
      "Epoch 11, Batch 4200/4948, Batch loss: 0.0010562234092503786\n",
      "Memory Usage: 1061.88 MB\n",
      "Epoch 11, Batch 4300/4948, Batch loss: 0.00014381132496055216\n",
      "Memory Usage: 1061.88 MB\n",
      "Epoch 11, Batch 4400/4948, Batch loss: 5.9548251556407195e-06\n",
      "Memory Usage: 1061.88 MB\n",
      "Epoch 11, Batch 4500/4948, Batch loss: 0.0010509619023650885\n",
      "Memory Usage: 1061.88 MB\n",
      "Epoch 11, Batch 4600/4948, Batch loss: 0.00026807640097104013\n",
      "Memory Usage: 1061.88 MB\n",
      "Epoch 11, Batch 4700/4948, Batch loss: 0.0004489339189603925\n",
      "Memory Usage: 1061.88 MB\n",
      "Epoch 11, Batch 4800/4948, Batch loss: 0.00011103289580205455\n",
      "Memory Usage: 1061.88 MB\n",
      "Epoch 11, Batch 4900/4948, Batch loss: 7.146500138333067e-05\n",
      "Memory Usage: 1061.88 MB\n",
      "Epoch 11, Batch 4948/4948, Batch loss: 0.001309555722400546\n",
      "Memory Usage: 1061.88 MB\n",
      "Epoch 11 completed in 66.98 seconds, Total Training Loss: 0.00036199926023372286\n",
      "Validation completed in 3.49 seconds, Average Validation Loss: 0.0005218547129223703\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 12/100\n",
      "Epoch 12, Batch 100/4948, Batch loss: 0.00017053601914085448\n",
      "Memory Usage: 1097.84 MB\n",
      "Epoch 12, Batch 200/4948, Batch loss: 4.85101954836864e-05\n",
      "Memory Usage: 1097.89 MB\n",
      "Epoch 12, Batch 300/4948, Batch loss: 0.0003410468634683639\n",
      "Memory Usage: 1097.89 MB\n",
      "Epoch 12, Batch 400/4948, Batch loss: 0.00012016862456221133\n",
      "Memory Usage: 1097.89 MB\n",
      "Epoch 12, Batch 500/4948, Batch loss: 5.375179534894414e-05\n",
      "Memory Usage: 1097.89 MB\n",
      "Epoch 12, Batch 600/4948, Batch loss: 0.0004765389021486044\n",
      "Memory Usage: 1097.89 MB\n",
      "Epoch 12, Batch 700/4948, Batch loss: 5.948244870523922e-05\n",
      "Memory Usage: 1097.89 MB\n",
      "Epoch 12, Batch 800/4948, Batch loss: 0.00013207858137320727\n",
      "Memory Usage: 1097.89 MB\n",
      "Epoch 12, Batch 900/4948, Batch loss: 0.0005775137105956674\n",
      "Memory Usage: 1097.91 MB\n",
      "Epoch 12, Batch 1000/4948, Batch loss: 8.577747212257236e-05\n",
      "Memory Usage: 1097.91 MB\n",
      "Epoch 12, Batch 1100/4948, Batch loss: 0.0009000981808640063\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 1200/4948, Batch loss: 0.0017339043552055955\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 1300/4948, Batch loss: 0.00016026932280510664\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 1400/4948, Batch loss: 0.0001232135109603405\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 1500/4948, Batch loss: 0.00014580854622181505\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 1600/4948, Batch loss: 2.6294799681636505e-05\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 1700/4948, Batch loss: 0.0004542214737739414\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 1800/4948, Batch loss: 0.00026986972079612315\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 1900/4948, Batch loss: 0.0069208224304020405\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 2000/4948, Batch loss: 0.00045670976396650076\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 2100/4948, Batch loss: 0.0007846666849218309\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 2200/4948, Batch loss: 0.00010753080277936533\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 2300/4948, Batch loss: 0.00022960532805882394\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 2400/4948, Batch loss: 0.00010594324703561142\n",
      "Memory Usage: 1097.92 MB\n",
      "Epoch 12, Batch 2500/4948, Batch loss: 4.88866789964959e-05\n",
      "Memory Usage: 1097.94 MB\n",
      "Epoch 12, Batch 2600/4948, Batch loss: 0.0003910195955540985\n",
      "Memory Usage: 1097.94 MB\n",
      "Epoch 12, Batch 2700/4948, Batch loss: 0.00041649845661595464\n",
      "Memory Usage: 1097.94 MB\n",
      "Epoch 12, Batch 2800/4948, Batch loss: 0.001607951708137989\n",
      "Memory Usage: 1097.94 MB\n",
      "Epoch 12, Batch 2900/4948, Batch loss: 0.0001989560987567529\n",
      "Memory Usage: 1097.94 MB\n",
      "Epoch 12, Batch 3000/4948, Batch loss: 5.80046144023072e-05\n",
      "Memory Usage: 1097.94 MB\n",
      "Epoch 12, Batch 3100/4948, Batch loss: 0.00013411187683232129\n",
      "Memory Usage: 1097.94 MB\n",
      "Epoch 12, Batch 3200/4948, Batch loss: 0.0003121471672784537\n",
      "Memory Usage: 1097.94 MB\n",
      "Epoch 12, Batch 3300/4948, Batch loss: 0.00014072665362618864\n",
      "Memory Usage: 1097.94 MB\n",
      "Epoch 12, Batch 3400/4948, Batch loss: 1.1710311810020357e-05\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 3500/4948, Batch loss: 4.6233075408963487e-05\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 3600/4948, Batch loss: 4.0783852455206215e-05\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 3700/4948, Batch loss: 0.0005548904300667346\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 3800/4948, Batch loss: 0.00010632836347213015\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 3900/4948, Batch loss: 0.00013451203994918615\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 4000/4948, Batch loss: 0.00015270656149368733\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 4100/4948, Batch loss: 5.6833650887710974e-05\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 4200/4948, Batch loss: 0.0010503982193768024\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 4300/4948, Batch loss: 0.0001438189356122166\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 4400/4948, Batch loss: 5.632879037875682e-06\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 4500/4948, Batch loss: 0.0010533271124586463\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 4600/4948, Batch loss: 0.0002650996029842645\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 4700/4948, Batch loss: 0.0004475515161175281\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 4800/4948, Batch loss: 0.00011366289254510775\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 4900/4948, Batch loss: 7.388266385532916e-05\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12, Batch 4948/4948, Batch loss: 0.0013166755670681596\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 12 completed in 75.76 seconds, Total Training Loss: 0.00034266529311610374\n",
      "\n",
      "Epoch 13/100\n",
      "Epoch 13, Batch 100/4948, Batch loss: 0.00017016548372339457\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 13, Batch 200/4948, Batch loss: 5.5675453040748835e-05\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 13, Batch 300/4948, Batch loss: 0.000335301854647696\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 13, Batch 400/4948, Batch loss: 0.00011928458843613043\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 13, Batch 500/4948, Batch loss: 5.641032475978136e-05\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 13, Batch 600/4948, Batch loss: 0.0004766479250974953\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 13, Batch 700/4948, Batch loss: 5.7759865740081295e-05\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 13, Batch 800/4948, Batch loss: 0.00012489079381339252\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 13, Batch 900/4948, Batch loss: 0.0005674133426509798\n",
      "Memory Usage: 1097.95 MB\n",
      "Epoch 13, Batch 1000/4948, Batch loss: 8.696698932908475e-05\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 1100/4948, Batch loss: 0.000894074619282037\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 1200/4948, Batch loss: 0.0017454226035624743\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 1300/4948, Batch loss: 0.0001597370719537139\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 1400/4948, Batch loss: 0.00012290533049963415\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 1500/4948, Batch loss: 0.00015336908109020442\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 1600/4948, Batch loss: 1.3227521776570939e-05\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 1700/4948, Batch loss: 0.0008563575684092939\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 1800/4948, Batch loss: 0.00026883301325142384\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 1900/4948, Batch loss: 0.007298783399164677\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 2000/4948, Batch loss: 0.0005603861063718796\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 2100/4948, Batch loss: 0.0008497084490954876\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 2200/4948, Batch loss: 0.00010858067980734631\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 2300/4948, Batch loss: 0.0002228252706117928\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 2400/4948, Batch loss: 0.00010735404066508636\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 2500/4948, Batch loss: 4.8422465624753386e-05\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 2600/4948, Batch loss: 0.00039218683377839625\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 2700/4948, Batch loss: 0.0004222316201776266\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 2800/4948, Batch loss: 0.0016156767960637808\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 2900/4948, Batch loss: 0.0002011169126490131\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 3000/4948, Batch loss: 5.786896508652717e-05\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 3100/4948, Batch loss: 0.0001268378837266937\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 3200/4948, Batch loss: 0.00033697145408950746\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 3300/4948, Batch loss: 0.0001425954105798155\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 3400/4948, Batch loss: 5.576099738391349e-06\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 3500/4948, Batch loss: 4.954653195454739e-05\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 3600/4948, Batch loss: 4.0034025005297735e-05\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 3700/4948, Batch loss: 0.0005393247702158988\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 3800/4948, Batch loss: 0.00010857175948331133\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 3900/4948, Batch loss: 0.0001354970590909943\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 4000/4948, Batch loss: 0.00015280504885595292\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 4100/4948, Batch loss: 5.502059138962068e-05\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 4200/4948, Batch loss: 0.0010457090102136135\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 4300/4948, Batch loss: 0.0001449581322958693\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 4400/4948, Batch loss: 5.072287876828341e-06\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 4500/4948, Batch loss: 0.0010469025000929832\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 4600/4948, Batch loss: 0.00026059470837935805\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 4700/4948, Batch loss: 0.00044880827772431076\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 4800/4948, Batch loss: 0.0001122524481615983\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 4900/4948, Batch loss: 7.212989294202998e-05\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13, Batch 4948/4948, Batch loss: 0.0012686637928709388\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 13 completed in 70.27 seconds, Total Training Loss: 0.0003528458705671724\n",
      "\n",
      "Epoch 14/100\n",
      "Epoch 14, Batch 100/4948, Batch loss: 0.00017030829621944577\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 14, Batch 200/4948, Batch loss: 6.011544246575795e-05\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 14, Batch 300/4948, Batch loss: 0.00033897781395353377\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 14, Batch 400/4948, Batch loss: 0.00011963958968408406\n",
      "Memory Usage: 1097.97 MB\n",
      "Epoch 14, Batch 500/4948, Batch loss: 5.30531324329786e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 600/4948, Batch loss: 0.00047419717884622514\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 700/4948, Batch loss: 5.622946991934441e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 800/4948, Batch loss: 0.00012687355047091842\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 900/4948, Batch loss: 0.0006034967373125255\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 1000/4948, Batch loss: 8.610609802417457e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 1100/4948, Batch loss: 0.0008978747064247727\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 1200/4948, Batch loss: 0.0017326890956610441\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 1300/4948, Batch loss: 0.00015357964730355889\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 1400/4948, Batch loss: 0.00012370452168397605\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 1500/4948, Batch loss: 0.0001330948871327564\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 1600/4948, Batch loss: 3.799740079557523e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 1700/4948, Batch loss: 0.00040083503699861467\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 1800/4948, Batch loss: 0.00027134211268275976\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 1900/4948, Batch loss: 0.0045731510035693645\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 2000/4948, Batch loss: 0.000450990250101313\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 2100/4948, Batch loss: 0.0007347092614509165\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 2200/4948, Batch loss: 0.00010768225911306217\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 2300/4948, Batch loss: 0.00022288250329438597\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 2400/4948, Batch loss: 0.00010600735549814999\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 2500/4948, Batch loss: 4.84466363559477e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 2600/4948, Batch loss: 0.0003899014845956117\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 2700/4948, Batch loss: 0.00040080142207443714\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 2800/4948, Batch loss: 0.0015892061637714505\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 2900/4948, Batch loss: 0.00020459761435631663\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 3000/4948, Batch loss: 5.8842935686698183e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 3100/4948, Batch loss: 0.00012438526027835906\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 3200/4948, Batch loss: 0.00031704892171546817\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 3300/4948, Batch loss: 0.00014246470527723432\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 3400/4948, Batch loss: 6.8198178269085474e-06\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 3500/4948, Batch loss: 0.00016723851149436086\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 3600/4948, Batch loss: 4.116813943255693e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 3700/4948, Batch loss: 0.0005450054304674268\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 3800/4948, Batch loss: 0.00011342300422256812\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 3900/4948, Batch loss: 0.0001389071549056098\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 4000/4948, Batch loss: 0.00015192318824119866\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 4100/4948, Batch loss: 4.4573080231202766e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 4200/4948, Batch loss: 0.0010567951248958707\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 4300/4948, Batch loss: 0.00014402969100046903\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 4400/4948, Batch loss: 1.5509889635723084e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 4500/4948, Batch loss: 0.001040013274177909\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 4600/4948, Batch loss: 0.0002566573675721884\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 4700/4948, Batch loss: 0.00044740427983924747\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 4800/4948, Batch loss: 0.00011153722880408168\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 4900/4948, Batch loss: 7.75143998907879e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14, Batch 4948/4948, Batch loss: 0.0012555749854072928\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 14 completed in 69.62 seconds, Total Training Loss: 0.0003295716968490984\n",
      "\n",
      "Epoch 15/100\n",
      "Epoch 15, Batch 100/4948, Batch loss: 0.0001707045448711142\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 200/4948, Batch loss: 2.2229654859984294e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 300/4948, Batch loss: 0.00033445004373788834\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 400/4948, Batch loss: 0.0001180721665150486\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 500/4948, Batch loss: 5.488992974278517e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 600/4948, Batch loss: 0.00047135050408542156\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 700/4948, Batch loss: 5.669984602718614e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 800/4948, Batch loss: 0.000133460751385428\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 900/4948, Batch loss: 0.0005866479477845132\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 1000/4948, Batch loss: 8.727872045710683e-05\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 1100/4948, Batch loss: 0.0008882223046384752\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 1200/4948, Batch loss: 0.0017127165338024497\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 1300/4948, Batch loss: 0.00015715646441094577\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 1400/4948, Batch loss: 0.00012355508806649595\n",
      "Memory Usage: 1097.98 MB\n",
      "Epoch 15, Batch 1500/4948, Batch loss: 9.975111606763676e-05\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 1600/4948, Batch loss: 1.5157579582592007e-05\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 1700/4948, Batch loss: 0.0004806912911590189\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 1800/4948, Batch loss: 0.0002689175889827311\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 1900/4948, Batch loss: 0.007102632895112038\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 2000/4948, Batch loss: 0.0005930712213739753\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 2100/4948, Batch loss: 0.0008841142407618463\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 2200/4948, Batch loss: 0.00010663843568181619\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 2300/4948, Batch loss: 0.0002279749751323834\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 2400/4948, Batch loss: 0.00010748473141575232\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 2500/4948, Batch loss: 4.946089393342845e-05\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 2600/4948, Batch loss: 0.0003928471705876291\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 2700/4948, Batch loss: 0.00041988620068877935\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 2800/4948, Batch loss: 0.0015994254499673843\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 2900/4948, Batch loss: 0.0002000232198042795\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 3000/4948, Batch loss: 5.806675471831113e-05\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 3100/4948, Batch loss: 0.00012412562500685453\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 3200/4948, Batch loss: 0.00033084198366850615\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 3300/4948, Batch loss: 0.00014140797429718077\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 3400/4948, Batch loss: 1.0112527888850309e-05\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 3500/4948, Batch loss: 5.245169086265378e-05\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 3600/4948, Batch loss: 4.14656096836552e-05\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 3700/4948, Batch loss: 0.0005361015792004764\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 3800/4948, Batch loss: 0.00010717019176809117\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 3900/4948, Batch loss: 0.00013680393749382347\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 4000/4948, Batch loss: 0.00015192010323517025\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 4100/4948, Batch loss: 7.118059875210747e-05\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 4200/4948, Batch loss: 0.0010550994193181396\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 4300/4948, Batch loss: 0.0001447007671231404\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 4400/4948, Batch loss: 5.375986347644357e-06\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 4500/4948, Batch loss: 0.0010356579441577196\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 4600/4948, Batch loss: 0.00024786486756056547\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 4700/4948, Batch loss: 0.0004436295130290091\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 4800/4948, Batch loss: 0.00010971317533403635\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 4900/4948, Batch loss: 7.569191075162962e-05\n",
      "Memory Usage: 1098.33 MB\n",
      "Epoch 15, Batch 4948/4948, Batch loss: 0.0012549776583909988\n",
      "Memory Usage: 1098.34 MB\n",
      "Epoch 15 completed in 70.10 seconds, Total Training Loss: 0.00034873172060520985\n",
      "\n",
      "Epoch 16/100\n",
      "Epoch 16, Batch 100/4948, Batch loss: 0.00017102483252529055\n",
      "Memory Usage: 1098.34 MB\n",
      "Epoch 16, Batch 200/4948, Batch loss: 4.1933355532819405e-05\n",
      "Memory Usage: 1098.34 MB\n",
      "Epoch 16, Batch 300/4948, Batch loss: 0.00033433851785957813\n",
      "Memory Usage: 1098.34 MB\n",
      "Epoch 16, Batch 400/4948, Batch loss: 0.00011819034989457577\n",
      "Memory Usage: 1098.34 MB\n",
      "Epoch 16, Batch 500/4948, Batch loss: 5.620117735816166e-05\n",
      "Memory Usage: 1098.34 MB\n",
      "Epoch 16, Batch 600/4948, Batch loss: 0.00047222099965438247\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 700/4948, Batch loss: 5.994151797494851e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 800/4948, Batch loss: 0.00012648328265640885\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 900/4948, Batch loss: 0.00055848149349913\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 1000/4948, Batch loss: 8.571840589866042e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 1100/4948, Batch loss: 0.0008923025452531874\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 1200/4948, Batch loss: 0.0017171368235722184\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 1300/4948, Batch loss: 0.000162276322953403\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 1400/4948, Batch loss: 0.00012327956210356206\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 1500/4948, Batch loss: 0.00015368488675449044\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 1600/4948, Batch loss: 1.4194596587913111e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 1700/4948, Batch loss: 0.0006225979304872453\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 1800/4948, Batch loss: 0.00026944122510030866\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 1900/4948, Batch loss: 0.007594638504087925\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 2000/4948, Batch loss: 0.0005928917671553791\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 2100/4948, Batch loss: 0.000978959957137704\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 2200/4948, Batch loss: 0.00010662283602869138\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 2300/4948, Batch loss: 0.00021657199249602854\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 2400/4948, Batch loss: 0.00010586233111098409\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 2500/4948, Batch loss: 4.8500449338462204e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 2600/4948, Batch loss: 0.00039244446088559926\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 2700/4948, Batch loss: 0.0004120630619581789\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 2800/4948, Batch loss: 0.0015878808917477727\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 2900/4948, Batch loss: 0.00019899469043593854\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 3000/4948, Batch loss: 5.761863212683238e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 3100/4948, Batch loss: 0.00012346956646069884\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 3200/4948, Batch loss: 0.00035076113999821246\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 3300/4948, Batch loss: 0.00014237237337511033\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 3400/4948, Batch loss: 6.889076303195907e-06\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 3500/4948, Batch loss: 4.6233391913119704e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 3600/4948, Batch loss: 4.338519647717476e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 3700/4948, Batch loss: 0.0005336417816579342\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 3800/4948, Batch loss: 0.0001068988858605735\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 3900/4948, Batch loss: 0.00013723188021685928\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 4000/4948, Batch loss: 0.00015119351155590266\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 4100/4948, Batch loss: 9.130396210821345e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 4200/4948, Batch loss: 0.0010453909635543823\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 4300/4948, Batch loss: 0.00014462432591244578\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 4400/4948, Batch loss: 4.2106130422325805e-06\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 4500/4948, Batch loss: 0.001035928726196289\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 4600/4948, Batch loss: 0.0002513599756639451\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 4700/4948, Batch loss: 0.00044306882773526013\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 4800/4948, Batch loss: 0.00012303605035413057\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 4900/4948, Batch loss: 7.370978710241616e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16, Batch 4948/4948, Batch loss: 0.001253386028110981\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 16 completed in 69.57 seconds, Total Training Loss: 0.0003458355732430848\n",
      "\n",
      "Epoch 17/100\n",
      "Epoch 17, Batch 100/4948, Batch loss: 0.00017100523109547794\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 200/4948, Batch loss: 2.470665640430525e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 300/4948, Batch loss: 0.0003336475929245353\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 400/4948, Batch loss: 0.00011779960914282128\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 500/4948, Batch loss: 5.355760004022159e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 600/4948, Batch loss: 0.00047249378985725343\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 700/4948, Batch loss: 6.7463843151927e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 800/4948, Batch loss: 0.00012448072084225714\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 900/4948, Batch loss: 0.0005454829661175609\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 1000/4948, Batch loss: 8.912508201319724e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 1100/4948, Batch loss: 0.0008883042028173804\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 1200/4948, Batch loss: 0.0017067447770386934\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 1300/4948, Batch loss: 0.00016045132360886782\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 1400/4948, Batch loss: 0.00012380623957142234\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 1500/4948, Batch loss: 0.00010296927212039009\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 1600/4948, Batch loss: 1.3825528185407165e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 1700/4948, Batch loss: 0.0007810724200680852\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 1800/4948, Batch loss: 0.00026842477382160723\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 1900/4948, Batch loss: 0.005135638173669577\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 2000/4948, Batch loss: 0.0004685441090259701\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 2100/4948, Batch loss: 0.0007611199398525059\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 2200/4948, Batch loss: 0.00010633868805598468\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 2300/4948, Batch loss: 0.00022016458387952298\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 2400/4948, Batch loss: 0.00010782781464513391\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 2500/4948, Batch loss: 4.847571108257398e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 2600/4948, Batch loss: 0.0003918999864254147\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 2700/4948, Batch loss: 0.0004034059529658407\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 2800/4948, Batch loss: 0.001584647223353386\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 2900/4948, Batch loss: 0.00020120295812375844\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 3000/4948, Batch loss: 5.777052138000727e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 3100/4948, Batch loss: 0.00012943882029503584\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 3200/4948, Batch loss: 0.0003262747486587614\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 3300/4948, Batch loss: 0.0001421336637577042\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 3400/4948, Batch loss: 6.380862032528967e-06\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 3500/4948, Batch loss: 5.1465522119542584e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 3600/4948, Batch loss: 4.232061473885551e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 3700/4948, Batch loss: 0.0005517213139683008\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 3800/4948, Batch loss: 0.00010915069287875667\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 3900/4948, Batch loss: 0.00013607497385237366\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 4000/4948, Batch loss: 0.00015127194637898356\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 4100/4948, Batch loss: 6.213691813172773e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 4200/4948, Batch loss: 0.0010494235903024673\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 4300/4948, Batch loss: 0.0001447355025447905\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 4400/4948, Batch loss: 3.5106245377392042e-06\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 4500/4948, Batch loss: 0.001032488071359694\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 4600/4948, Batch loss: 0.00024820116232149303\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 4700/4948, Batch loss: 0.0004388282832223922\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 4800/4948, Batch loss: 0.00011871506285388023\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 4900/4948, Batch loss: 7.119248039089143e-05\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17, Batch 4948/4948, Batch loss: 0.0012558947782963514\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 17 completed in 66.92 seconds, Total Training Loss: 0.00033825225196606014\n",
      "\n",
      "Epoch 18/100\n",
      "Epoch 18, Batch 100/4948, Batch loss: 0.00017134772497229278\n",
      "Memory Usage: 1098.36 MB\n",
      "Epoch 18, Batch 200/4948, Batch loss: 2.1315943740773946e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 300/4948, Batch loss: 0.0003346117155160755\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 400/4948, Batch loss: 0.00011757050378946587\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 500/4948, Batch loss: 5.436455103335902e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 600/4948, Batch loss: 0.0004729045322164893\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 700/4948, Batch loss: 5.585120015894063e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 800/4948, Batch loss: 0.00012495201372075826\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 900/4948, Batch loss: 0.0005613570683635771\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 1000/4948, Batch loss: 8.519743278156966e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 1100/4948, Batch loss: 0.0008966283639892936\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 1200/4948, Batch loss: 0.0016919615445658565\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 1300/4948, Batch loss: 0.00016239512478932738\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 1400/4948, Batch loss: 0.00012224502279423177\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 1500/4948, Batch loss: 0.00012627633986994624\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 1600/4948, Batch loss: 1.5088978216226678e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 1700/4948, Batch loss: 0.00041907207923941314\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 1800/4948, Batch loss: 0.00026931861066259444\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 1900/4948, Batch loss: 0.006523448042571545\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 2000/4948, Batch loss: 0.0005406391574069858\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 2100/4948, Batch loss: 0.0008872044854797423\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 2200/4948, Batch loss: 0.00010607803415041417\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 2300/4948, Batch loss: 0.0002142057055607438\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 2400/4948, Batch loss: 0.00010764219041448087\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 2500/4948, Batch loss: 4.864445509156212e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 2600/4948, Batch loss: 0.00039124509203247726\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 2700/4948, Batch loss: 0.0004109915462322533\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 2800/4948, Batch loss: 0.0015842975117266178\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 2900/4948, Batch loss: 0.00020004418911412358\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 3000/4948, Batch loss: 5.822449384140782e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 3100/4948, Batch loss: 0.00013071272405795753\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 3200/4948, Batch loss: 0.00031700264662504196\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 3300/4948, Batch loss: 0.00014209120126906782\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 3400/4948, Batch loss: 8.109699592750985e-06\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 3500/4948, Batch loss: 4.818271554540843e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 3600/4948, Batch loss: 4.433405410964042e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 3700/4948, Batch loss: 0.0005423512775450945\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 3800/4948, Batch loss: 0.00010901223140535876\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 3900/4948, Batch loss: 0.00013498318730853498\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 4000/4948, Batch loss: 0.0001518377976026386\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 4100/4948, Batch loss: 9.73244896158576e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 4200/4948, Batch loss: 0.001048669102601707\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 4300/4948, Batch loss: 0.00014493841445073485\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 4400/4948, Batch loss: 5.383922143664677e-06\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 4500/4948, Batch loss: 0.0010354429250583053\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 4600/4948, Batch loss: 0.0002448910381644964\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 4700/4948, Batch loss: 0.0004403453494887799\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 4800/4948, Batch loss: 0.0001145061178249307\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 4900/4948, Batch loss: 7.285999890882522e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18, Batch 4948/4948, Batch loss: 0.0012115027057006955\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 18 completed in 69.82 seconds, Total Training Loss: 0.000339531356777477\n",
      "\n",
      "Epoch 19/100\n",
      "Epoch 19, Batch 100/4948, Batch loss: 0.00017178340931423008\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 200/4948, Batch loss: 2.2495172743219882e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 300/4948, Batch loss: 0.00033301874645985663\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 400/4948, Batch loss: 0.00011654117406578735\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 500/4948, Batch loss: 5.453647463582456e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 600/4948, Batch loss: 0.0004772758693434298\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 700/4948, Batch loss: 5.946771125309169e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 800/4948, Batch loss: 0.0001278914714930579\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 900/4948, Batch loss: 0.0005497525562532246\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 1000/4948, Batch loss: 8.80235675140284e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 1100/4948, Batch loss: 0.0008965304587036371\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 1200/4948, Batch loss: 0.001680895802564919\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 1300/4948, Batch loss: 0.00015502277528867126\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 1400/4948, Batch loss: 0.0001245180465048179\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 1500/4948, Batch loss: 9.005108586279675e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 1600/4948, Batch loss: 1.3324671272130217e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 1700/4948, Batch loss: 0.0006064055487513542\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 1800/4948, Batch loss: 0.00026713692932389677\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 1900/4948, Batch loss: 0.006113552954047918\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 2000/4948, Batch loss: 0.0005537617835216224\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 2100/4948, Batch loss: 0.0008467050502076745\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 2200/4948, Batch loss: 0.00010638591629685834\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 2300/4948, Batch loss: 0.0002204560296377167\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 2400/4948, Batch loss: 0.00010723307786975056\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 2500/4948, Batch loss: 4.8126701585715637e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 2600/4948, Batch loss: 0.00039355564513243735\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 2700/4948, Batch loss: 0.00040335924131795764\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 2800/4948, Batch loss: 0.0015745179262012243\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 2900/4948, Batch loss: 0.00020045317069161683\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 3000/4948, Batch loss: 5.833706381963566e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 3100/4948, Batch loss: 0.00012444522872101516\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 3200/4948, Batch loss: 0.0003315985086373985\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 3300/4948, Batch loss: 0.00014260897296480834\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 3400/4948, Batch loss: 6.484790901595261e-06\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 3500/4948, Batch loss: 4.705795072368346e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 3600/4948, Batch loss: 4.4114040065323934e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 3700/4948, Batch loss: 0.000540989451110363\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 3800/4948, Batch loss: 0.00010803467739606276\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 3900/4948, Batch loss: 0.0001349311787635088\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 4000/4948, Batch loss: 0.00015117699513211846\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 4100/4948, Batch loss: 7.040529453661293e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 4200/4948, Batch loss: 0.0010523913661018014\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 4300/4948, Batch loss: 0.00014366723189596087\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 4400/4948, Batch loss: 4.2119108911720105e-06\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 4500/4948, Batch loss: 0.0010227130260318518\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 4600/4948, Batch loss: 0.0002518539840821177\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 4700/4948, Batch loss: 0.00044133063056506217\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 4800/4948, Batch loss: 0.00012308696750551462\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 4900/4948, Batch loss: 7.173873746069148e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19, Batch 4948/4948, Batch loss: 0.001234969706274569\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 19 completed in 69.95 seconds, Total Training Loss: 0.000338986941144349\n",
      "\n",
      "Epoch 20/100\n",
      "Epoch 20, Batch 100/4948, Batch loss: 0.00017261318862438202\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 20, Batch 200/4948, Batch loss: 2.5644087145337835e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 20, Batch 300/4948, Batch loss: 0.0003319345705676824\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 20, Batch 400/4948, Batch loss: 0.00011693927081068978\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 20, Batch 500/4948, Batch loss: 5.3849373216507956e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 20, Batch 600/4948, Batch loss: 0.00047249277122318745\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 20, Batch 700/4948, Batch loss: 5.283294012770057e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 20, Batch 800/4948, Batch loss: 0.00012440189311746508\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 20, Batch 900/4948, Batch loss: 0.0005669178790412843\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 20, Batch 1000/4948, Batch loss: 8.582635928178206e-05\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 20, Batch 1100/4948, Batch loss: 0.000907297944650054\n",
      "Memory Usage: 1098.89 MB\n",
      "Epoch 20, Batch 1200/4948, Batch loss: 0.0016879303148016334\n",
      "Memory Usage: 927.94 MB\n",
      "Epoch 20, Batch 1300/4948, Batch loss: 0.0001596396614331752\n",
      "Memory Usage: 932.23 MB\n",
      "Epoch 20, Batch 1400/4948, Batch loss: 0.00012292918108869344\n",
      "Memory Usage: 928.95 MB\n",
      "Epoch 20, Batch 1500/4948, Batch loss: 0.0001571362663526088\n",
      "Memory Usage: 935.00 MB\n",
      "Epoch 20, Batch 1600/4948, Batch loss: 1.3308679626788944e-05\n",
      "Memory Usage: 936.97 MB\n",
      "Epoch 20, Batch 1700/4948, Batch loss: 0.00046781337005086243\n",
      "Memory Usage: 937.95 MB\n",
      "Epoch 20, Batch 1800/4948, Batch loss: 0.00026701902970671654\n",
      "Memory Usage: 940.23 MB\n",
      "Epoch 20, Batch 1900/4948, Batch loss: 0.006033213809132576\n",
      "Memory Usage: 940.23 MB\n",
      "Epoch 20, Batch 2000/4948, Batch loss: 0.0005227160290814936\n",
      "Memory Usage: 940.23 MB\n",
      "Epoch 20, Batch 2100/4948, Batch loss: 0.000846206268761307\n",
      "Memory Usage: 940.44 MB\n",
      "Epoch 20, Batch 2200/4948, Batch loss: 0.00010587980796117336\n",
      "Memory Usage: 940.44 MB\n",
      "Epoch 20, Batch 2300/4948, Batch loss: 0.00021711361478082836\n",
      "Memory Usage: 940.72 MB\n",
      "Epoch 20, Batch 2400/4948, Batch loss: 0.00010563889372861013\n",
      "Memory Usage: 941.20 MB\n",
      "Epoch 20, Batch 2500/4948, Batch loss: 4.884541704086587e-05\n",
      "Memory Usage: 941.55 MB\n",
      "Epoch 20, Batch 2600/4948, Batch loss: 0.00039250298868864775\n",
      "Memory Usage: 941.67 MB\n",
      "Epoch 20, Batch 2700/4948, Batch loss: 0.0004076994373463094\n",
      "Memory Usage: 941.67 MB\n",
      "Epoch 20, Batch 2800/4948, Batch loss: 0.0015626155072823167\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 2900/4948, Batch loss: 0.00019949048873968422\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 3000/4948, Batch loss: 5.835836418555118e-05\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 3100/4948, Batch loss: 0.00012220248754601926\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 3200/4948, Batch loss: 0.0003171743010170758\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 3300/4948, Batch loss: 0.00014193398237694055\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 3400/4948, Batch loss: 5.5205364333232865e-06\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 3500/4948, Batch loss: 4.8437050281791016e-05\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 3600/4948, Batch loss: 4.1794268327066675e-05\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 3700/4948, Batch loss: 0.0005400643567554653\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 3800/4948, Batch loss: 0.00010770041990326717\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 3900/4948, Batch loss: 0.0001357129222014919\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 4000/4948, Batch loss: 0.00015122089826036245\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 4100/4948, Batch loss: 5.404637704486959e-05\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 4200/4948, Batch loss: 0.001033900072798133\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 4300/4948, Batch loss: 0.00014473059854935855\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 4400/4948, Batch loss: 4.391708898765501e-06\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 4500/4948, Batch loss: 0.0010232715867459774\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 4600/4948, Batch loss: 0.0002508046163711697\n",
      "Memory Usage: 941.91 MB\n",
      "Epoch 20, Batch 4700/4948, Batch loss: 0.0004416771698743105\n",
      "Memory Usage: 941.98 MB\n",
      "Epoch 20, Batch 4800/4948, Batch loss: 0.00012292296742089093\n",
      "Memory Usage: 942.55 MB\n",
      "Epoch 20, Batch 4900/4948, Batch loss: 7.348935469053686e-05\n",
      "Memory Usage: 942.55 MB\n",
      "Epoch 20, Batch 4948/4948, Batch loss: 0.0011896459618583322\n",
      "Memory Usage: 942.55 MB\n",
      "Epoch 20 completed in 70.62 seconds, Total Training Loss: 0.00033543240776479804\n",
      "\n",
      "Epoch 21/100\n",
      "Epoch 21, Batch 100/4948, Batch loss: 0.00017209327779710293\n",
      "Memory Usage: 947.97 MB\n",
      "Epoch 21, Batch 200/4948, Batch loss: 2.4926916012191214e-05\n",
      "Memory Usage: 949.12 MB\n",
      "Epoch 21, Batch 300/4948, Batch loss: 0.0003317674854770303\n",
      "Memory Usage: 949.12 MB\n",
      "Epoch 21, Batch 400/4948, Batch loss: 0.00011624843318713829\n",
      "Memory Usage: 949.12 MB\n",
      "Epoch 21, Batch 500/4948, Batch loss: 5.5448614148190245e-05\n",
      "Memory Usage: 949.12 MB\n",
      "Epoch 21, Batch 600/4948, Batch loss: 0.00047444499796256423\n",
      "Memory Usage: 949.12 MB\n",
      "Epoch 21, Batch 700/4948, Batch loss: 5.629485167446546e-05\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 800/4948, Batch loss: 0.00012365500151645392\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 900/4948, Batch loss: 0.0005500579718500376\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 1000/4948, Batch loss: 8.844168769428506e-05\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 1100/4948, Batch loss: 0.0008942480781115592\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 1200/4948, Batch loss: 0.001670714234933257\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 1300/4948, Batch loss: 0.00016045388474594802\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 1400/4948, Batch loss: 0.00012320888345129788\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 1500/4948, Batch loss: 0.00010939503408735618\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 1600/4948, Batch loss: 2.0147674149484374e-05\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 1700/4948, Batch loss: 0.0005672001861967146\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 1800/4948, Batch loss: 0.00026514436467550695\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 1900/4948, Batch loss: 0.005917966831475496\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 2000/4948, Batch loss: 0.0005303098005242646\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 2100/4948, Batch loss: 0.0008208607323467731\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 2200/4948, Batch loss: 0.00010548052523517981\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 2300/4948, Batch loss: 0.00022031478874851018\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 2400/4948, Batch loss: 0.00010958679195027798\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 2500/4948, Batch loss: 4.9678183131618425e-05\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 2600/4948, Batch loss: 0.00039191273390315473\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 2700/4948, Batch loss: 0.00040899612940847874\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 2800/4948, Batch loss: 0.0015640612691640854\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 2900/4948, Batch loss: 0.00020050644525326788\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 3000/4948, Batch loss: 5.773122757091187e-05\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 3100/4948, Batch loss: 0.00012337237421888858\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 3200/4948, Batch loss: 0.00032566505251452327\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 3300/4948, Batch loss: 0.00014242964971344918\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 3400/4948, Batch loss: 6.9437742240552325e-06\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 3500/4948, Batch loss: 4.7079687647055835e-05\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 3600/4948, Batch loss: 4.727789928438142e-05\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 3700/4948, Batch loss: 0.0005384220276027918\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 3800/4948, Batch loss: 0.0001071962688001804\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 3900/4948, Batch loss: 0.00013551941083278507\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 4000/4948, Batch loss: 0.00015160706243477762\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 4100/4948, Batch loss: 4.760260708280839e-05\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 4200/4948, Batch loss: 0.0010359968291595578\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 4300/4948, Batch loss: 0.00014431620365940034\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 4400/4948, Batch loss: 6.212724656506907e-06\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 4500/4948, Batch loss: 0.0010277642868459225\n",
      "Memory Usage: 949.19 MB\n",
      "Epoch 21, Batch 4600/4948, Batch loss: 0.0002522498252801597\n",
      "Memory Usage: 949.20 MB\n",
      "Epoch 21, Batch 4700/4948, Batch loss: 0.00044131220784038305\n",
      "Memory Usage: 949.20 MB\n",
      "Epoch 21, Batch 4800/4948, Batch loss: 0.00012026730109937489\n",
      "Memory Usage: 949.20 MB\n",
      "Epoch 21, Batch 4900/4948, Batch loss: 7.100182119756937e-05\n",
      "Memory Usage: 949.20 MB\n",
      "Epoch 21, Batch 4948/4948, Batch loss: 0.0011712017003446817\n",
      "Memory Usage: 949.20 MB\n",
      "Epoch 21 completed in 69.86 seconds, Total Training Loss: 0.00033434052899214466\n",
      "Validation completed in 3.38 seconds, Average Validation Loss: 0.0005095266512041571\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 22/100\n",
      "Epoch 22, Batch 100/4948, Batch loss: 0.00017044365813490003\n",
      "Memory Usage: 954.73 MB\n",
      "Epoch 22, Batch 200/4948, Batch loss: 2.3250084268511273e-05\n",
      "Memory Usage: 954.73 MB\n",
      "Epoch 22, Batch 300/4948, Batch loss: 0.00033222412457689643\n",
      "Memory Usage: 954.73 MB\n",
      "Epoch 22, Batch 400/4948, Batch loss: 0.00011672029359033331\n",
      "Memory Usage: 954.73 MB\n",
      "Epoch 22, Batch 500/4948, Batch loss: 5.480318213813007e-05\n",
      "Memory Usage: 954.73 MB\n",
      "Epoch 22, Batch 600/4948, Batch loss: 0.00047352368710562587\n",
      "Memory Usage: 954.75 MB\n",
      "Epoch 22, Batch 700/4948, Batch loss: 6.539744208566844e-05\n",
      "Memory Usage: 954.75 MB\n",
      "Epoch 22, Batch 800/4948, Batch loss: 0.00012459485151339322\n",
      "Memory Usage: 954.75 MB\n",
      "Epoch 22, Batch 900/4948, Batch loss: 0.0005614813999272883\n",
      "Memory Usage: 954.75 MB\n",
      "Epoch 22, Batch 1000/4948, Batch loss: 8.673797856317833e-05\n",
      "Memory Usage: 954.77 MB\n",
      "Epoch 22, Batch 1100/4948, Batch loss: 0.0008933112258091569\n",
      "Memory Usage: 954.77 MB\n",
      "Epoch 22, Batch 1200/4948, Batch loss: 0.0016693093348294497\n",
      "Memory Usage: 954.78 MB\n",
      "Epoch 22, Batch 1300/4948, Batch loss: 0.0001601297699380666\n",
      "Memory Usage: 954.78 MB\n",
      "Epoch 22, Batch 1400/4948, Batch loss: 0.00012279105430934578\n",
      "Memory Usage: 954.78 MB\n",
      "Epoch 22, Batch 1500/4948, Batch loss: 0.00012637546751648188\n",
      "Memory Usage: 954.78 MB\n",
      "Epoch 22, Batch 1600/4948, Batch loss: 1.3456514352583326e-05\n",
      "Memory Usage: 954.78 MB\n",
      "Epoch 22, Batch 1700/4948, Batch loss: 0.000495442480314523\n",
      "Memory Usage: 954.78 MB\n",
      "Epoch 22, Batch 1800/4948, Batch loss: 0.00026765852817334235\n",
      "Memory Usage: 954.78 MB\n",
      "Epoch 22, Batch 1900/4948, Batch loss: 0.00531338807195425\n",
      "Memory Usage: 954.78 MB\n",
      "Epoch 22, Batch 2000/4948, Batch loss: 0.0005607831990346313\n",
      "Memory Usage: 954.80 MB\n",
      "Epoch 22, Batch 2100/4948, Batch loss: 0.0008410274749621749\n",
      "Memory Usage: 954.81 MB\n",
      "Epoch 22, Batch 2200/4948, Batch loss: 0.00010559656220721081\n",
      "Memory Usage: 954.81 MB\n",
      "Epoch 22, Batch 2300/4948, Batch loss: 0.00021351837494876236\n",
      "Memory Usage: 954.81 MB\n",
      "Epoch 22, Batch 2400/4948, Batch loss: 0.00010756374831544235\n",
      "Memory Usage: 954.81 MB\n",
      "Epoch 22, Batch 2500/4948, Batch loss: 4.8972924560075626e-05\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 2600/4948, Batch loss: 0.0003931416431441903\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 2700/4948, Batch loss: 0.0003992950078099966\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 2800/4948, Batch loss: 0.0015556288417428732\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 2900/4948, Batch loss: 0.00019994867034256458\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 3000/4948, Batch loss: 5.75665726501029e-05\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 3100/4948, Batch loss: 0.00012723132385872304\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 3200/4948, Batch loss: 0.0003375197702553123\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 3300/4948, Batch loss: 0.0001430893607903272\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 3400/4948, Batch loss: 7.502194421249442e-06\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 3500/4948, Batch loss: 4.7991568862926215e-05\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 3600/4948, Batch loss: 4.396055010147393e-05\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 3700/4948, Batch loss: 0.0005470376345328987\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 3800/4948, Batch loss: 0.00010768777428893372\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 3900/4948, Batch loss: 0.0001356256689177826\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 4000/4948, Batch loss: 0.0001523864921182394\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 4100/4948, Batch loss: 6.897057028254494e-05\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 4200/4948, Batch loss: 0.0010476696770638227\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 4300/4948, Batch loss: 0.00014476088108494878\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 4400/4948, Batch loss: 4.983763119525975e-06\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 4500/4948, Batch loss: 0.001018927781842649\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 4600/4948, Batch loss: 0.0002416108181932941\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 4700/4948, Batch loss: 0.0004357266298029572\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 4800/4948, Batch loss: 0.00011524229194037616\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 4900/4948, Batch loss: 7.29159582988359e-05\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22, Batch 4948/4948, Batch loss: 0.0011559856357052922\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 22 completed in 68.65 seconds, Total Training Loss: 0.00033422937002390384\n",
      "\n",
      "Epoch 23/100\n",
      "Epoch 23, Batch 100/4948, Batch loss: 0.00017001881496980786\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 23, Batch 200/4948, Batch loss: 1.4899088455422316e-05\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 23, Batch 300/4948, Batch loss: 0.0003303065022919327\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 23, Batch 400/4948, Batch loss: 0.00011736663873307407\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 23, Batch 500/4948, Batch loss: 5.1918796089012176e-05\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 23, Batch 600/4948, Batch loss: 0.00047487736446782947\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 23, Batch 700/4948, Batch loss: 5.4631011153105646e-05\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 23, Batch 800/4948, Batch loss: 0.00013279923587106168\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 23, Batch 900/4948, Batch loss: 0.0005500107072293758\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 23, Batch 1000/4948, Batch loss: 8.777016046224162e-05\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 23, Batch 1100/4948, Batch loss: 0.0008888516458682716\n",
      "Memory Usage: 954.83 MB\n",
      "Epoch 23, Batch 1200/4948, Batch loss: 0.0016588243888691068\n",
      "Memory Usage: 954.84 MB\n",
      "Epoch 23, Batch 1300/4948, Batch loss: 0.00015453189553227276\n",
      "Memory Usage: 954.86 MB\n",
      "Epoch 23, Batch 1400/4948, Batch loss: 0.00012345085269771516\n",
      "Memory Usage: 954.86 MB\n",
      "Epoch 23, Batch 1500/4948, Batch loss: 8.814747707219794e-05\n",
      "Memory Usage: 954.86 MB\n",
      "Epoch 23, Batch 1600/4948, Batch loss: 1.3180965652281884e-05\n",
      "Memory Usage: 954.86 MB\n",
      "Epoch 23, Batch 1700/4948, Batch loss: 0.0006132781272754073\n",
      "Memory Usage: 954.86 MB\n",
      "Epoch 23, Batch 1800/4948, Batch loss: 0.0002659790334291756\n",
      "Memory Usage: 954.86 MB\n",
      "Epoch 23, Batch 1900/4948, Batch loss: 0.005231042392551899\n",
      "Memory Usage: 954.86 MB\n",
      "Epoch 23, Batch 2000/4948, Batch loss: 0.0005357218906283379\n",
      "Memory Usage: 954.86 MB\n",
      "Epoch 23, Batch 2100/4948, Batch loss: 0.0008205804042518139\n",
      "Memory Usage: 954.86 MB\n",
      "Epoch 23, Batch 2200/4948, Batch loss: 0.00010508670675335452\n",
      "Memory Usage: 954.86 MB\n",
      "Epoch 23, Batch 2300/4948, Batch loss: 0.0002178206923417747\n",
      "Memory Usage: 954.86 MB\n",
      "Epoch 23, Batch 2400/4948, Batch loss: 0.00010529989958740771\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 2500/4948, Batch loss: 4.869467011303641e-05\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 2600/4948, Batch loss: 0.00039065926102921367\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 2700/4948, Batch loss: 0.0003992291749455035\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 2800/4948, Batch loss: 0.0015604114159941673\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 2900/4948, Batch loss: 0.00020011900051031262\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 3000/4948, Batch loss: 5.833053000969812e-05\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 3100/4948, Batch loss: 0.0001262717560166493\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 3200/4948, Batch loss: 0.00033102708403021097\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 3300/4948, Batch loss: 0.00014192274829838425\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 3400/4948, Batch loss: 6.864812803542009e-06\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 3500/4948, Batch loss: 4.96862230647821e-05\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 3600/4948, Batch loss: 5.06565957039129e-05\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 3700/4948, Batch loss: 0.0005368345300666988\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 3800/4948, Batch loss: 0.00010728580673458055\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 3900/4948, Batch loss: 0.0001352310209767893\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 4000/4948, Batch loss: 0.00015077782154548913\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 4100/4948, Batch loss: 6.805206794524565e-05\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 4200/4948, Batch loss: 0.001049188431352377\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 4300/4948, Batch loss: 0.00014572832151316106\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 4400/4948, Batch loss: 4.426691248227144e-06\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 4500/4948, Batch loss: 0.0010230360785499215\n",
      "Memory Usage: 954.89 MB\n",
      "Epoch 23, Batch 4600/4948, Batch loss: 0.0002429382293485105\n",
      "Memory Usage: 954.91 MB\n",
      "Epoch 23, Batch 4700/4948, Batch loss: 0.00043368246406316757\n",
      "Memory Usage: 954.91 MB\n",
      "Epoch 23, Batch 4800/4948, Batch loss: 0.00012995919678360224\n",
      "Memory Usage: 954.91 MB\n",
      "Epoch 23, Batch 4900/4948, Batch loss: 7.2162423748523e-05\n",
      "Memory Usage: 954.91 MB\n",
      "Epoch 23, Batch 4948/4948, Batch loss: 0.0011643271427601576\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 23 completed in 68.59 seconds, Total Training Loss: 0.0003320866456429585\n",
      "\n",
      "Epoch 24/100\n",
      "Epoch 24, Batch 100/4948, Batch loss: 0.00017135095549747348\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 200/4948, Batch loss: 1.2346889889158774e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 300/4948, Batch loss: 0.000329876143950969\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 400/4948, Batch loss: 0.00011672176333377138\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 500/4948, Batch loss: 5.2981260523665696e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 600/4948, Batch loss: 0.00047062241355888546\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 700/4948, Batch loss: 7.003897189861163e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 800/4948, Batch loss: 0.00012922476162202656\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 900/4948, Batch loss: 0.000561273714993149\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 1000/4948, Batch loss: 8.698442979948595e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 1100/4948, Batch loss: 0.0008937689708545804\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 1200/4948, Batch loss: 0.00166152766905725\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 1300/4948, Batch loss: 0.00015656939649488777\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 1400/4948, Batch loss: 0.0001247342152055353\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 1500/4948, Batch loss: 8.616015838924795e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 1600/4948, Batch loss: 1.378776414640015e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 1700/4948, Batch loss: 0.0005646004574373364\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 1800/4948, Batch loss: 0.0002680635079741478\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 1900/4948, Batch loss: 0.005069364327937365\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 2000/4948, Batch loss: 0.00043667256250046194\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 2100/4948, Batch loss: 0.0007234042859636247\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 2200/4948, Batch loss: 0.00010589136945782229\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 2300/4948, Batch loss: 0.00021794295753352344\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 2400/4948, Batch loss: 0.00010649942851159722\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 2500/4948, Batch loss: 4.840291876462288e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 2600/4948, Batch loss: 0.0003894303517881781\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 2700/4948, Batch loss: 0.0003945940698031336\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 2800/4948, Batch loss: 0.0015630675479769707\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 2900/4948, Batch loss: 0.00019956899632234126\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 3000/4948, Batch loss: 5.7756937167141587e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 3100/4948, Batch loss: 0.00012539888848550618\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 3200/4948, Batch loss: 0.0003197509213350713\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 3300/4948, Batch loss: 0.0001412394776707515\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 3400/4948, Batch loss: 8.08022014098242e-06\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 3500/4948, Batch loss: 5.1529230404412374e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 3600/4948, Batch loss: 4.2787847633007914e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 3700/4948, Batch loss: 0.0005471938056871295\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 3800/4948, Batch loss: 0.0001059519054251723\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 3900/4948, Batch loss: 0.00013574260810855776\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 4000/4948, Batch loss: 0.00015087109932210296\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 4100/4948, Batch loss: 4.591953620547429e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 4200/4948, Batch loss: 0.0010347255738452077\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 4300/4948, Batch loss: 0.00014478560478892177\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 4400/4948, Batch loss: 4.522001290752087e-06\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 4500/4948, Batch loss: 0.0010077233891934156\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 4600/4948, Batch loss: 0.00025183483376167715\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 4700/4948, Batch loss: 0.0004425211518537253\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 4800/4948, Batch loss: 0.00012566297664307058\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 4900/4948, Batch loss: 7.050912972772494e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24, Batch 4948/4948, Batch loss: 0.0011608742643147707\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 24 completed in 69.05 seconds, Total Training Loss: 0.0003238101219967812\n",
      "\n",
      "Epoch 25/100\n",
      "Epoch 25, Batch 100/4948, Batch loss: 0.0001722127344692126\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 200/4948, Batch loss: 2.453199340379797e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 300/4948, Batch loss: 0.00032703165197744966\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 400/4948, Batch loss: 0.00011808579438365996\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 500/4948, Batch loss: 5.5230982979992405e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 600/4948, Batch loss: 0.00047043131780810654\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 700/4948, Batch loss: 6.463222962338477e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 800/4948, Batch loss: 0.00012464441533666104\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 900/4948, Batch loss: 0.0005431747413240373\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 1000/4948, Batch loss: 9.074529953068122e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 1100/4948, Batch loss: 0.0009058018331415951\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 1200/4948, Batch loss: 0.0016563487006351352\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 1300/4948, Batch loss: 0.0001574264606460929\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 1400/4948, Batch loss: 0.00012211500143166631\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 1500/4948, Batch loss: 0.0001512005546828732\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 1600/4948, Batch loss: 1.4032473700353876e-05\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 1700/4948, Batch loss: 0.0005656491266563535\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 1800/4948, Batch loss: 0.00026715450803749263\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 1900/4948, Batch loss: 0.005177382845431566\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 2000/4948, Batch loss: 0.0005729543627239764\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 2100/4948, Batch loss: 0.0009349897736683488\n",
      "Memory Usage: 954.94 MB\n",
      "Epoch 25, Batch 2200/4948, Batch loss: 0.00010613008635118604\n",
      "Memory Usage: 955.39 MB\n",
      "Epoch 25, Batch 2300/4948, Batch loss: 0.00022585740953218192\n",
      "Memory Usage: 955.39 MB\n",
      "Epoch 25, Batch 2400/4948, Batch loss: 0.00010523475066293031\n",
      "Memory Usage: 955.39 MB\n",
      "Epoch 25, Batch 2500/4948, Batch loss: 4.933732270728797e-05\n",
      "Memory Usage: 955.39 MB\n",
      "Epoch 25, Batch 2600/4948, Batch loss: 0.00039155303966253996\n",
      "Memory Usage: 955.39 MB\n",
      "Epoch 25, Batch 2700/4948, Batch loss: 0.0004002662899438292\n",
      "Memory Usage: 955.39 MB\n",
      "Epoch 25, Batch 2800/4948, Batch loss: 0.001566988998092711\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 2900/4948, Batch loss: 0.00019952958973590285\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 3000/4948, Batch loss: 5.751001663156785e-05\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 3100/4948, Batch loss: 0.00012538090231828392\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 3200/4948, Batch loss: 0.00033112597884610295\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 3300/4948, Batch loss: 0.00013968723942525685\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 3400/4948, Batch loss: 5.7177703638444655e-06\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 3500/4948, Batch loss: 4.8696638259571046e-05\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 3600/4948, Batch loss: 4.245009768055752e-05\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 3700/4948, Batch loss: 0.0005438864463940263\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 3800/4948, Batch loss: 0.00010705221211537719\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 3900/4948, Batch loss: 0.00013627353473566473\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 4000/4948, Batch loss: 0.00015109387459233403\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 4100/4948, Batch loss: 5.4918193200137466e-05\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 4200/4948, Batch loss: 0.001044998993165791\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 4300/4948, Batch loss: 0.0001445213274564594\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 4400/4948, Batch loss: 4.921520030620741e-06\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 4500/4948, Batch loss: 0.0010163037804886699\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 4600/4948, Batch loss: 0.00024473306257277727\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 4700/4948, Batch loss: 0.00044046377297490835\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 4800/4948, Batch loss: 0.00012428117042873055\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 4900/4948, Batch loss: 7.386720972135663e-05\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25, Batch 4948/4948, Batch loss: 0.0011532307835295796\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 25 completed in 68.82 seconds, Total Training Loss: 0.0003299296889943776\n",
      "\n",
      "Epoch 26/100\n",
      "Epoch 26, Batch 100/4948, Batch loss: 0.0001709880743874237\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 26, Batch 200/4948, Batch loss: 2.672282062121667e-05\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 26, Batch 300/4948, Batch loss: 0.0003246379492338747\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 26, Batch 400/4948, Batch loss: 0.00011716529843397439\n",
      "Memory Usage: 955.41 MB\n",
      "Epoch 26, Batch 500/4948, Batch loss: 5.639650407829322e-05\n",
      "Memory Usage: 955.56 MB\n",
      "Epoch 26, Batch 600/4948, Batch loss: 0.0004697974363807589\n",
      "Memory Usage: 912.91 MB\n",
      "Epoch 26, Batch 700/4948, Batch loss: 8.452855399809778e-05\n",
      "Memory Usage: 913.05 MB\n",
      "Epoch 26, Batch 800/4948, Batch loss: 0.00017812574515119195\n",
      "Memory Usage: 867.81 MB\n",
      "Epoch 26, Batch 900/4948, Batch loss: 0.0005512861534953117\n",
      "Memory Usage: 838.88 MB\n",
      "Epoch 26, Batch 1000/4948, Batch loss: 8.639131556265056e-05\n",
      "Memory Usage: 845.94 MB\n",
      "Epoch 26, Batch 1100/4948, Batch loss: 0.0008872421458363533\n",
      "Memory Usage: 847.42 MB\n",
      "Epoch 26, Batch 1200/4948, Batch loss: 0.0016442341729998589\n",
      "Memory Usage: 848.38 MB\n",
      "Epoch 26, Batch 1300/4948, Batch loss: 0.00015277217607945204\n",
      "Memory Usage: 843.67 MB\n",
      "Epoch 26, Batch 1400/4948, Batch loss: 0.00012390637130010873\n",
      "Memory Usage: 840.52 MB\n",
      "Epoch 26, Batch 1500/4948, Batch loss: 8.019346569199115e-05\n",
      "Memory Usage: 841.97 MB\n",
      "Epoch 26, Batch 1600/4948, Batch loss: 1.3478974324243609e-05\n",
      "Memory Usage: 837.61 MB\n",
      "Epoch 26, Batch 1700/4948, Batch loss: 0.0003741106193047017\n",
      "Memory Usage: 834.81 MB\n",
      "Epoch 26, Batch 1800/4948, Batch loss: 0.0002645317290443927\n",
      "Memory Usage: 832.06 MB\n",
      "Epoch 26, Batch 1900/4948, Batch loss: 0.004695334937423468\n",
      "Memory Usage: 829.03 MB\n",
      "Epoch 26, Batch 2000/4948, Batch loss: 0.00046246612328104675\n",
      "Memory Usage: 830.95 MB\n",
      "Epoch 26, Batch 2100/4948, Batch loss: 0.0007630984764546156\n",
      "Memory Usage: 837.75 MB\n",
      "Epoch 26, Batch 2200/4948, Batch loss: 0.00010494891466805711\n",
      "Memory Usage: 848.50 MB\n",
      "Epoch 26, Batch 2300/4948, Batch loss: 0.00022213641204871237\n",
      "Memory Usage: 849.05 MB\n",
      "Epoch 26, Batch 2400/4948, Batch loss: 0.00010697951802285388\n",
      "Memory Usage: 849.05 MB\n",
      "Epoch 26, Batch 2500/4948, Batch loss: 4.841419286094606e-05\n",
      "Memory Usage: 849.69 MB\n",
      "Epoch 26, Batch 2600/4948, Batch loss: 0.00038769893581047654\n",
      "Memory Usage: 850.17 MB\n",
      "Epoch 26, Batch 2700/4948, Batch loss: 0.0003945755888707936\n",
      "Memory Usage: 850.19 MB\n",
      "Epoch 26, Batch 2800/4948, Batch loss: 0.0015398241812363267\n",
      "Memory Usage: 850.20 MB\n",
      "Epoch 26, Batch 2900/4948, Batch loss: 0.0002009788149734959\n",
      "Memory Usage: 850.73 MB\n",
      "Epoch 26, Batch 3000/4948, Batch loss: 5.815802433062345e-05\n",
      "Memory Usage: 851.06 MB\n",
      "Epoch 26, Batch 3100/4948, Batch loss: 0.0001289175997953862\n",
      "Memory Usage: 851.06 MB\n",
      "Epoch 26, Batch 3200/4948, Batch loss: 0.0003230040892958641\n",
      "Memory Usage: 852.80 MB\n",
      "Epoch 26, Batch 3300/4948, Batch loss: 0.0001415996375726536\n",
      "Memory Usage: 852.80 MB\n",
      "Epoch 26, Batch 3400/4948, Batch loss: 9.874088391370606e-06\n",
      "Memory Usage: 852.80 MB\n",
      "Epoch 26, Batch 3500/4948, Batch loss: 4.921758591081016e-05\n",
      "Memory Usage: 852.80 MB\n",
      "Epoch 26, Batch 3600/4948, Batch loss: 4.361554601928219e-05\n",
      "Memory Usage: 852.80 MB\n",
      "Epoch 26, Batch 3700/4948, Batch loss: 0.0005427392316050828\n",
      "Memory Usage: 853.20 MB\n",
      "Epoch 26, Batch 3800/4948, Batch loss: 0.00010875506995944306\n",
      "Memory Usage: 853.22 MB\n",
      "Epoch 26, Batch 3900/4948, Batch loss: 0.000159985211212188\n",
      "Memory Usage: 853.55 MB\n",
      "Epoch 26, Batch 4000/4948, Batch loss: 0.00015037288540042937\n",
      "Memory Usage: 853.56 MB\n",
      "Epoch 26, Batch 4100/4948, Batch loss: 4.535193875199184e-05\n",
      "Memory Usage: 853.69 MB\n",
      "Epoch 26, Batch 4200/4948, Batch loss: 0.0010331615339964628\n",
      "Memory Usage: 853.69 MB\n",
      "Epoch 26, Batch 4300/4948, Batch loss: 0.00014552549691870809\n",
      "Memory Usage: 853.69 MB\n",
      "Epoch 26, Batch 4400/4948, Batch loss: 3.7285587950464105e-06\n",
      "Memory Usage: 853.69 MB\n",
      "Epoch 26, Batch 4500/4948, Batch loss: 0.0010085435351356864\n",
      "Memory Usage: 853.81 MB\n",
      "Epoch 26, Batch 4600/4948, Batch loss: 0.00024222087813541293\n",
      "Memory Usage: 854.77 MB\n",
      "Epoch 26, Batch 4700/4948, Batch loss: 0.0004298560961615294\n",
      "Memory Usage: 866.55 MB\n",
      "Epoch 26, Batch 4800/4948, Batch loss: 0.00011417581117711961\n",
      "Memory Usage: 878.38 MB\n",
      "Epoch 26, Batch 4900/4948, Batch loss: 7.092613668646663e-05\n",
      "Memory Usage: 890.75 MB\n",
      "Epoch 26, Batch 4948/4948, Batch loss: 0.001124757225625217\n",
      "Memory Usage: 895.67 MB\n",
      "Epoch 26 completed in 94.75 seconds, Total Training Loss: 0.0003249368009106027\n",
      "\n",
      "Epoch 27/100\n",
      "Epoch 27, Batch 100/4948, Batch loss: 0.00016942352522164583\n",
      "Memory Usage: 895.67 MB\n",
      "Epoch 27, Batch 200/4948, Batch loss: 1.450853596907109e-05\n",
      "Memory Usage: 895.69 MB\n",
      "Epoch 27, Batch 300/4948, Batch loss: 0.0003270051965955645\n",
      "Memory Usage: 857.53 MB\n",
      "Epoch 27, Batch 400/4948, Batch loss: 0.00011647747305687517\n",
      "Memory Usage: 859.97 MB\n",
      "Epoch 27, Batch 500/4948, Batch loss: 5.60210864932742e-05\n",
      "Memory Usage: 863.55 MB\n",
      "Epoch 27, Batch 600/4948, Batch loss: 0.00047072803135961294\n",
      "Memory Usage: 865.42 MB\n",
      "Epoch 27, Batch 700/4948, Batch loss: 5.526236418518238e-05\n",
      "Memory Usage: 867.69 MB\n",
      "Epoch 27, Batch 800/4948, Batch loss: 0.00012470866204239428\n",
      "Memory Usage: 873.89 MB\n",
      "Epoch 27, Batch 900/4948, Batch loss: 0.000540740555152297\n",
      "Memory Usage: 877.91 MB\n",
      "Epoch 27, Batch 1000/4948, Batch loss: 8.461999095743522e-05\n",
      "Memory Usage: 883.42 MB\n",
      "Epoch 27, Batch 1100/4948, Batch loss: 0.0008998855482786894\n",
      "Memory Usage: 884.03 MB\n",
      "Epoch 27, Batch 1200/4948, Batch loss: 0.0016394726699218154\n",
      "Memory Usage: 885.05 MB\n",
      "Epoch 27, Batch 1300/4948, Batch loss: 0.00015919614816084504\n",
      "Memory Usage: 888.00 MB\n",
      "Epoch 27, Batch 1400/4948, Batch loss: 0.0001223483996000141\n",
      "Memory Usage: 888.00 MB\n",
      "Epoch 27, Batch 1500/4948, Batch loss: 0.00016364679322578013\n",
      "Memory Usage: 888.02 MB\n",
      "Epoch 27, Batch 1600/4948, Batch loss: 1.3811620192427654e-05\n",
      "Memory Usage: 888.75 MB\n",
      "Epoch 27, Batch 1700/4948, Batch loss: 0.0005123173468746245\n",
      "Memory Usage: 890.27 MB\n",
      "Epoch 27, Batch 1800/4948, Batch loss: 0.0002653072588145733\n",
      "Memory Usage: 890.27 MB\n",
      "Epoch 27, Batch 1900/4948, Batch loss: 0.004826046526432037\n",
      "Memory Usage: 890.28 MB\n",
      "Epoch 27, Batch 2000/4948, Batch loss: 0.0005768680712208152\n",
      "Memory Usage: 890.30 MB\n",
      "Epoch 27, Batch 2100/4948, Batch loss: 0.0008815812179818749\n",
      "Memory Usage: 890.30 MB\n",
      "Epoch 27, Batch 2200/4948, Batch loss: 0.00010539535287534818\n",
      "Memory Usage: 890.30 MB\n",
      "Epoch 27, Batch 2300/4948, Batch loss: 0.0002235139545518905\n",
      "Memory Usage: 890.31 MB\n",
      "Epoch 27, Batch 2400/4948, Batch loss: 0.00010431367991259322\n",
      "Memory Usage: 890.31 MB\n",
      "Epoch 27, Batch 2500/4948, Batch loss: 4.9352122005075216e-05\n",
      "Memory Usage: 890.31 MB\n",
      "Epoch 27, Batch 2600/4948, Batch loss: 0.0003878259740304202\n",
      "Memory Usage: 890.31 MB\n",
      "Epoch 27, Batch 2700/4948, Batch loss: 0.00040160113712772727\n",
      "Memory Usage: 890.88 MB\n",
      "Epoch 27, Batch 2800/4948, Batch loss: 0.001541437697596848\n",
      "Memory Usage: 890.88 MB\n",
      "Epoch 27, Batch 2900/4948, Batch loss: 0.00020152280922047794\n",
      "Memory Usage: 890.88 MB\n",
      "Epoch 27, Batch 3000/4948, Batch loss: 5.7984929298982024e-05\n",
      "Memory Usage: 890.88 MB\n",
      "Epoch 27, Batch 3100/4948, Batch loss: 0.000122966812341474\n",
      "Memory Usage: 890.88 MB\n",
      "Epoch 27, Batch 3200/4948, Batch loss: 0.0003241936501581222\n",
      "Memory Usage: 890.89 MB\n",
      "Epoch 27, Batch 3300/4948, Batch loss: 0.0001412658457411453\n",
      "Memory Usage: 890.91 MB\n",
      "Epoch 27, Batch 3400/4948, Batch loss: 5.421215519163525e-06\n",
      "Memory Usage: 890.91 MB\n",
      "Epoch 27, Batch 3500/4948, Batch loss: 4.8672820412321016e-05\n",
      "Memory Usage: 890.91 MB\n",
      "Epoch 27, Batch 3600/4948, Batch loss: 4.075282049598172e-05\n",
      "Memory Usage: 890.91 MB\n",
      "Epoch 27, Batch 3700/4948, Batch loss: 0.000550515134818852\n",
      "Memory Usage: 876.02 MB\n",
      "Epoch 27, Batch 3800/4948, Batch loss: 0.00010714519885368645\n",
      "Memory Usage: 859.47 MB\n",
      "Epoch 27, Batch 3900/4948, Batch loss: 0.00014152585936244577\n",
      "Memory Usage: 859.81 MB\n",
      "Epoch 27, Batch 4000/4948, Batch loss: 0.000151050349813886\n",
      "Memory Usage: 859.81 MB\n",
      "Epoch 27, Batch 4100/4948, Batch loss: 4.58540853287559e-05\n",
      "Memory Usage: 859.81 MB\n",
      "Epoch 27, Batch 4200/4948, Batch loss: 0.0010410664835944772\n",
      "Memory Usage: 859.83 MB\n",
      "Epoch 27, Batch 4300/4948, Batch loss: 0.00014476667274720967\n",
      "Memory Usage: 859.86 MB\n",
      "Epoch 27, Batch 4400/4948, Batch loss: 3.4725417208392173e-06\n",
      "Memory Usage: 859.88 MB\n",
      "Epoch 27, Batch 4500/4948, Batch loss: 0.0009995590662583709\n",
      "Memory Usage: 859.88 MB\n",
      "Epoch 27, Batch 4600/4948, Batch loss: 0.00024225818924605846\n",
      "Memory Usage: 859.88 MB\n",
      "Epoch 27, Batch 4700/4948, Batch loss: 0.0004345430643297732\n",
      "Memory Usage: 859.88 MB\n",
      "Epoch 27, Batch 4800/4948, Batch loss: 0.00017837306950241327\n",
      "Memory Usage: 859.88 MB\n",
      "Epoch 27, Batch 4900/4948, Batch loss: 7.261786959134042e-05\n",
      "Memory Usage: 859.98 MB\n",
      "Epoch 27, Batch 4948/4948, Batch loss: 0.0011208507930859923\n",
      "Memory Usage: 860.14 MB\n",
      "Epoch 27 completed in 107.44 seconds, Total Training Loss: 0.00032769771074512606\n",
      "\n",
      "Epoch 28/100\n",
      "Epoch 28, Batch 100/4948, Batch loss: 0.00017014476179610938\n",
      "Memory Usage: 866.58 MB\n",
      "Epoch 28, Batch 200/4948, Batch loss: 1.622834497538861e-05\n",
      "Memory Usage: 877.16 MB\n",
      "Epoch 28, Batch 300/4948, Batch loss: 0.0003229469002690166\n",
      "Memory Usage: 885.00 MB\n",
      "Epoch 28, Batch 400/4948, Batch loss: 0.00011652777902781963\n",
      "Memory Usage: 885.00 MB\n",
      "Epoch 28, Batch 500/4948, Batch loss: 5.1738392357947305e-05\n",
      "Memory Usage: 885.02 MB\n",
      "Epoch 28, Batch 600/4948, Batch loss: 0.00047236340469680727\n",
      "Memory Usage: 885.02 MB\n",
      "Epoch 28, Batch 700/4948, Batch loss: 5.79214101890102e-05\n",
      "Memory Usage: 885.86 MB\n",
      "Epoch 28, Batch 800/4948, Batch loss: 0.00015405156591441482\n",
      "Memory Usage: 886.84 MB\n",
      "Epoch 28, Batch 900/4948, Batch loss: 0.0005565602914430201\n",
      "Memory Usage: 886.84 MB\n",
      "Epoch 28, Batch 1000/4948, Batch loss: 8.86506459210068e-05\n",
      "Memory Usage: 886.84 MB\n",
      "Epoch 28, Batch 1100/4948, Batch loss: 0.0008857740322127938\n",
      "Memory Usage: 886.84 MB\n",
      "Epoch 28, Batch 1200/4948, Batch loss: 0.0016199183883145452\n",
      "Memory Usage: 886.84 MB\n",
      "Epoch 28, Batch 1300/4948, Batch loss: 0.00015428726328536868\n",
      "Memory Usage: 886.84 MB\n",
      "Epoch 28, Batch 1400/4948, Batch loss: 0.0001239571865880862\n",
      "Memory Usage: 886.84 MB\n",
      "Epoch 28, Batch 1500/4948, Batch loss: 9.504621993983164e-05\n",
      "Memory Usage: 886.84 MB\n",
      "Epoch 28, Batch 1600/4948, Batch loss: 1.5712383174104616e-05\n",
      "Memory Usage: 886.84 MB\n",
      "Epoch 28, Batch 1700/4948, Batch loss: 0.00047309548244811594\n",
      "Memory Usage: 887.34 MB\n",
      "Epoch 28, Batch 1800/4948, Batch loss: 0.0002658480661921203\n",
      "Memory Usage: 887.34 MB\n",
      "Epoch 28, Batch 1900/4948, Batch loss: 0.004240660462528467\n",
      "Memory Usage: 887.72 MB\n",
      "Epoch 28, Batch 2000/4948, Batch loss: 0.00045952567597851157\n",
      "Memory Usage: 887.72 MB\n",
      "Epoch 28, Batch 2100/4948, Batch loss: 0.0008072662749327719\n",
      "Memory Usage: 887.75 MB\n",
      "Epoch 28, Batch 2200/4948, Batch loss: 0.00010615324572427198\n",
      "Memory Usage: 887.77 MB\n",
      "Epoch 28, Batch 2300/4948, Batch loss: 0.0002257962478324771\n",
      "Memory Usage: 887.78 MB\n",
      "Epoch 28, Batch 2400/4948, Batch loss: 0.00010528452548896894\n",
      "Memory Usage: 887.78 MB\n",
      "Epoch 28, Batch 2500/4948, Batch loss: 4.941908991895616e-05\n",
      "Memory Usage: 887.78 MB\n",
      "Epoch 28, Batch 2600/4948, Batch loss: 0.00038420813507400453\n",
      "Memory Usage: 887.78 MB\n",
      "Epoch 28, Batch 2700/4948, Batch loss: 0.00038921006489545107\n",
      "Memory Usage: 887.78 MB\n",
      "Epoch 28, Batch 2800/4948, Batch loss: 0.0015377677045762539\n",
      "Memory Usage: 887.78 MB\n",
      "Epoch 28, Batch 2900/4948, Batch loss: 0.0002011886244872585\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 3000/4948, Batch loss: 5.801760562462732e-05\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 3100/4948, Batch loss: 0.00012512406101450324\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 3200/4948, Batch loss: 0.00032645766623318195\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 3300/4948, Batch loss: 0.00014276558067649603\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 3400/4948, Batch loss: 7.1142962951853406e-06\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 3500/4948, Batch loss: 4.948144487570971e-05\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 3600/4948, Batch loss: 4.139546581427567e-05\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 3700/4948, Batch loss: 0.0005498226382769644\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 3800/4948, Batch loss: 0.00010809850209625438\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 3900/4948, Batch loss: 0.00013769802171736956\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 4000/4948, Batch loss: 0.00014990103954914957\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 4100/4948, Batch loss: 5.5736032663844526e-05\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 4200/4948, Batch loss: 0.0010390712413936853\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 4300/4948, Batch loss: 0.00014476549404207617\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 4400/4948, Batch loss: 3.737879978871206e-06\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 4500/4948, Batch loss: 0.0010078075574710965\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 4600/4948, Batch loss: 0.0002469810133334249\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 4700/4948, Batch loss: 0.0004333638062234968\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 4800/4948, Batch loss: 0.00011005353735527024\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 4900/4948, Batch loss: 7.115782500477508e-05\n",
      "Memory Usage: 887.80 MB\n",
      "Epoch 28, Batch 4948/4948, Batch loss: 0.0011376808397471905\n",
      "Memory Usage: 887.83 MB\n",
      "Epoch 28 completed in 66.74 seconds, Total Training Loss: 0.0003214838247843039\n",
      "\n",
      "Epoch 29/100\n",
      "Epoch 29, Batch 100/4948, Batch loss: 0.00017104129074141383\n",
      "Memory Usage: 887.83 MB\n",
      "Epoch 29, Batch 200/4948, Batch loss: 1.2137860721850302e-05\n",
      "Memory Usage: 887.83 MB\n",
      "Epoch 29, Batch 300/4948, Batch loss: 0.0003235770855098963\n",
      "Memory Usage: 887.83 MB\n",
      "Epoch 29, Batch 400/4948, Batch loss: 0.00011669153172988445\n",
      "Memory Usage: 887.83 MB\n",
      "Epoch 29, Batch 500/4948, Batch loss: 5.339998460840434e-05\n",
      "Memory Usage: 887.83 MB\n",
      "Epoch 29, Batch 600/4948, Batch loss: 0.0004734041285701096\n",
      "Memory Usage: 887.83 MB\n",
      "Epoch 29, Batch 700/4948, Batch loss: 5.594078902504407e-05\n",
      "Memory Usage: 889.36 MB\n",
      "Epoch 29, Batch 800/4948, Batch loss: 0.00012448806955944747\n",
      "Memory Usage: 889.36 MB\n",
      "Epoch 29, Batch 900/4948, Batch loss: 0.000547136995010078\n",
      "Memory Usage: 889.36 MB\n",
      "Epoch 29, Batch 1000/4948, Batch loss: 8.643070759717375e-05\n",
      "Memory Usage: 889.36 MB\n",
      "Epoch 29, Batch 1100/4948, Batch loss: 0.0008949038456194103\n",
      "Memory Usage: 889.39 MB\n",
      "Epoch 29, Batch 1200/4948, Batch loss: 0.0016133253229781985\n",
      "Memory Usage: 889.39 MB\n",
      "Epoch 29, Batch 1300/4948, Batch loss: 0.00015490192163269967\n",
      "Memory Usage: 889.39 MB\n",
      "Epoch 29, Batch 1400/4948, Batch loss: 0.00012398845865391195\n",
      "Memory Usage: 873.77 MB\n",
      "Epoch 29, Batch 1500/4948, Batch loss: 9.36458891374059e-05\n",
      "Memory Usage: 863.89 MB\n",
      "Epoch 29, Batch 1600/4948, Batch loss: 1.4362960428115912e-05\n",
      "Memory Usage: 769.86 MB\n",
      "Epoch 29, Batch 1700/4948, Batch loss: 0.0004906580434180796\n",
      "Memory Usage: 772.14 MB\n",
      "Epoch 29, Batch 1800/4948, Batch loss: 0.00026623273151926696\n",
      "Memory Usage: 778.20 MB\n",
      "Epoch 29, Batch 1900/4948, Batch loss: 0.0016376280691474676\n",
      "Memory Usage: 778.20 MB\n",
      "Epoch 29, Batch 2000/4948, Batch loss: 0.00042212478001601994\n",
      "Memory Usage: 780.70 MB\n",
      "Epoch 29, Batch 2100/4948, Batch loss: 0.0006851692451164126\n",
      "Memory Usage: 783.19 MB\n",
      "Epoch 29, Batch 2200/4948, Batch loss: 0.00010709233902161941\n",
      "Memory Usage: 785.64 MB\n",
      "Epoch 29, Batch 2300/4948, Batch loss: 0.000226431482587941\n",
      "Memory Usage: 785.64 MB\n",
      "Epoch 29, Batch 2400/4948, Batch loss: 0.00010674803343135864\n",
      "Memory Usage: 788.52 MB\n",
      "Epoch 29, Batch 2500/4948, Batch loss: 4.8306472308468074e-05\n",
      "Memory Usage: 699.23 MB\n",
      "Epoch 29, Batch 2600/4948, Batch loss: 0.0003902512835338712\n",
      "Memory Usage: 689.59 MB\n",
      "Epoch 29, Batch 2700/4948, Batch loss: 0.0004032226570416242\n",
      "Memory Usage: 637.38 MB\n",
      "Epoch 29, Batch 2800/4948, Batch loss: 0.0015248864656314254\n",
      "Memory Usage: 639.69 MB\n",
      "Epoch 29, Batch 2900/4948, Batch loss: 0.0002126233302988112\n",
      "Memory Usage: 642.97 MB\n",
      "Epoch 29, Batch 3000/4948, Batch loss: 6.27337212790735e-05\n",
      "Memory Usage: 646.88 MB\n",
      "Epoch 29, Batch 3100/4948, Batch loss: 0.00012369711475912482\n",
      "Memory Usage: 648.05 MB\n",
      "Epoch 29, Batch 3200/4948, Batch loss: 0.000324215303407982\n",
      "Memory Usage: 654.09 MB\n",
      "Epoch 29, Batch 3300/4948, Batch loss: 0.00014461035607382655\n",
      "Memory Usage: 654.59 MB\n",
      "Epoch 29, Batch 3400/4948, Batch loss: 5.863117166882148e-06\n",
      "Memory Usage: 655.38 MB\n",
      "Epoch 29, Batch 3500/4948, Batch loss: 5.41237423021812e-05\n",
      "Memory Usage: 656.72 MB\n",
      "Epoch 29, Batch 3600/4948, Batch loss: 4.127949068788439e-05\n",
      "Memory Usage: 656.72 MB\n",
      "Epoch 29, Batch 3700/4948, Batch loss: 0.0005457081715576351\n",
      "Memory Usage: 657.30 MB\n",
      "Epoch 29, Batch 3800/4948, Batch loss: 0.00010937597107840702\n",
      "Memory Usage: 658.17 MB\n",
      "Epoch 29, Batch 3900/4948, Batch loss: 0.00014147079491522163\n",
      "Memory Usage: 664.69 MB\n",
      "Epoch 29, Batch 4000/4948, Batch loss: 0.00015078742580953985\n",
      "Memory Usage: 676.44 MB\n",
      "Epoch 29, Batch 4100/4948, Batch loss: 4.501141302171163e-05\n",
      "Memory Usage: 688.70 MB\n",
      "Epoch 29, Batch 4200/4948, Batch loss: 0.0010363338515162468\n",
      "Memory Usage: 700.52 MB\n",
      "Epoch 29, Batch 4300/4948, Batch loss: 0.0001458222686778754\n",
      "Memory Usage: 712.97 MB\n",
      "Epoch 29, Batch 4400/4948, Batch loss: 3.90212608181173e-06\n",
      "Memory Usage: 727.02 MB\n",
      "Epoch 29, Batch 4500/4948, Batch loss: 0.001003609853796661\n",
      "Memory Usage: 738.88 MB\n",
      "Epoch 29, Batch 4600/4948, Batch loss: 0.00024832115741446614\n",
      "Memory Usage: 751.98 MB\n",
      "Epoch 29, Batch 4700/4948, Batch loss: 0.0004266431787982583\n",
      "Memory Usage: 763.80 MB\n",
      "Epoch 29, Batch 4800/4948, Batch loss: 0.00010907214891631156\n",
      "Memory Usage: 776.14 MB\n",
      "Epoch 29, Batch 4900/4948, Batch loss: 7.085876859491691e-05\n",
      "Memory Usage: 788.00 MB\n",
      "Epoch 29, Batch 4948/4948, Batch loss: 0.0011205202899873257\n",
      "Memory Usage: 793.55 MB\n",
      "Epoch 29 completed in 69.74 seconds, Total Training Loss: 0.00030598994640456556\n",
      "\n",
      "Epoch 30/100\n",
      "Epoch 30, Batch 100/4948, Batch loss: 0.00017116889648605138\n",
      "Memory Usage: 793.73 MB\n",
      "Epoch 30, Batch 200/4948, Batch loss: 1.4172695955494419e-05\n",
      "Memory Usage: 793.75 MB\n",
      "Epoch 30, Batch 300/4948, Batch loss: 0.0003204173408448696\n",
      "Memory Usage: 794.00 MB\n",
      "Epoch 30, Batch 400/4948, Batch loss: 0.00011661323514999822\n",
      "Memory Usage: 794.00 MB\n",
      "Epoch 30, Batch 500/4948, Batch loss: 5.1074664952466264e-05\n",
      "Memory Usage: 794.52 MB\n",
      "Epoch 30, Batch 600/4948, Batch loss: 0.0004686900938395411\n",
      "Memory Usage: 794.52 MB\n",
      "Epoch 30, Batch 700/4948, Batch loss: 5.374803367885761e-05\n",
      "Memory Usage: 794.52 MB\n",
      "Epoch 30, Batch 800/4948, Batch loss: 0.00014464066771324724\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 30, Batch 900/4948, Batch loss: 0.0005437399377115071\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 30, Batch 1000/4948, Batch loss: 8.783605881035328e-05\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 30, Batch 1100/4948, Batch loss: 0.0008862591930665076\n",
      "Memory Usage: 794.95 MB\n",
      "Epoch 30, Batch 1200/4948, Batch loss: 0.0016106070252135396\n",
      "Memory Usage: 794.95 MB\n",
      "Epoch 30, Batch 1300/4948, Batch loss: 0.0001596908550709486\n",
      "Memory Usage: 795.47 MB\n",
      "Epoch 30, Batch 1400/4948, Batch loss: 0.0001233074872288853\n",
      "Memory Usage: 795.48 MB\n",
      "Epoch 30, Batch 1500/4948, Batch loss: 9.242751548299566e-05\n",
      "Memory Usage: 795.50 MB\n",
      "Epoch 30, Batch 1600/4948, Batch loss: 1.6866240912349895e-05\n",
      "Memory Usage: 795.52 MB\n",
      "Epoch 30, Batch 1700/4948, Batch loss: 0.0005425589042715728\n",
      "Memory Usage: 795.55 MB\n",
      "Epoch 30, Batch 1800/4948, Batch loss: 0.0002648071967996657\n",
      "Memory Usage: 795.58 MB\n",
      "Epoch 30, Batch 1900/4948, Batch loss: 0.004610844422131777\n",
      "Memory Usage: 795.61 MB\n",
      "Epoch 30, Batch 2000/4948, Batch loss: 0.00045023614075034857\n",
      "Memory Usage: 795.77 MB\n",
      "Epoch 30, Batch 2100/4948, Batch loss: 0.0007245074375532568\n",
      "Memory Usage: 795.78 MB\n",
      "Epoch 30, Batch 2200/4948, Batch loss: 0.00010586898133624345\n",
      "Memory Usage: 795.78 MB\n",
      "Epoch 30, Batch 2300/4948, Batch loss: 0.00022516390890814364\n",
      "Memory Usage: 795.78 MB\n",
      "Epoch 30, Batch 2400/4948, Batch loss: 0.00010615045175654814\n",
      "Memory Usage: 795.78 MB\n",
      "Epoch 30, Batch 2500/4948, Batch loss: 5.0466336688259616e-05\n",
      "Memory Usage: 796.36 MB\n",
      "Epoch 30, Batch 2600/4948, Batch loss: 0.0003884668694809079\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 2700/4948, Batch loss: 0.00038247465272434056\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 2800/4948, Batch loss: 0.0015103417681530118\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 2900/4948, Batch loss: 0.00020027215941809118\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 3000/4948, Batch loss: 5.79539846512489e-05\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 3100/4948, Batch loss: 0.00012326092110015452\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 3200/4948, Batch loss: 0.00032707289210520685\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 3300/4948, Batch loss: 0.00014154404925648123\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 3400/4948, Batch loss: 5.158789463166613e-06\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 3500/4948, Batch loss: 4.7734047257108614e-05\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 3600/4948, Batch loss: 4.176788934273645e-05\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 3700/4948, Batch loss: 0.0005433987826108932\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 3800/4948, Batch loss: 0.00010735407704487443\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 3900/4948, Batch loss: 0.00015017554687801749\n",
      "Memory Usage: 797.36 MB\n",
      "Epoch 30, Batch 4000/4948, Batch loss: 0.00014932319754734635\n",
      "Memory Usage: 797.38 MB\n",
      "Epoch 30, Batch 4100/4948, Batch loss: 4.664615335059352e-05\n",
      "Memory Usage: 797.38 MB\n",
      "Epoch 30, Batch 4200/4948, Batch loss: 0.0010289668571203947\n",
      "Memory Usage: 796.66 MB\n",
      "Epoch 30, Batch 4300/4948, Batch loss: 0.0001451819553039968\n",
      "Memory Usage: 797.00 MB\n",
      "Epoch 30, Batch 4400/4948, Batch loss: 3.542788817867404e-06\n",
      "Memory Usage: 797.25 MB\n",
      "Epoch 30, Batch 4500/4948, Batch loss: 0.0009957026923075318\n",
      "Memory Usage: 797.25 MB\n",
      "Epoch 30, Batch 4600/4948, Batch loss: 0.0002430742752039805\n",
      "Memory Usage: 797.25 MB\n",
      "Epoch 30, Batch 4700/4948, Batch loss: 0.0004301067965570837\n",
      "Memory Usage: 797.30 MB\n",
      "Epoch 30, Batch 4800/4948, Batch loss: 0.0001104622715502046\n",
      "Memory Usage: 797.39 MB\n",
      "Epoch 30, Batch 4900/4948, Batch loss: 7.239948899950832e-05\n",
      "Memory Usage: 797.41 MB\n",
      "Epoch 30, Batch 4948/4948, Batch loss: 0.001125922310166061\n",
      "Memory Usage: 797.41 MB\n",
      "Epoch 30 completed in 69.93 seconds, Total Training Loss: 0.00031986925367597137\n",
      "\n",
      "Epoch 31/100\n",
      "Epoch 31, Batch 100/4948, Batch loss: 0.0001705663453321904\n",
      "Memory Usage: 797.44 MB\n",
      "Epoch 31, Batch 200/4948, Batch loss: 1.9193232219549827e-05\n",
      "Memory Usage: 797.44 MB\n",
      "Epoch 31, Batch 300/4948, Batch loss: 0.00031969640986062586\n",
      "Memory Usage: 797.44 MB\n",
      "Epoch 31, Batch 400/4948, Batch loss: 0.00011637840361800045\n",
      "Memory Usage: 797.44 MB\n",
      "Epoch 31, Batch 500/4948, Batch loss: 5.114633677294478e-05\n",
      "Memory Usage: 797.44 MB\n",
      "Epoch 31, Batch 600/4948, Batch loss: 0.000471110746730119\n",
      "Memory Usage: 797.45 MB\n",
      "Epoch 31, Batch 700/4948, Batch loss: 5.538756158784963e-05\n",
      "Memory Usage: 797.45 MB\n",
      "Epoch 31, Batch 800/4948, Batch loss: 0.0001423294161213562\n",
      "Memory Usage: 797.45 MB\n",
      "Epoch 31, Batch 900/4948, Batch loss: 0.0005433147889561951\n",
      "Memory Usage: 797.48 MB\n",
      "Epoch 31, Batch 1000/4948, Batch loss: 8.718029857845977e-05\n",
      "Memory Usage: 797.50 MB\n",
      "Epoch 31, Batch 1100/4948, Batch loss: 0.0008806248661130667\n",
      "Memory Usage: 797.52 MB\n",
      "Epoch 31, Batch 1200/4948, Batch loss: 0.0016015676083043218\n",
      "Memory Usage: 797.52 MB\n",
      "Epoch 31, Batch 1300/4948, Batch loss: 0.00015555504069197923\n",
      "Memory Usage: 797.52 MB\n",
      "Epoch 31, Batch 1400/4948, Batch loss: 0.0001229393674293533\n",
      "Memory Usage: 797.52 MB\n",
      "Epoch 31, Batch 1500/4948, Batch loss: 8.764332596911117e-05\n",
      "Memory Usage: 797.52 MB\n",
      "Epoch 31, Batch 1600/4948, Batch loss: 1.4995938727224711e-05\n",
      "Memory Usage: 797.52 MB\n",
      "Epoch 31, Batch 1700/4948, Batch loss: 0.0005366622935980558\n",
      "Memory Usage: 797.52 MB\n",
      "Epoch 31, Batch 1800/4948, Batch loss: 0.0002640100719872862\n",
      "Memory Usage: 797.52 MB\n",
      "Epoch 31, Batch 1900/4948, Batch loss: 0.004800193011760712\n",
      "Memory Usage: 797.52 MB\n",
      "Epoch 31, Batch 2000/4948, Batch loss: 0.0005288855754770339\n",
      "Memory Usage: 797.52 MB\n",
      "Epoch 31, Batch 2100/4948, Batch loss: 0.000809664314147085\n",
      "Memory Usage: 797.52 MB\n",
      "Epoch 31, Batch 2200/4948, Batch loss: 0.0001054663080140017\n",
      "Memory Usage: 797.52 MB\n",
      "Epoch 31, Batch 2300/4948, Batch loss: 0.00022355871624313295\n",
      "Memory Usage: 789.88 MB\n",
      "Epoch 31, Batch 2400/4948, Batch loss: 0.00010499467316549271\n",
      "Memory Usage: 787.00 MB\n",
      "Epoch 31, Batch 2500/4948, Batch loss: 4.91445061925333e-05\n",
      "Memory Usage: 796.50 MB\n",
      "Epoch 31, Batch 2600/4948, Batch loss: 0.0003841929428745061\n",
      "Memory Usage: 797.27 MB\n",
      "Epoch 31, Batch 2700/4948, Batch loss: 0.00037757845711894333\n",
      "Memory Usage: 797.27 MB\n",
      "Epoch 31, Batch 2800/4948, Batch loss: 0.0015196215827018023\n",
      "Memory Usage: 797.42 MB\n",
      "Epoch 31, Batch 2900/4948, Batch loss: 0.00020125729497522116\n",
      "Memory Usage: 797.42 MB\n",
      "Epoch 31, Batch 3000/4948, Batch loss: 5.79103289055638e-05\n",
      "Memory Usage: 797.42 MB\n",
      "Epoch 31, Batch 3100/4948, Batch loss: 0.0001233998773386702\n",
      "Memory Usage: 797.42 MB\n",
      "Epoch 31, Batch 3200/4948, Batch loss: 0.0003363015130162239\n",
      "Memory Usage: 797.42 MB\n",
      "Epoch 31, Batch 3300/4948, Batch loss: 0.00014087096496950835\n",
      "Memory Usage: 797.44 MB\n",
      "Epoch 31, Batch 3400/4948, Batch loss: 6.486554411821999e-06\n",
      "Memory Usage: 797.44 MB\n",
      "Epoch 31, Batch 3500/4948, Batch loss: 4.72825558972545e-05\n",
      "Memory Usage: 797.44 MB\n",
      "Epoch 31, Batch 3600/4948, Batch loss: 4.146199353272095e-05\n",
      "Memory Usage: 797.92 MB\n",
      "Epoch 31, Batch 3700/4948, Batch loss: 0.0005454907659441233\n",
      "Memory Usage: 797.92 MB\n",
      "Epoch 31, Batch 3800/4948, Batch loss: 0.00010694344382500276\n",
      "Memory Usage: 797.92 MB\n",
      "Epoch 31, Batch 3900/4948, Batch loss: 0.0001367208460578695\n",
      "Memory Usage: 797.92 MB\n",
      "Epoch 31, Batch 4000/4948, Batch loss: 0.00015265654656104743\n",
      "Memory Usage: 797.94 MB\n",
      "Epoch 31, Batch 4100/4948, Batch loss: 4.952326344209723e-05\n",
      "Memory Usage: 797.94 MB\n",
      "Epoch 31, Batch 4200/4948, Batch loss: 0.0010390841634944081\n",
      "Memory Usage: 797.53 MB\n",
      "Epoch 31, Batch 4300/4948, Batch loss: 0.0001442147622583434\n",
      "Memory Usage: 797.75 MB\n",
      "Epoch 31, Batch 4400/4948, Batch loss: 3.848006599582732e-06\n",
      "Memory Usage: 797.75 MB\n",
      "Epoch 31, Batch 4500/4948, Batch loss: 0.0010027793468907475\n",
      "Memory Usage: 797.75 MB\n",
      "Epoch 31, Batch 4600/4948, Batch loss: 0.0002447861188556999\n",
      "Memory Usage: 797.75 MB\n",
      "Epoch 31, Batch 4700/4948, Batch loss: 0.00042910801130346954\n",
      "Memory Usage: 798.00 MB\n",
      "Epoch 31, Batch 4800/4948, Batch loss: 0.00011027818982256576\n",
      "Memory Usage: 798.00 MB\n",
      "Epoch 31, Batch 4900/4948, Batch loss: 7.150855526560917e-05\n",
      "Memory Usage: 798.00 MB\n",
      "Epoch 31, Batch 4948/4948, Batch loss: 0.0011386941187083721\n",
      "Memory Usage: 798.02 MB\n",
      "Epoch 31 completed in 72.93 seconds, Total Training Loss: 0.0003231392553707139\n",
      "Validation completed in 3.63 seconds, Average Validation Loss: 0.000500623974825236\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 32/100\n",
      "Epoch 32, Batch 100/4948, Batch loss: 0.000170701983734034\n",
      "Memory Usage: 935.69 MB\n",
      "Epoch 32, Batch 200/4948, Batch loss: 1.7371181456837803e-05\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 32, Batch 300/4948, Batch loss: 0.00031871412647888064\n",
      "Memory Usage: 935.67 MB\n",
      "Epoch 32, Batch 400/4948, Batch loss: 0.00011600583093240857\n",
      "Memory Usage: 935.69 MB\n",
      "Epoch 32, Batch 500/4948, Batch loss: 5.1117909606546164e-05\n",
      "Memory Usage: 935.69 MB\n",
      "Epoch 32, Batch 600/4948, Batch loss: 0.00046872117673046887\n",
      "Memory Usage: 935.69 MB\n",
      "Epoch 32, Batch 700/4948, Batch loss: 6.622586806770414e-05\n",
      "Memory Usage: 935.69 MB\n",
      "Epoch 32, Batch 800/4948, Batch loss: 0.00012918934226036072\n",
      "Memory Usage: 935.69 MB\n",
      "Epoch 32, Batch 900/4948, Batch loss: 0.0005418200162239373\n",
      "Memory Usage: 935.69 MB\n",
      "Epoch 32, Batch 1000/4948, Batch loss: 8.527124737156555e-05\n",
      "Memory Usage: 935.70 MB\n",
      "Epoch 32, Batch 1100/4948, Batch loss: 0.0009118517627939582\n",
      "Memory Usage: 935.72 MB\n",
      "Epoch 32, Batch 1200/4948, Batch loss: 0.0015859749400988221\n",
      "Memory Usage: 935.72 MB\n",
      "Epoch 32, Batch 1300/4948, Batch loss: 0.0001605173893040046\n",
      "Memory Usage: 935.72 MB\n",
      "Epoch 32, Batch 1400/4948, Batch loss: 0.00012359140964690596\n",
      "Memory Usage: 935.72 MB\n",
      "Epoch 32, Batch 1500/4948, Batch loss: 8.566928590880707e-05\n",
      "Memory Usage: 935.72 MB\n",
      "Epoch 32, Batch 1600/4948, Batch loss: 1.427262668585172e-05\n",
      "Memory Usage: 931.66 MB\n",
      "Epoch 32, Batch 1700/4948, Batch loss: 0.0005885420250706375\n",
      "Memory Usage: 931.66 MB\n",
      "Epoch 32, Batch 1800/4948, Batch loss: 0.00026232184609398246\n",
      "Memory Usage: 931.66 MB\n",
      "Epoch 32, Batch 1900/4948, Batch loss: 0.0034858391154557467\n",
      "Memory Usage: 932.41 MB\n",
      "Epoch 32, Batch 2000/4948, Batch loss: 0.0004121584934182465\n",
      "Memory Usage: 932.41 MB\n",
      "Epoch 32, Batch 2100/4948, Batch loss: 0.0006902223685756326\n",
      "Memory Usage: 932.41 MB\n",
      "Epoch 32, Batch 2200/4948, Batch loss: 0.00010645575093803927\n",
      "Memory Usage: 932.66 MB\n",
      "Epoch 32, Batch 2300/4948, Batch loss: 0.00022492361313197762\n",
      "Memory Usage: 933.41 MB\n",
      "Epoch 32, Batch 2400/4948, Batch loss: 0.00010513453162275255\n",
      "Memory Usage: 933.41 MB\n",
      "Epoch 32, Batch 2500/4948, Batch loss: 4.796078792423941e-05\n",
      "Memory Usage: 933.41 MB\n",
      "Epoch 32, Batch 2600/4948, Batch loss: 0.0003894615510944277\n",
      "Memory Usage: 933.03 MB\n",
      "Epoch 32, Batch 2700/4948, Batch loss: 0.00039420544635504484\n",
      "Memory Usage: 933.16 MB\n",
      "Epoch 32, Batch 2800/4948, Batch loss: 0.0015229610726237297\n",
      "Memory Usage: 933.47 MB\n",
      "Epoch 32, Batch 2900/4948, Batch loss: 0.00020438792125787586\n",
      "Memory Usage: 933.48 MB\n",
      "Epoch 32, Batch 3000/4948, Batch loss: 5.903618148295209e-05\n",
      "Memory Usage: 933.48 MB\n",
      "Epoch 32, Batch 3100/4948, Batch loss: 0.00012569307000376284\n",
      "Memory Usage: 934.02 MB\n",
      "Epoch 32, Batch 3200/4948, Batch loss: 0.00031136273173615336\n",
      "Memory Usage: 934.02 MB\n",
      "Epoch 32, Batch 3300/4948, Batch loss: 0.00014078093226999044\n",
      "Memory Usage: 934.20 MB\n",
      "Epoch 32, Batch 3400/4948, Batch loss: 4.765288849739591e-06\n",
      "Memory Usage: 934.20 MB\n",
      "Epoch 32, Batch 3500/4948, Batch loss: 4.569144221022725e-05\n",
      "Memory Usage: 934.72 MB\n",
      "Epoch 32, Batch 3600/4948, Batch loss: 3.9851318433647975e-05\n",
      "Memory Usage: 934.72 MB\n",
      "Epoch 32, Batch 3700/4948, Batch loss: 0.0005462642875500023\n",
      "Memory Usage: 934.73 MB\n",
      "Epoch 32, Batch 3800/4948, Batch loss: 0.00010832410771399736\n",
      "Memory Usage: 934.73 MB\n",
      "Epoch 32, Batch 3900/4948, Batch loss: 0.00013925546954851598\n",
      "Memory Usage: 934.73 MB\n",
      "Epoch 32, Batch 4000/4948, Batch loss: 0.00014886705321259797\n",
      "Memory Usage: 934.73 MB\n",
      "Epoch 32, Batch 4100/4948, Batch loss: 4.6889988880138844e-05\n",
      "Memory Usage: 934.73 MB\n",
      "Epoch 32, Batch 4200/4948, Batch loss: 0.0010231839260086417\n",
      "Memory Usage: 934.73 MB\n",
      "Epoch 32, Batch 4300/4948, Batch loss: 0.00014381435175891966\n",
      "Memory Usage: 934.77 MB\n",
      "Epoch 32, Batch 4400/4948, Batch loss: 4.344268745626323e-06\n",
      "Memory Usage: 934.77 MB\n",
      "Epoch 32, Batch 4500/4948, Batch loss: 0.0010055555030703545\n",
      "Memory Usage: 934.77 MB\n",
      "Epoch 32, Batch 4600/4948, Batch loss: 0.00025224548880942166\n",
      "Memory Usage: 934.77 MB\n",
      "Epoch 32, Batch 4700/4948, Batch loss: 0.00042604695772752166\n",
      "Memory Usage: 934.77 MB\n",
      "Epoch 32, Batch 4800/4948, Batch loss: 0.00011042899859603494\n",
      "Memory Usage: 934.77 MB\n",
      "Epoch 32, Batch 4900/4948, Batch loss: 7.219670078484342e-05\n",
      "Memory Usage: 934.77 MB\n",
      "Epoch 32, Batch 4948/4948, Batch loss: 0.0010971722658723593\n",
      "Memory Usage: 934.80 MB\n",
      "Epoch 32 completed in 69.10 seconds, Total Training Loss: 0.00031014592598066905\n",
      "\n",
      "Epoch 33/100\n",
      "Epoch 33, Batch 100/4948, Batch loss: 0.00017060832760762423\n",
      "Memory Usage: 934.80 MB\n",
      "Epoch 33, Batch 200/4948, Batch loss: 1.7102684068959206e-05\n",
      "Memory Usage: 934.80 MB\n",
      "Epoch 33, Batch 300/4948, Batch loss: 0.0003197241749148816\n",
      "Memory Usage: 934.80 MB\n",
      "Epoch 33, Batch 400/4948, Batch loss: 0.00011667664512060583\n",
      "Memory Usage: 934.80 MB\n",
      "Epoch 33, Batch 500/4948, Batch loss: 5.164246613276191e-05\n",
      "Memory Usage: 934.80 MB\n",
      "Epoch 33, Batch 600/4948, Batch loss: 0.0004765316843986511\n",
      "Memory Usage: 934.84 MB\n",
      "Epoch 33, Batch 700/4948, Batch loss: 8.195934788091108e-05\n",
      "Memory Usage: 935.12 MB\n",
      "Epoch 33, Batch 800/4948, Batch loss: 0.0001248213811777532\n",
      "Memory Usage: 935.12 MB\n",
      "Epoch 33, Batch 900/4948, Batch loss: 0.0005452207988128066\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 33, Batch 1000/4948, Batch loss: 8.496218651998788e-05\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 33, Batch 1100/4948, Batch loss: 0.0008811746374703944\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 33, Batch 1200/4948, Batch loss: 0.0015793805941939354\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 33, Batch 1300/4948, Batch loss: 0.0001542049431009218\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 33, Batch 1400/4948, Batch loss: 0.00012465898180380464\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 33, Batch 1500/4948, Batch loss: 9.142985800281167e-05\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 33, Batch 1600/4948, Batch loss: 1.339638311037561e-05\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 33, Batch 1700/4948, Batch loss: 0.0005254297284409404\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 33, Batch 1800/4948, Batch loss: 0.0002650410169735551\n",
      "Memory Usage: 935.69 MB\n",
      "Epoch 33, Batch 1900/4948, Batch loss: 0.004502135328948498\n",
      "Memory Usage: 934.98 MB\n",
      "Epoch 33, Batch 2000/4948, Batch loss: 0.0005163747700862586\n",
      "Memory Usage: 935.61 MB\n",
      "Epoch 33, Batch 2100/4948, Batch loss: 0.0008287537493743002\n",
      "Memory Usage: 936.11 MB\n",
      "Epoch 33, Batch 2200/4948, Batch loss: 0.00010542524250922725\n",
      "Memory Usage: 936.11 MB\n",
      "Epoch 33, Batch 2300/4948, Batch loss: 0.00021838987595401704\n",
      "Memory Usage: 936.11 MB\n",
      "Epoch 33, Batch 2400/4948, Batch loss: 0.00010409300739411265\n",
      "Memory Usage: 936.11 MB\n",
      "Epoch 33, Batch 2500/4948, Batch loss: 4.852102938457392e-05\n",
      "Memory Usage: 936.27 MB\n",
      "Epoch 33, Batch 2600/4948, Batch loss: 0.00038590794429183006\n",
      "Memory Usage: 936.27 MB\n",
      "Epoch 33, Batch 2700/4948, Batch loss: 0.00037413722020573914\n",
      "Memory Usage: 936.27 MB\n",
      "Epoch 33, Batch 2800/4948, Batch loss: 0.0015205907402560115\n",
      "Memory Usage: 936.27 MB\n",
      "Epoch 33, Batch 2900/4948, Batch loss: 0.00019948264525737613\n",
      "Memory Usage: 936.27 MB\n",
      "Epoch 33, Batch 3000/4948, Batch loss: 5.765643436461687e-05\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 33, Batch 3100/4948, Batch loss: 0.00012454442912712693\n",
      "Memory Usage: 935.48 MB\n",
      "Epoch 33, Batch 3200/4948, Batch loss: 0.0003283261612523347\n",
      "Memory Usage: 935.48 MB\n",
      "Epoch 33, Batch 3300/4948, Batch loss: 0.00014156315592117608\n",
      "Memory Usage: 935.48 MB\n",
      "Epoch 33, Batch 3400/4948, Batch loss: 4.942498890159186e-06\n",
      "Memory Usage: 935.48 MB\n",
      "Epoch 33, Batch 3500/4948, Batch loss: 5.00218229717575e-05\n",
      "Memory Usage: 935.48 MB\n",
      "Epoch 33, Batch 3600/4948, Batch loss: 3.99943528464064e-05\n",
      "Memory Usage: 935.88 MB\n",
      "Epoch 33, Batch 3700/4948, Batch loss: 0.0005348939448595047\n",
      "Memory Usage: 935.88 MB\n",
      "Epoch 33, Batch 3800/4948, Batch loss: 0.00010622247646097094\n",
      "Memory Usage: 935.88 MB\n",
      "Epoch 33, Batch 3900/4948, Batch loss: 0.00015562849876005203\n",
      "Memory Usage: 935.88 MB\n",
      "Epoch 33, Batch 4000/4948, Batch loss: 0.00014958895917516202\n",
      "Memory Usage: 935.88 MB\n",
      "Epoch 33, Batch 4100/4948, Batch loss: 4.4555483327712864e-05\n",
      "Memory Usage: 935.88 MB\n",
      "Epoch 33, Batch 4200/4948, Batch loss: 0.0010300049325451255\n",
      "Memory Usage: 935.88 MB\n",
      "Epoch 33, Batch 4300/4948, Batch loss: 0.00014451515744440258\n",
      "Memory Usage: 935.88 MB\n",
      "Epoch 33, Batch 4400/4948, Batch loss: 4.083186468051281e-06\n",
      "Memory Usage: 935.88 MB\n",
      "Epoch 33, Batch 4500/4948, Batch loss: 0.0009890258079394698\n",
      "Memory Usage: 935.88 MB\n",
      "Epoch 33, Batch 4600/4948, Batch loss: 0.0002443187986500561\n",
      "Memory Usage: 935.88 MB\n",
      "Epoch 33, Batch 4700/4948, Batch loss: 0.00042174526606686413\n",
      "Memory Usage: 935.88 MB\n",
      "Epoch 33, Batch 4800/4948, Batch loss: 0.00010944408131763339\n",
      "Memory Usage: 933.17 MB\n",
      "Epoch 33, Batch 4900/4948, Batch loss: 8.780694770393893e-05\n",
      "Memory Usage: 935.23 MB\n",
      "Epoch 33, Batch 4948/4948, Batch loss: 0.0011075845686718822\n",
      "Memory Usage: 935.39 MB\n",
      "Epoch 33 completed in 69.56 seconds, Total Training Loss: 0.0003204908717159438\n",
      "\n",
      "Epoch 34/100\n",
      "Epoch 34, Batch 100/4948, Batch loss: 0.0001712748344289139\n",
      "Memory Usage: 935.53 MB\n",
      "Epoch 34, Batch 200/4948, Batch loss: 1.5855881429160945e-05\n",
      "Memory Usage: 935.53 MB\n",
      "Epoch 34, Batch 300/4948, Batch loss: 0.0003182710788678378\n",
      "Memory Usage: 935.53 MB\n",
      "Epoch 34, Batch 400/4948, Batch loss: 0.00011766635725507513\n",
      "Memory Usage: 935.53 MB\n",
      "Epoch 34, Batch 500/4948, Batch loss: 5.165817492525093e-05\n",
      "Memory Usage: 935.53 MB\n",
      "Epoch 34, Batch 600/4948, Batch loss: 0.0004682616563513875\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 34, Batch 700/4948, Batch loss: 6.267850403673947e-05\n",
      "Memory Usage: 935.42 MB\n",
      "Epoch 34, Batch 800/4948, Batch loss: 0.0001245557505171746\n",
      "Memory Usage: 935.42 MB\n",
      "Epoch 34, Batch 900/4948, Batch loss: 0.0005306488601490855\n",
      "Memory Usage: 935.53 MB\n",
      "Epoch 34, Batch 1000/4948, Batch loss: 8.533430082025006e-05\n",
      "Memory Usage: 935.53 MB\n",
      "Epoch 34, Batch 1100/4948, Batch loss: 0.000895940640475601\n",
      "Memory Usage: 935.53 MB\n",
      "Epoch 34, Batch 1200/4948, Batch loss: 0.0015773993218317628\n",
      "Memory Usage: 935.53 MB\n",
      "Epoch 34, Batch 1300/4948, Batch loss: 0.00015720211376901716\n",
      "Memory Usage: 935.53 MB\n",
      "Epoch 34, Batch 1400/4948, Batch loss: 0.0001232084323419258\n",
      "Memory Usage: 935.53 MB\n",
      "Epoch 34, Batch 1500/4948, Batch loss: 9.555766155244783e-05\n",
      "Memory Usage: 935.53 MB\n",
      "Epoch 34, Batch 1600/4948, Batch loss: 2.4398614186793566e-05\n",
      "Memory Usage: 935.55 MB\n",
      "Epoch 34, Batch 1700/4948, Batch loss: 0.0005261147161945701\n",
      "Memory Usage: 935.55 MB\n",
      "Epoch 34, Batch 1800/4948, Batch loss: 0.00026341580087319016\n",
      "Memory Usage: 935.55 MB\n",
      "Epoch 34, Batch 1900/4948, Batch loss: 0.00492255249992013\n",
      "Memory Usage: 935.55 MB\n",
      "Epoch 34, Batch 2000/4948, Batch loss: 0.0005038875970058143\n",
      "Memory Usage: 935.55 MB\n",
      "Epoch 34, Batch 2100/4948, Batch loss: 0.0007734714890830219\n",
      "Memory Usage: 935.55 MB\n",
      "Epoch 34, Batch 2200/4948, Batch loss: 0.00010562166426097974\n",
      "Memory Usage: 935.97 MB\n",
      "Epoch 34, Batch 2300/4948, Batch loss: 0.00022037346207071096\n",
      "Memory Usage: 935.97 MB\n",
      "Epoch 34, Batch 2400/4948, Batch loss: 0.00010480123455636203\n",
      "Memory Usage: 935.97 MB\n",
      "Epoch 34, Batch 2500/4948, Batch loss: 4.869997064815834e-05\n",
      "Memory Usage: 935.97 MB\n",
      "Epoch 34, Batch 2600/4948, Batch loss: 0.0003835683746729046\n",
      "Memory Usage: 935.97 MB\n",
      "Epoch 34, Batch 2700/4948, Batch loss: 0.0003890782536473125\n",
      "Memory Usage: 935.97 MB\n",
      "Epoch 34, Batch 2800/4948, Batch loss: 0.0015099941520020366\n",
      "Memory Usage: 935.97 MB\n",
      "Epoch 34, Batch 2900/4948, Batch loss: 0.00020606315229088068\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 3000/4948, Batch loss: 5.826651613460854e-05\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 3100/4948, Batch loss: 0.00012614228762686253\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 3200/4948, Batch loss: 0.00032614037627354264\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 3300/4948, Batch loss: 0.0001415683946106583\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 3400/4948, Batch loss: 6.21129993305658e-06\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 3500/4948, Batch loss: 4.999107841285877e-05\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 3600/4948, Batch loss: 4.021706627099775e-05\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 3700/4948, Batch loss: 0.0005442723631858826\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 3800/4948, Batch loss: 0.00010656418453436345\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 3900/4948, Batch loss: 0.00013785780174657702\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 4000/4948, Batch loss: 0.0001512823801022023\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 4100/4948, Batch loss: 4.4816042645834386e-05\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 4200/4948, Batch loss: 0.0010387044167146087\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 4300/4948, Batch loss: 0.0001445978123228997\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 4400/4948, Batch loss: 5.051556854596129e-06\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 4500/4948, Batch loss: 0.0009948501829057932\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 4600/4948, Batch loss: 0.0002400663506705314\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 4700/4948, Batch loss: 0.00043225116678513587\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 4800/4948, Batch loss: 0.00012552774569485337\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 4900/4948, Batch loss: 7.030891720205545e-05\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34, Batch 4948/4948, Batch loss: 0.0010767236817628145\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 34 completed in 68.11 seconds, Total Training Loss: 0.00032002331992988994\n",
      "\n",
      "Epoch 35/100\n",
      "Epoch 35, Batch 100/4948, Batch loss: 0.00017149226914625615\n",
      "Memory Usage: 936.31 MB\n",
      "Epoch 35, Batch 200/4948, Batch loss: 1.174912085843971e-05\n",
      "Memory Usage: 908.95 MB\n",
      "Epoch 35, Batch 300/4948, Batch loss: 0.00031503214268013835\n",
      "Memory Usage: 915.33 MB\n",
      "Epoch 35, Batch 400/4948, Batch loss: 0.00011601257574511692\n",
      "Memory Usage: 919.69 MB\n",
      "Epoch 35, Batch 500/4948, Batch loss: 5.1062907004961744e-05\n",
      "Memory Usage: 921.53 MB\n",
      "Epoch 35, Batch 600/4948, Batch loss: 0.00046850650687702\n",
      "Memory Usage: 922.27 MB\n",
      "Epoch 35, Batch 700/4948, Batch loss: 6.1008067859802395e-05\n",
      "Memory Usage: 922.67 MB\n",
      "Epoch 35, Batch 800/4948, Batch loss: 0.00014610517246183008\n",
      "Memory Usage: 922.67 MB\n",
      "Epoch 35, Batch 900/4948, Batch loss: 0.0005535211530514061\n",
      "Memory Usage: 922.67 MB\n",
      "Epoch 35, Batch 1000/4948, Batch loss: 8.93965334398672e-05\n",
      "Memory Usage: 922.67 MB\n",
      "Epoch 35, Batch 1100/4948, Batch loss: 0.0008771723369136453\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 1200/4948, Batch loss: 0.0015681572258472443\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 1300/4948, Batch loss: 0.00015760952373966575\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 1400/4948, Batch loss: 0.00012544334458652884\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 1500/4948, Batch loss: 7.818095036782324e-05\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 1600/4948, Batch loss: 1.3581129678641446e-05\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 1700/4948, Batch loss: 0.00043008531793020666\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 1800/4948, Batch loss: 0.000264672125922516\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 1900/4948, Batch loss: 0.0038533478509634733\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 2000/4948, Batch loss: 0.00041102233808487654\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 2100/4948, Batch loss: 0.0006855165120214224\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 2200/4948, Batch loss: 0.00010552821186138317\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 2300/4948, Batch loss: 0.00022447039373219013\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 2400/4948, Batch loss: 0.00010508892592042685\n",
      "Memory Usage: 922.77 MB\n",
      "Epoch 35, Batch 2500/4948, Batch loss: 4.853285281569697e-05\n",
      "Memory Usage: 922.89 MB\n",
      "Epoch 35, Batch 2600/4948, Batch loss: 0.0003856125404126942\n",
      "Memory Usage: 922.89 MB\n",
      "Epoch 35, Batch 2700/4948, Batch loss: 0.0003889843646902591\n",
      "Memory Usage: 922.89 MB\n",
      "Epoch 35, Batch 2800/4948, Batch loss: 0.0014672577381134033\n",
      "Memory Usage: 897.11 MB\n",
      "Epoch 35, Batch 2900/4948, Batch loss: 0.0002007554576266557\n",
      "Memory Usage: 898.69 MB\n",
      "Epoch 35, Batch 3000/4948, Batch loss: 5.7730256230570376e-05\n",
      "Memory Usage: 894.33 MB\n",
      "Epoch 35, Batch 3100/4948, Batch loss: 0.00013037948519922793\n",
      "Memory Usage: 855.59 MB\n",
      "Epoch 35, Batch 3200/4948, Batch loss: 0.00031116348691284657\n",
      "Memory Usage: 869.00 MB\n",
      "Epoch 35, Batch 3300/4948, Batch loss: 0.0001409660471836105\n",
      "Memory Usage: 882.83 MB\n",
      "Epoch 35, Batch 3400/4948, Batch loss: 8.886288924259134e-06\n",
      "Memory Usage: 885.59 MB\n",
      "Epoch 35, Batch 3500/4948, Batch loss: 4.748960418510251e-05\n",
      "Memory Usage: 885.72 MB\n",
      "Epoch 35, Batch 3600/4948, Batch loss: 4.10801476391498e-05\n",
      "Memory Usage: 887.16 MB\n",
      "Epoch 35, Batch 3700/4948, Batch loss: 0.0005445267888717353\n",
      "Memory Usage: 888.19 MB\n",
      "Epoch 35, Batch 3800/4948, Batch loss: 0.00010679402475943789\n",
      "Memory Usage: 889.62 MB\n",
      "Epoch 35, Batch 3900/4948, Batch loss: 0.00013577078061643988\n",
      "Memory Usage: 889.75 MB\n",
      "Epoch 35, Batch 4000/4948, Batch loss: 0.00014956171798985451\n",
      "Memory Usage: 889.75 MB\n",
      "Epoch 35, Batch 4100/4948, Batch loss: 4.675805030274205e-05\n",
      "Memory Usage: 891.19 MB\n",
      "Epoch 35, Batch 4200/4948, Batch loss: 0.001026483136229217\n",
      "Memory Usage: 891.70 MB\n",
      "Epoch 35, Batch 4300/4948, Batch loss: 0.0001446579408366233\n",
      "Memory Usage: 891.72 MB\n",
      "Epoch 35, Batch 4400/4948, Batch loss: 4.506547611526912e-06\n",
      "Memory Usage: 891.73 MB\n",
      "Epoch 35, Batch 4500/4948, Batch loss: 0.0009885141626000404\n",
      "Memory Usage: 892.58 MB\n",
      "Epoch 35, Batch 4600/4948, Batch loss: 0.00024101094459183514\n",
      "Memory Usage: 892.58 MB\n",
      "Epoch 35, Batch 4700/4948, Batch loss: 0.0004226405580993742\n",
      "Memory Usage: 892.58 MB\n",
      "Epoch 35, Batch 4800/4948, Batch loss: 0.0001102198802982457\n",
      "Memory Usage: 892.59 MB\n",
      "Epoch 35, Batch 4900/4948, Batch loss: 7.444075890816748e-05\n",
      "Memory Usage: 892.67 MB\n",
      "Epoch 35, Batch 4948/4948, Batch loss: 0.0010879056062549353\n",
      "Memory Usage: 892.70 MB\n",
      "Epoch 35 completed in 66.50 seconds, Total Training Loss: 0.0003109316090218396\n",
      "\n",
      "Epoch 36/100\n",
      "Epoch 36, Batch 100/4948, Batch loss: 0.0001717461709631607\n",
      "Memory Usage: 893.44 MB\n",
      "Epoch 36, Batch 200/4948, Batch loss: 1.7226268028025515e-05\n",
      "Memory Usage: 893.44 MB\n",
      "Epoch 36, Batch 300/4948, Batch loss: 0.00031264047720469534\n",
      "Memory Usage: 894.44 MB\n",
      "Epoch 36, Batch 400/4948, Batch loss: 0.00011662791803246364\n",
      "Memory Usage: 894.42 MB\n",
      "Epoch 36, Batch 500/4948, Batch loss: 5.130059435032308e-05\n",
      "Memory Usage: 894.28 MB\n",
      "Epoch 36, Batch 600/4948, Batch loss: 0.0004910208517685533\n",
      "Memory Usage: 893.86 MB\n",
      "Epoch 36, Batch 700/4948, Batch loss: 5.4949341574683785e-05\n",
      "Memory Usage: 874.47 MB\n",
      "Epoch 36, Batch 800/4948, Batch loss: 0.0001352000836050138\n",
      "Memory Usage: 884.70 MB\n",
      "Epoch 36, Batch 900/4948, Batch loss: 0.0005409409641288221\n",
      "Memory Usage: 883.92 MB\n",
      "Epoch 36, Batch 1000/4948, Batch loss: 8.649493975099176e-05\n",
      "Memory Usage: 884.42 MB\n",
      "Epoch 36, Batch 1100/4948, Batch loss: 0.00087910977890715\n",
      "Memory Usage: 884.42 MB\n",
      "Epoch 36, Batch 1200/4948, Batch loss: 0.0015609620604664087\n",
      "Memory Usage: 885.45 MB\n",
      "Epoch 36, Batch 1300/4948, Batch loss: 0.0001551456662127748\n",
      "Memory Usage: 885.47 MB\n",
      "Epoch 36, Batch 1400/4948, Batch loss: 0.00012346889707259834\n",
      "Memory Usage: 885.47 MB\n",
      "Epoch 36, Batch 1500/4948, Batch loss: 7.63935677241534e-05\n",
      "Memory Usage: 892.34 MB\n",
      "Epoch 36, Batch 1600/4948, Batch loss: 1.3696008863917086e-05\n",
      "Memory Usage: 892.81 MB\n",
      "Epoch 36, Batch 1700/4948, Batch loss: 0.00033287916448898613\n",
      "Memory Usage: 895.31 MB\n",
      "Epoch 36, Batch 1800/4948, Batch loss: 0.0002635125711094588\n",
      "Memory Usage: 895.33 MB\n",
      "Epoch 36, Batch 1900/4948, Batch loss: 0.002005947520956397\n",
      "Memory Usage: 896.52 MB\n",
      "Epoch 36, Batch 2000/4948, Batch loss: 0.00045129761565476656\n",
      "Memory Usage: 896.53 MB\n",
      "Epoch 36, Batch 2100/4948, Batch loss: 0.0007107330020517111\n",
      "Memory Usage: 896.55 MB\n",
      "Epoch 36, Batch 2200/4948, Batch loss: 0.00010570845915935934\n",
      "Memory Usage: 896.58 MB\n",
      "Epoch 36, Batch 2300/4948, Batch loss: 0.00022276282834354788\n",
      "Memory Usage: 897.86 MB\n",
      "Epoch 36, Batch 2400/4948, Batch loss: 0.00010521326476009563\n",
      "Memory Usage: 870.05 MB\n",
      "Epoch 36, Batch 2500/4948, Batch loss: 4.849277320317924e-05\n",
      "Memory Usage: 872.80 MB\n",
      "Epoch 36, Batch 2600/4948, Batch loss: 0.00038618926191702485\n",
      "Memory Usage: 872.81 MB\n",
      "Epoch 36, Batch 2700/4948, Batch loss: 0.0003755990765057504\n",
      "Memory Usage: 872.81 MB\n",
      "Epoch 36, Batch 2800/4948, Batch loss: 0.0014917247463017702\n",
      "Memory Usage: 872.83 MB\n",
      "Epoch 36, Batch 2900/4948, Batch loss: 0.0002063143183477223\n",
      "Memory Usage: 873.14 MB\n",
      "Epoch 36, Batch 3000/4948, Batch loss: 5.8232631999999285e-05\n",
      "Memory Usage: 873.17 MB\n",
      "Epoch 36, Batch 3100/4948, Batch loss: 0.0001246134052053094\n",
      "Memory Usage: 873.19 MB\n",
      "Epoch 36, Batch 3200/4948, Batch loss: 0.00031582891824655235\n",
      "Memory Usage: 873.20 MB\n",
      "Epoch 36, Batch 3300/4948, Batch loss: 0.00014307520177681\n",
      "Memory Usage: 873.20 MB\n",
      "Epoch 36, Batch 3400/4948, Batch loss: 5.593517926172353e-06\n",
      "Memory Usage: 824.92 MB\n",
      "Epoch 36, Batch 3500/4948, Batch loss: 4.686706233769655e-05\n",
      "Memory Usage: 829.80 MB\n",
      "Epoch 36, Batch 3600/4948, Batch loss: 3.987334275734611e-05\n",
      "Memory Usage: 829.80 MB\n",
      "Epoch 36, Batch 3700/4948, Batch loss: 0.0005384418182075024\n",
      "Memory Usage: 830.92 MB\n",
      "Epoch 36, Batch 3800/4948, Batch loss: 0.00010633113561198115\n",
      "Memory Usage: 831.77 MB\n",
      "Epoch 36, Batch 3900/4948, Batch loss: 0.0001364095223834738\n",
      "Memory Usage: 798.73 MB\n",
      "Epoch 36, Batch 4000/4948, Batch loss: 0.0001485740503994748\n",
      "Memory Usage: 800.20 MB\n",
      "Epoch 36, Batch 4100/4948, Batch loss: 4.894178346148692e-05\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 36, Batch 4200/4948, Batch loss: 0.001025358447805047\n",
      "Memory Usage: 801.00 MB\n",
      "Epoch 36, Batch 4300/4948, Batch loss: 0.0001440418272977695\n",
      "Memory Usage: 801.00 MB\n",
      "Epoch 36, Batch 4400/4948, Batch loss: 3.7421746128529776e-06\n",
      "Memory Usage: 801.00 MB\n",
      "Epoch 36, Batch 4500/4948, Batch loss: 0.0009939160663634539\n",
      "Memory Usage: 801.00 MB\n",
      "Epoch 36, Batch 4600/4948, Batch loss: 0.00024121839669533074\n",
      "Memory Usage: 801.00 MB\n",
      "Epoch 36, Batch 4700/4948, Batch loss: 0.00041817768942564726\n",
      "Memory Usage: 801.00 MB\n",
      "Epoch 36, Batch 4800/4948, Batch loss: 0.00011077080125687644\n",
      "Memory Usage: 800.22 MB\n",
      "Epoch 36, Batch 4900/4948, Batch loss: 7.444732182193547e-05\n",
      "Memory Usage: 801.19 MB\n",
      "Epoch 36, Batch 4948/4948, Batch loss: 0.0010935855098068714\n",
      "Memory Usage: 801.55 MB\n",
      "Epoch 36 completed in 64.81 seconds, Total Training Loss: 0.0003020529113574538\n",
      "\n",
      "Epoch 37/100\n",
      "Epoch 37, Batch 100/4948, Batch loss: 0.0001692615624051541\n",
      "Memory Usage: 801.58 MB\n",
      "Epoch 37, Batch 200/4948, Batch loss: 1.5790505131008103e-05\n",
      "Memory Usage: 801.58 MB\n",
      "Epoch 37, Batch 300/4948, Batch loss: 0.0003131045086774975\n",
      "Memory Usage: 801.58 MB\n",
      "Epoch 37, Batch 400/4948, Batch loss: 0.00011609653302002698\n",
      "Memory Usage: 801.62 MB\n",
      "Epoch 37, Batch 500/4948, Batch loss: 5.092791252536699e-05\n",
      "Memory Usage: 801.62 MB\n",
      "Epoch 37, Batch 600/4948, Batch loss: 0.0004710140055976808\n",
      "Memory Usage: 801.48 MB\n",
      "Epoch 37, Batch 700/4948, Batch loss: 5.2053826948395e-05\n",
      "Memory Usage: 801.59 MB\n",
      "Epoch 37, Batch 800/4948, Batch loss: 0.0001239253324456513\n",
      "Memory Usage: 801.59 MB\n",
      "Epoch 37, Batch 900/4948, Batch loss: 0.0005333265289664268\n",
      "Memory Usage: 801.59 MB\n",
      "Epoch 37, Batch 1000/4948, Batch loss: 8.704217907506973e-05\n",
      "Memory Usage: 801.59 MB\n",
      "Epoch 37, Batch 1100/4948, Batch loss: 0.0008882835973054171\n",
      "Memory Usage: 802.84 MB\n",
      "Epoch 37, Batch 1200/4948, Batch loss: 0.0015345781575888395\n",
      "Memory Usage: 802.84 MB\n",
      "Epoch 37, Batch 1300/4948, Batch loss: 0.00015616713790223002\n",
      "Memory Usage: 802.86 MB\n",
      "Epoch 37, Batch 1400/4948, Batch loss: 0.00012367423914838582\n",
      "Memory Usage: 802.86 MB\n",
      "Epoch 37, Batch 1500/4948, Batch loss: 9.623841469874606e-05\n",
      "Memory Usage: 802.86 MB\n",
      "Epoch 37, Batch 1600/4948, Batch loss: 1.406541468895739e-05\n",
      "Memory Usage: 802.86 MB\n",
      "Epoch 37, Batch 1700/4948, Batch loss: 0.0004746053891722113\n",
      "Memory Usage: 802.86 MB\n",
      "Epoch 37, Batch 1800/4948, Batch loss: 0.00026267272187396884\n",
      "Memory Usage: 802.86 MB\n",
      "Epoch 37, Batch 1900/4948, Batch loss: 0.006225657649338245\n",
      "Memory Usage: 802.88 MB\n",
      "Epoch 37, Batch 2000/4948, Batch loss: 0.0004896098398603499\n",
      "Memory Usage: 802.88 MB\n",
      "Epoch 37, Batch 2100/4948, Batch loss: 0.000739591137971729\n",
      "Memory Usage: 802.88 MB\n",
      "Epoch 37, Batch 2200/4948, Batch loss: 0.00010536611807765439\n",
      "Memory Usage: 802.88 MB\n",
      "Epoch 37, Batch 2300/4948, Batch loss: 0.00021978127188049257\n",
      "Memory Usage: 802.88 MB\n",
      "Epoch 37, Batch 2400/4948, Batch loss: 0.00010437741002533585\n",
      "Memory Usage: 802.88 MB\n",
      "Epoch 37, Batch 2500/4948, Batch loss: 4.816537693841383e-05\n",
      "Memory Usage: 802.88 MB\n",
      "Epoch 37, Batch 2600/4948, Batch loss: 0.00038376179873012006\n",
      "Memory Usage: 802.89 MB\n",
      "Epoch 37, Batch 2700/4948, Batch loss: 0.00038455831236205995\n",
      "Memory Usage: 802.89 MB\n",
      "Epoch 37, Batch 2800/4948, Batch loss: 0.0014865888515487313\n",
      "Memory Usage: 802.89 MB\n",
      "Epoch 37, Batch 2900/4948, Batch loss: 0.0001997964282054454\n",
      "Memory Usage: 802.89 MB\n",
      "Epoch 37, Batch 3000/4948, Batch loss: 5.796492041554302e-05\n",
      "Memory Usage: 802.89 MB\n",
      "Epoch 37, Batch 3100/4948, Batch loss: 0.00012464616156648844\n",
      "Memory Usage: 803.73 MB\n",
      "Epoch 37, Batch 3200/4948, Batch loss: 0.0003280982782598585\n",
      "Memory Usage: 803.73 MB\n",
      "Epoch 37, Batch 3300/4948, Batch loss: 0.00014052620099391788\n",
      "Memory Usage: 803.73 MB\n",
      "Epoch 37, Batch 3400/4948, Batch loss: 6.966076398384757e-06\n",
      "Memory Usage: 803.73 MB\n",
      "Epoch 37, Batch 3500/4948, Batch loss: 4.9104972276836634e-05\n",
      "Memory Usage: 803.73 MB\n",
      "Epoch 37, Batch 3600/4948, Batch loss: 4.085194450453855e-05\n",
      "Memory Usage: 803.73 MB\n",
      "Epoch 37, Batch 3700/4948, Batch loss: 0.0005348798003979027\n",
      "Memory Usage: 803.73 MB\n",
      "Epoch 37, Batch 3800/4948, Batch loss: 0.00010713878145907074\n",
      "Memory Usage: 803.73 MB\n",
      "Epoch 37, Batch 3900/4948, Batch loss: 0.0001453798613511026\n",
      "Memory Usage: 803.73 MB\n",
      "Epoch 37, Batch 4000/4948, Batch loss: 0.00014809388085268438\n",
      "Memory Usage: 803.73 MB\n",
      "Epoch 37, Batch 4100/4948, Batch loss: 5.2781157137360424e-05\n",
      "Memory Usage: 803.73 MB\n",
      "Epoch 37, Batch 4200/4948, Batch loss: 0.0010236804373562336\n",
      "Memory Usage: 803.73 MB\n",
      "Epoch 37, Batch 4300/4948, Batch loss: 0.0001442006614524871\n",
      "Memory Usage: 803.75 MB\n",
      "Epoch 37, Batch 4400/4948, Batch loss: 3.4815047911251895e-06\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 37, Batch 4500/4948, Batch loss: 0.000981542281806469\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 37, Batch 4600/4948, Batch loss: 0.0002418378135189414\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 37, Batch 4700/4948, Batch loss: 0.0004265317984391004\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 37, Batch 4800/4948, Batch loss: 0.00011138177069369704\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 37, Batch 4900/4948, Batch loss: 7.674339576624334e-05\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 37, Batch 4948/4948, Batch loss: 0.001119007240049541\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 37 completed in 65.04 seconds, Total Training Loss: 0.00031765329556145876\n",
      "\n",
      "Epoch 38/100\n",
      "Epoch 38, Batch 100/4948, Batch loss: 0.00017084970022551715\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 38, Batch 200/4948, Batch loss: 1.6572201275266707e-05\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 38, Batch 300/4948, Batch loss: 0.0003122267662547529\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 38, Batch 400/4948, Batch loss: 0.00011732895654859021\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 38, Batch 500/4948, Batch loss: 5.165589027456008e-05\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 38, Batch 600/4948, Batch loss: 0.0004685249296016991\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 38, Batch 700/4948, Batch loss: 5.4092073696665466e-05\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 38, Batch 800/4948, Batch loss: 0.00014919808018021286\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 38, Batch 900/4948, Batch loss: 0.000543943140655756\n",
      "Memory Usage: 804.25 MB\n",
      "Epoch 38, Batch 1000/4948, Batch loss: 8.625256305094808e-05\n",
      "Memory Usage: 804.27 MB\n",
      "Epoch 38, Batch 1100/4948, Batch loss: 0.0008696194272488356\n",
      "Memory Usage: 804.27 MB\n",
      "Epoch 38, Batch 1200/4948, Batch loss: 0.0015330655733123422\n",
      "Memory Usage: 804.27 MB\n",
      "Epoch 38, Batch 1300/4948, Batch loss: 0.00015474062820430845\n",
      "Memory Usage: 804.28 MB\n",
      "Epoch 38, Batch 1400/4948, Batch loss: 0.00012316227366682142\n",
      "Memory Usage: 804.28 MB\n",
      "Epoch 38, Batch 1500/4948, Batch loss: 8.16202737041749e-05\n",
      "Memory Usage: 804.31 MB\n",
      "Epoch 38, Batch 1600/4948, Batch loss: 1.345502369076712e-05\n",
      "Memory Usage: 804.31 MB\n",
      "Epoch 38, Batch 1700/4948, Batch loss: 0.0004577099171001464\n",
      "Memory Usage: 804.31 MB\n",
      "Epoch 38, Batch 1800/4948, Batch loss: 0.000263016700046137\n",
      "Memory Usage: 804.31 MB\n",
      "Epoch 38, Batch 1900/4948, Batch loss: 0.004789888858795166\n",
      "Memory Usage: 804.31 MB\n",
      "Epoch 38, Batch 2000/4948, Batch loss: 0.0004904374945908785\n",
      "Memory Usage: 804.31 MB\n",
      "Epoch 38, Batch 2100/4948, Batch loss: 0.0007620340911671519\n",
      "Memory Usage: 804.33 MB\n",
      "Epoch 38, Batch 2200/4948, Batch loss: 0.00010606673458823934\n",
      "Memory Usage: 804.34 MB\n",
      "Epoch 38, Batch 2300/4948, Batch loss: 0.0002201453607995063\n",
      "Memory Usage: 804.34 MB\n",
      "Epoch 38, Batch 2400/4948, Batch loss: 0.00010450155241414905\n",
      "Memory Usage: 804.34 MB\n",
      "Epoch 38, Batch 2500/4948, Batch loss: 4.888383409706876e-05\n",
      "Memory Usage: 804.34 MB\n",
      "Epoch 38, Batch 2600/4948, Batch loss: 0.00038571044569835067\n",
      "Memory Usage: 804.36 MB\n",
      "Epoch 38, Batch 2700/4948, Batch loss: 0.0003757257363758981\n",
      "Memory Usage: 804.36 MB\n",
      "Epoch 38, Batch 2800/4948, Batch loss: 0.0015032946830615401\n",
      "Memory Usage: 804.36 MB\n",
      "Epoch 38, Batch 2900/4948, Batch loss: 0.0002008799056056887\n",
      "Memory Usage: 804.36 MB\n",
      "Epoch 38, Batch 3000/4948, Batch loss: 5.7921621191781014e-05\n",
      "Memory Usage: 804.36 MB\n",
      "Epoch 38, Batch 3100/4948, Batch loss: 0.0001233231887454167\n",
      "Memory Usage: 804.36 MB\n",
      "Epoch 38, Batch 3200/4948, Batch loss: 0.0003263675607740879\n",
      "Memory Usage: 804.36 MB\n",
      "Epoch 38, Batch 3300/4948, Batch loss: 0.00013937366020400077\n",
      "Memory Usage: 804.36 MB\n",
      "Epoch 38, Batch 3400/4948, Batch loss: 6.151520210551098e-06\n",
      "Memory Usage: 804.36 MB\n",
      "Epoch 38, Batch 3500/4948, Batch loss: 4.603765773936175e-05\n",
      "Memory Usage: 804.36 MB\n",
      "Epoch 38, Batch 3600/4948, Batch loss: 4.007264942629263e-05\n",
      "Memory Usage: 804.36 MB\n",
      "Epoch 38, Batch 3700/4948, Batch loss: 0.0005451996112242341\n",
      "Memory Usage: 804.56 MB\n",
      "Epoch 38, Batch 3800/4948, Batch loss: 0.00010711504728533328\n",
      "Memory Usage: 804.56 MB\n",
      "Epoch 38, Batch 3900/4948, Batch loss: 0.00013534093159250915\n",
      "Memory Usage: 804.56 MB\n",
      "Epoch 38, Batch 4000/4948, Batch loss: 0.00014858895156066865\n",
      "Memory Usage: 805.06 MB\n",
      "Epoch 38, Batch 4100/4948, Batch loss: 5.074297951068729e-05\n",
      "Memory Usage: 805.06 MB\n",
      "Epoch 38, Batch 4200/4948, Batch loss: 0.0010314441751688719\n",
      "Memory Usage: 805.45 MB\n",
      "Epoch 38, Batch 4300/4948, Batch loss: 0.00014456757344305515\n",
      "Memory Usage: 806.05 MB\n",
      "Epoch 38, Batch 4400/4948, Batch loss: 3.6609496874007164e-06\n",
      "Memory Usage: 806.05 MB\n",
      "Epoch 38, Batch 4500/4948, Batch loss: 0.0009891183581203222\n",
      "Memory Usage: 806.05 MB\n",
      "Epoch 38, Batch 4600/4948, Batch loss: 0.00024241601931862533\n",
      "Memory Usage: 806.05 MB\n",
      "Epoch 38, Batch 4700/4948, Batch loss: 0.0004205302393529564\n",
      "Memory Usage: 806.05 MB\n",
      "Epoch 38, Batch 4800/4948, Batch loss: 0.0001100259178201668\n",
      "Memory Usage: 806.05 MB\n",
      "Epoch 38, Batch 4900/4948, Batch loss: 7.233094947878271e-05\n",
      "Memory Usage: 806.05 MB\n",
      "Epoch 38, Batch 4948/4948, Batch loss: 0.0010979748331010342\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 38 completed in 64.60 seconds, Total Training Loss: 0.00031653364687779186\n",
      "\n",
      "Epoch 39/100\n",
      "Epoch 39, Batch 100/4948, Batch loss: 0.00017062219558283687\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 200/4948, Batch loss: 1.4954203834349755e-05\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 300/4948, Batch loss: 0.00031529105035588145\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 400/4948, Batch loss: 0.00011632197856670246\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 500/4948, Batch loss: 5.113980660098605e-05\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 600/4948, Batch loss: 0.0004693660303018987\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 700/4948, Batch loss: 6.105784268584102e-05\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 800/4948, Batch loss: 0.00012613486615009606\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 900/4948, Batch loss: 0.0005239545716904104\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 1000/4948, Batch loss: 8.57232662383467e-05\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 1100/4948, Batch loss: 0.0008922059205360711\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 1200/4948, Batch loss: 0.0015330191235989332\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 1300/4948, Batch loss: 0.00016363039321731776\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 1400/4948, Batch loss: 0.00012297109060455114\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 1500/4948, Batch loss: 8.812123269308358e-05\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 1600/4948, Batch loss: 1.4740466212970205e-05\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 1700/4948, Batch loss: 0.0004292436351533979\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 1800/4948, Batch loss: 0.0002617870341055095\n",
      "Memory Usage: 806.06 MB\n",
      "Epoch 39, Batch 1900/4948, Batch loss: 0.0017886278219521046\n",
      "Memory Usage: 806.08 MB\n",
      "Epoch 39, Batch 2000/4948, Batch loss: 0.0003938478766940534\n",
      "Memory Usage: 806.08 MB\n",
      "Epoch 39, Batch 2100/4948, Batch loss: 0.0006746415165252984\n",
      "Memory Usage: 806.08 MB\n",
      "Epoch 39, Batch 2200/4948, Batch loss: 0.00010541044321144\n",
      "Memory Usage: 806.08 MB\n",
      "Epoch 39, Batch 2300/4948, Batch loss: 0.00023012844030745327\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 2400/4948, Batch loss: 0.00011016869393642992\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 2500/4948, Batch loss: 4.9962563934968784e-05\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 2600/4948, Batch loss: 0.00038627631147392094\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 2700/4948, Batch loss: 0.0003908715443685651\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 2800/4948, Batch loss: 0.0014785807579755783\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 2900/4948, Batch loss: 0.00020956146181561053\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 3000/4948, Batch loss: 5.901836993871257e-05\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 3100/4948, Batch loss: 0.0001238985569216311\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 3200/4948, Batch loss: 0.00031526421662420034\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 3300/4948, Batch loss: 0.0001426124363206327\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 3400/4948, Batch loss: 5.500427050719736e-06\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 3500/4948, Batch loss: 6.811032653786242e-05\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 3600/4948, Batch loss: 4.0717754018260166e-05\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 3700/4948, Batch loss: 0.0005627230857498944\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 3800/4948, Batch loss: 0.00011609376815613359\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 3900/4948, Batch loss: 0.00015078794967848808\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 4000/4948, Batch loss: 0.00014827724953647703\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 4100/4948, Batch loss: 4.536147753242403e-05\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 4200/4948, Batch loss: 0.0010106745176017284\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 4300/4948, Batch loss: 0.00014446111163124442\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 4400/4948, Batch loss: 3.99525970351533e-06\n",
      "Memory Usage: 806.09 MB\n",
      "Epoch 39, Batch 4500/4948, Batch loss: 0.0009732106700539589\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 39, Batch 4600/4948, Batch loss: 0.00024508265778422356\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 39, Batch 4700/4948, Batch loss: 0.0004197841335553676\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 39, Batch 4800/4948, Batch loss: 0.00011028476001229137\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 39, Batch 4900/4948, Batch loss: 7.434731378452852e-05\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 39, Batch 4948/4948, Batch loss: 0.0010660061379894614\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 39 completed in 63.99 seconds, Total Training Loss: 0.00029884090248992565\n",
      "\n",
      "Epoch 40/100\n",
      "Epoch 40, Batch 100/4948, Batch loss: 0.00017040417878888547\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 40, Batch 200/4948, Batch loss: 1.6935191524680704e-05\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 40, Batch 300/4948, Batch loss: 0.0003161169297527522\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 40, Batch 400/4948, Batch loss: 0.00012080493615940213\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 40, Batch 500/4948, Batch loss: 5.1882296247640625e-05\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 40, Batch 600/4948, Batch loss: 0.00047239623381756246\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 40, Batch 700/4948, Batch loss: 5.4399923101300374e-05\n",
      "Memory Usage: 806.59 MB\n",
      "Epoch 40, Batch 800/4948, Batch loss: 0.00013509530981536955\n",
      "Memory Usage: 806.61 MB\n",
      "Epoch 40, Batch 900/4948, Batch loss: 0.000533160986378789\n",
      "Memory Usage: 806.61 MB\n",
      "Epoch 40, Batch 1000/4948, Batch loss: 8.815481123747304e-05\n",
      "Memory Usage: 806.61 MB\n",
      "Epoch 40, Batch 1100/4948, Batch loss: 0.0008778710034675896\n",
      "Memory Usage: 806.61 MB\n",
      "Epoch 40, Batch 1200/4948, Batch loss: 0.0015140693867579103\n",
      "Memory Usage: 806.61 MB\n",
      "Epoch 40, Batch 1300/4948, Batch loss: 0.00015153377898968756\n",
      "Memory Usage: 806.62 MB\n",
      "Epoch 40, Batch 1400/4948, Batch loss: 0.0001227575703524053\n",
      "Memory Usage: 806.62 MB\n",
      "Epoch 40, Batch 1500/4948, Batch loss: 8.769414853304625e-05\n",
      "Memory Usage: 806.62 MB\n",
      "Epoch 40, Batch 1600/4948, Batch loss: 1.3986156773171388e-05\n",
      "Memory Usage: 806.62 MB\n",
      "Epoch 40, Batch 1700/4948, Batch loss: 0.0005324180237948895\n",
      "Memory Usage: 806.62 MB\n",
      "Epoch 40, Batch 1800/4948, Batch loss: 0.00026238159625791013\n",
      "Memory Usage: 806.62 MB\n",
      "Epoch 40, Batch 1900/4948, Batch loss: 0.005204768385738134\n",
      "Memory Usage: 806.62 MB\n",
      "Epoch 40, Batch 2000/4948, Batch loss: 0.0005066314479336143\n",
      "Memory Usage: 806.62 MB\n",
      "Epoch 40, Batch 2100/4948, Batch loss: 0.0007887536776252091\n",
      "Memory Usage: 756.34 MB\n",
      "Epoch 40, Batch 2200/4948, Batch loss: 0.00010556445340625942\n",
      "Memory Usage: 655.23 MB\n",
      "Epoch 40, Batch 2300/4948, Batch loss: 0.00021927185298409313\n",
      "Memory Usage: 659.78 MB\n",
      "Epoch 40, Batch 2400/4948, Batch loss: 0.00010433857096359134\n",
      "Memory Usage: 663.34 MB\n",
      "Epoch 40, Batch 2500/4948, Batch loss: 4.785247801919468e-05\n",
      "Memory Usage: 598.17 MB\n",
      "Epoch 40, Batch 2600/4948, Batch loss: 0.00038655815296806395\n",
      "Memory Usage: 510.42 MB\n",
      "Epoch 40, Batch 2700/4948, Batch loss: 0.00039153688703663647\n",
      "Memory Usage: 523.91 MB\n",
      "Epoch 40, Batch 2800/4948, Batch loss: 0.0014655893901363015\n",
      "Memory Usage: 541.27 MB\n",
      "Epoch 40, Batch 2900/4948, Batch loss: 0.00019870710093528032\n",
      "Memory Usage: 553.44 MB\n",
      "Epoch 40, Batch 3000/4948, Batch loss: 5.808178320876323e-05\n",
      "Memory Usage: 565.02 MB\n",
      "Epoch 40, Batch 3100/4948, Batch loss: 0.00012321039685048163\n",
      "Memory Usage: 565.55 MB\n",
      "Epoch 40, Batch 3200/4948, Batch loss: 0.0003170466225128621\n",
      "Memory Usage: 577.12 MB\n",
      "Epoch 40, Batch 3300/4948, Batch loss: 0.00013990093430038542\n",
      "Memory Usage: 588.80 MB\n",
      "Epoch 40, Batch 3400/4948, Batch loss: 5.899957159272162e-06\n",
      "Memory Usage: 602.77 MB\n",
      "Epoch 40, Batch 3500/4948, Batch loss: 4.7928046114975587e-05\n",
      "Memory Usage: 605.86 MB\n",
      "Epoch 40, Batch 3600/4948, Batch loss: 4.094463656656444e-05\n",
      "Memory Usage: 607.89 MB\n",
      "Epoch 40, Batch 3700/4948, Batch loss: 0.0005518979742191732\n",
      "Memory Usage: 607.95 MB\n",
      "Epoch 40, Batch 3800/4948, Batch loss: 0.00010680400009732693\n",
      "Memory Usage: 608.45 MB\n",
      "Epoch 40, Batch 3900/4948, Batch loss: 0.00013506280083674937\n",
      "Memory Usage: 608.45 MB\n",
      "Epoch 40, Batch 4000/4948, Batch loss: 0.00014898554945830256\n",
      "Memory Usage: 608.45 MB\n",
      "Epoch 40, Batch 4100/4948, Batch loss: 4.5492568460758775e-05\n",
      "Memory Usage: 608.45 MB\n",
      "Epoch 40, Batch 4200/4948, Batch loss: 0.0010095678735524416\n",
      "Memory Usage: 608.45 MB\n",
      "Epoch 40, Batch 4300/4948, Batch loss: 0.00014473825285676867\n",
      "Memory Usage: 608.47 MB\n",
      "Epoch 40, Batch 4400/4948, Batch loss: 3.69166173186386e-06\n",
      "Memory Usage: 608.47 MB\n",
      "Epoch 40, Batch 4500/4948, Batch loss: 0.000976315641310066\n",
      "Memory Usage: 608.47 MB\n",
      "Epoch 40, Batch 4600/4948, Batch loss: 0.00024088253849186003\n",
      "Memory Usage: 586.45 MB\n",
      "Epoch 40, Batch 4700/4948, Batch loss: 0.0004218150570522994\n",
      "Memory Usage: 594.00 MB\n",
      "Epoch 40, Batch 4800/4948, Batch loss: 0.00011157269909745082\n",
      "Memory Usage: 596.83 MB\n",
      "Epoch 40, Batch 4900/4948, Batch loss: 6.93659603712149e-05\n",
      "Memory Usage: 601.61 MB\n",
      "Epoch 40, Batch 4948/4948, Batch loss: 0.0010780065786093473\n",
      "Memory Usage: 601.62 MB\n",
      "Epoch 40 completed in 67.14 seconds, Total Training Loss: 0.00031546406052712823\n",
      "\n",
      "Epoch 41/100\n",
      "Epoch 41, Batch 100/4948, Batch loss: 0.00017028221918735653\n",
      "Memory Usage: 608.84 MB\n",
      "Epoch 41, Batch 200/4948, Batch loss: 1.7048687368514948e-05\n",
      "Memory Usage: 610.56 MB\n",
      "Epoch 41, Batch 300/4948, Batch loss: 0.00031029534875415266\n",
      "Memory Usage: 612.06 MB\n",
      "Epoch 41, Batch 400/4948, Batch loss: 0.00011507437739055604\n",
      "Memory Usage: 612.06 MB\n",
      "Epoch 41, Batch 500/4948, Batch loss: 5.144296301295981e-05\n",
      "Memory Usage: 612.06 MB\n",
      "Epoch 41, Batch 600/4948, Batch loss: 0.00046849201316945255\n",
      "Memory Usage: 612.06 MB\n",
      "Epoch 41, Batch 700/4948, Batch loss: 5.867711661267094e-05\n",
      "Memory Usage: 617.16 MB\n",
      "Epoch 41, Batch 800/4948, Batch loss: 0.00012667042028624564\n",
      "Memory Usage: 629.14 MB\n",
      "Epoch 41, Batch 900/4948, Batch loss: 0.0005326449172571301\n",
      "Memory Usage: 640.91 MB\n",
      "Epoch 41, Batch 1000/4948, Batch loss: 8.598471322329715e-05\n",
      "Memory Usage: 652.69 MB\n",
      "Epoch 41, Batch 1100/4948, Batch loss: 0.0008691527182236314\n",
      "Memory Usage: 664.47 MB\n",
      "Epoch 41, Batch 1200/4948, Batch loss: 0.0015088015934452415\n",
      "Memory Usage: 676.27 MB\n",
      "Epoch 41, Batch 1300/4948, Batch loss: 0.00015516194980591536\n",
      "Memory Usage: 688.05 MB\n",
      "Epoch 41, Batch 1400/4948, Batch loss: 0.0001225279993377626\n",
      "Memory Usage: 699.73 MB\n",
      "Epoch 41, Batch 1500/4948, Batch loss: 9.007780317915604e-05\n",
      "Memory Usage: 712.33 MB\n",
      "Epoch 41, Batch 1600/4948, Batch loss: 1.3567179848905653e-05\n",
      "Memory Usage: 724.11 MB\n",
      "Epoch 41, Batch 1700/4948, Batch loss: 0.0005020504468120635\n",
      "Memory Usage: 736.75 MB\n",
      "Epoch 41, Batch 1800/4948, Batch loss: 0.0002632891992107034\n",
      "Memory Usage: 748.50 MB\n",
      "Epoch 41, Batch 1900/4948, Batch loss: 0.004970631096512079\n",
      "Memory Usage: 760.25 MB\n",
      "Epoch 41, Batch 2000/4948, Batch loss: 0.0004303398891352117\n",
      "Memory Usage: 772.05 MB\n",
      "Epoch 41, Batch 2100/4948, Batch loss: 0.0007141607929952443\n",
      "Memory Usage: 783.81 MB\n",
      "Epoch 41, Batch 2200/4948, Batch loss: 0.00010596327047096565\n",
      "Memory Usage: 790.86 MB\n",
      "Epoch 41, Batch 2300/4948, Batch loss: 0.00021324104454834014\n",
      "Memory Usage: 790.86 MB\n",
      "Epoch 41, Batch 2400/4948, Batch loss: 0.00010542503878241405\n",
      "Memory Usage: 790.86 MB\n",
      "Epoch 41, Batch 2500/4948, Batch loss: 4.9766847951104864e-05\n",
      "Memory Usage: 790.86 MB\n",
      "Epoch 41, Batch 2600/4948, Batch loss: 0.0003851758374366909\n",
      "Memory Usage: 790.86 MB\n",
      "Epoch 41, Batch 2700/4948, Batch loss: 0.00037696852814406157\n",
      "Memory Usage: 790.86 MB\n",
      "Epoch 41, Batch 2800/4948, Batch loss: 0.0014660373562946916\n",
      "Memory Usage: 712.69 MB\n",
      "Epoch 41, Batch 2900/4948, Batch loss: 0.00019939076446462423\n",
      "Memory Usage: 711.09 MB\n",
      "Epoch 41, Batch 3000/4948, Batch loss: 5.712190977646969e-05\n",
      "Memory Usage: 694.17 MB\n",
      "Epoch 41, Batch 3100/4948, Batch loss: 0.00012365120346657932\n",
      "Memory Usage: 699.98 MB\n",
      "Epoch 41, Batch 3200/4948, Batch loss: 0.0003287611179985106\n",
      "Memory Usage: 708.64 MB\n",
      "Epoch 41, Batch 3300/4948, Batch loss: 0.00013942852092441171\n",
      "Memory Usage: 711.58 MB\n",
      "Epoch 41, Batch 3400/4948, Batch loss: 5.944745680608321e-06\n",
      "Memory Usage: 711.58 MB\n",
      "Epoch 41, Batch 3500/4948, Batch loss: 4.947206252836622e-05\n",
      "Memory Usage: 713.36 MB\n",
      "Epoch 41, Batch 3600/4948, Batch loss: 4.142860052525066e-05\n",
      "Memory Usage: 713.97 MB\n",
      "Epoch 41, Batch 3700/4948, Batch loss: 0.0005531114875338972\n",
      "Memory Usage: 713.97 MB\n",
      "Epoch 41, Batch 3800/4948, Batch loss: 0.00010761802695924416\n",
      "Memory Usage: 714.00 MB\n",
      "Epoch 41, Batch 3900/4948, Batch loss: 0.00013548703282140195\n",
      "Memory Usage: 720.81 MB\n",
      "Epoch 41, Batch 4000/4948, Batch loss: 0.00014803322847001255\n",
      "Memory Usage: 732.56 MB\n",
      "Epoch 41, Batch 4100/4948, Batch loss: 5.155026519787498e-05\n",
      "Memory Usage: 744.50 MB\n",
      "Epoch 41, Batch 4200/4948, Batch loss: 0.0010231537744402885\n",
      "Memory Usage: 756.27 MB\n",
      "Epoch 41, Batch 4300/4948, Batch loss: 0.0001453064032830298\n",
      "Memory Usage: 757.58 MB\n",
      "Epoch 41, Batch 4400/4948, Batch loss: 3.457960929154069e-06\n",
      "Memory Usage: 770.83 MB\n",
      "Epoch 41, Batch 4500/4948, Batch loss: 0.000979626551270485\n",
      "Memory Usage: 785.59 MB\n",
      "Epoch 41, Batch 4600/4948, Batch loss: 0.0002404794649919495\n",
      "Memory Usage: 788.84 MB\n",
      "Epoch 41, Batch 4700/4948, Batch loss: 0.0004192749911453575\n",
      "Memory Usage: 790.03 MB\n",
      "Epoch 41, Batch 4800/4948, Batch loss: 0.00011184620234416798\n",
      "Memory Usage: 790.38 MB\n",
      "Epoch 41, Batch 4900/4948, Batch loss: 7.236486271722242e-05\n",
      "Memory Usage: 790.77 MB\n",
      "Epoch 41, Batch 4948/4948, Batch loss: 0.0010761927114799619\n",
      "Memory Usage: 790.81 MB\n",
      "Epoch 41 completed in 65.52 seconds, Total Training Loss: 0.0003119010125299516\n",
      "Validation completed in 3.55 seconds, Average Validation Loss: 0.0004960345773479842\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 42/100\n",
      "Epoch 42, Batch 100/4948, Batch loss: 0.00017143615696113557\n",
      "Memory Usage: 931.17 MB\n",
      "Epoch 42, Batch 200/4948, Batch loss: 1.4813836060056929e-05\n",
      "Memory Usage: 931.17 MB\n",
      "Epoch 42, Batch 300/4948, Batch loss: 0.0003099971800111234\n",
      "Memory Usage: 931.92 MB\n",
      "Epoch 42, Batch 400/4948, Batch loss: 0.00011549970076885074\n",
      "Memory Usage: 931.95 MB\n",
      "Epoch 42, Batch 500/4948, Batch loss: 5.1570132200140506e-05\n",
      "Memory Usage: 932.00 MB\n",
      "Epoch 42, Batch 600/4948, Batch loss: 0.0004667842877097428\n",
      "Memory Usage: 932.03 MB\n",
      "Epoch 42, Batch 700/4948, Batch loss: 5.853284528711811e-05\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 42, Batch 800/4948, Batch loss: 0.00012422657164279372\n",
      "Memory Usage: 931.41 MB\n",
      "Epoch 42, Batch 900/4948, Batch loss: 0.0005365305114537477\n",
      "Memory Usage: 932.03 MB\n",
      "Epoch 42, Batch 1000/4948, Batch loss: 8.513782813679427e-05\n",
      "Memory Usage: 931.55 MB\n",
      "Epoch 42, Batch 1100/4948, Batch loss: 0.0008612473611719906\n",
      "Memory Usage: 931.55 MB\n",
      "Epoch 42, Batch 1200/4948, Batch loss: 0.001510858186520636\n",
      "Memory Usage: 931.86 MB\n",
      "Epoch 42, Batch 1300/4948, Batch loss: 0.00015164379146881402\n",
      "Memory Usage: 931.58 MB\n",
      "Epoch 42, Batch 1400/4948, Batch loss: 0.000122836310765706\n",
      "Memory Usage: 931.75 MB\n",
      "Epoch 42, Batch 1500/4948, Batch loss: 7.402676419587806e-05\n",
      "Memory Usage: 932.00 MB\n",
      "Epoch 42, Batch 1600/4948, Batch loss: 1.419208729203092e-05\n",
      "Memory Usage: 932.00 MB\n",
      "Epoch 42, Batch 1700/4948, Batch loss: 0.0004505949036683887\n",
      "Memory Usage: 932.00 MB\n",
      "Epoch 42, Batch 1800/4948, Batch loss: 0.0002612476237118244\n",
      "Memory Usage: 932.00 MB\n",
      "Epoch 42, Batch 1900/4948, Batch loss: 0.004157275427132845\n",
      "Memory Usage: 932.02 MB\n",
      "Epoch 42, Batch 2000/4948, Batch loss: 0.0004483335360419005\n",
      "Memory Usage: 932.03 MB\n",
      "Epoch 42, Batch 2100/4948, Batch loss: 0.0007424411014653742\n",
      "Memory Usage: 932.03 MB\n",
      "Epoch 42, Batch 2200/4948, Batch loss: 0.00010570933955023065\n",
      "Memory Usage: 932.03 MB\n",
      "Epoch 42, Batch 2300/4948, Batch loss: 0.00021643482614308596\n",
      "Memory Usage: 932.03 MB\n",
      "Epoch 42, Batch 2400/4948, Batch loss: 0.0001045180470100604\n",
      "Memory Usage: 932.03 MB\n",
      "Epoch 42, Batch 2500/4948, Batch loss: 4.7919817006913945e-05\n",
      "Memory Usage: 930.09 MB\n",
      "Epoch 42, Batch 2600/4948, Batch loss: 0.00038463022792711854\n",
      "Memory Usage: 930.83 MB\n",
      "Epoch 42, Batch 2700/4948, Batch loss: 0.00036621681647375226\n",
      "Memory Usage: 931.06 MB\n",
      "Epoch 42, Batch 2800/4948, Batch loss: 0.0014756146119907498\n",
      "Memory Usage: 931.08 MB\n",
      "Epoch 42, Batch 2900/4948, Batch loss: 0.00019992835586890578\n",
      "Memory Usage: 931.08 MB\n",
      "Epoch 42, Batch 3000/4948, Batch loss: 5.775500903837383e-05\n",
      "Memory Usage: 931.08 MB\n",
      "Epoch 42, Batch 3100/4948, Batch loss: 0.00012280883674975485\n",
      "Memory Usage: 931.08 MB\n",
      "Epoch 42, Batch 3200/4948, Batch loss: 0.000326374894939363\n",
      "Memory Usage: 931.11 MB\n",
      "Epoch 42, Batch 3300/4948, Batch loss: 0.00014094174548517913\n",
      "Memory Usage: 931.11 MB\n",
      "Epoch 42, Batch 3400/4948, Batch loss: 6.367979949573055e-06\n",
      "Memory Usage: 931.11 MB\n",
      "Epoch 42, Batch 3500/4948, Batch loss: 5.2603390940930694e-05\n",
      "Memory Usage: 931.11 MB\n",
      "Epoch 42, Batch 3600/4948, Batch loss: 4.024000372737646e-05\n",
      "Memory Usage: 931.12 MB\n",
      "Epoch 42, Batch 3700/4948, Batch loss: 0.0005475958460010588\n",
      "Memory Usage: 931.12 MB\n",
      "Epoch 42, Batch 3800/4948, Batch loss: 0.00010811475658556446\n",
      "Memory Usage: 931.12 MB\n",
      "Epoch 42, Batch 3900/4948, Batch loss: 0.0001424673100700602\n",
      "Memory Usage: 931.73 MB\n",
      "Epoch 42, Batch 4000/4948, Batch loss: 0.00014766521053388715\n",
      "Memory Usage: 931.19 MB\n",
      "Epoch 42, Batch 4100/4948, Batch loss: 4.497496411204338e-05\n",
      "Memory Usage: 931.50 MB\n",
      "Epoch 42, Batch 4200/4948, Batch loss: 0.0010196499060839415\n",
      "Memory Usage: 931.50 MB\n",
      "Epoch 42, Batch 4300/4948, Batch loss: 0.0001443759974790737\n",
      "Memory Usage: 931.72 MB\n",
      "Epoch 42, Batch 4400/4948, Batch loss: 3.535393716447288e-06\n",
      "Memory Usage: 931.75 MB\n",
      "Epoch 42, Batch 4500/4948, Batch loss: 0.0009759378153830767\n",
      "Memory Usage: 928.34 MB\n",
      "Epoch 42, Batch 4600/4948, Batch loss: 0.00024186640803236514\n",
      "Memory Usage: 930.56 MB\n",
      "Epoch 42, Batch 4700/4948, Batch loss: 0.0004209491889923811\n",
      "Memory Usage: 930.66 MB\n",
      "Epoch 42, Batch 4800/4948, Batch loss: 0.00011108259786851704\n",
      "Memory Usage: 931.36 MB\n",
      "Epoch 42, Batch 4900/4948, Batch loss: 7.028587424429134e-05\n",
      "Memory Usage: 931.41 MB\n",
      "Epoch 42, Batch 4948/4948, Batch loss: 0.001027817139402032\n",
      "Memory Usage: 928.81 MB\n",
      "Epoch 42 completed in 64.02 seconds, Total Training Loss: 0.0003124723919246327\n",
      "\n",
      "Epoch 43/100\n",
      "Epoch 43, Batch 100/4948, Batch loss: 0.00017066803411580622\n",
      "Memory Usage: 928.83 MB\n",
      "Epoch 43, Batch 200/4948, Batch loss: 1.7959746401174925e-05\n",
      "Memory Usage: 923.97 MB\n",
      "Epoch 43, Batch 300/4948, Batch loss: 0.00030706115649081767\n",
      "Memory Usage: 925.58 MB\n",
      "Epoch 43, Batch 400/4948, Batch loss: 0.00011614313552854583\n",
      "Memory Usage: 927.53 MB\n",
      "Epoch 43, Batch 500/4948, Batch loss: 5.142694499227218e-05\n",
      "Memory Usage: 928.28 MB\n",
      "Epoch 43, Batch 600/4948, Batch loss: 0.0004695833195000887\n",
      "Memory Usage: 928.66 MB\n",
      "Epoch 43, Batch 700/4948, Batch loss: 7.682736759306863e-05\n",
      "Memory Usage: 929.17 MB\n",
      "Epoch 43, Batch 800/4948, Batch loss: 0.00015353424532804638\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 43, Batch 900/4948, Batch loss: 0.0005357344634830952\n",
      "Memory Usage: 930.52 MB\n",
      "Epoch 43, Batch 1000/4948, Batch loss: 8.661198808113113e-05\n",
      "Memory Usage: 930.52 MB\n",
      "Epoch 43, Batch 1100/4948, Batch loss: 0.0008580775465816259\n",
      "Memory Usage: 930.52 MB\n",
      "Epoch 43, Batch 1200/4948, Batch loss: 0.0015104245394468307\n",
      "Memory Usage: 930.53 MB\n",
      "Epoch 43, Batch 1300/4948, Batch loss: 0.0001533455215394497\n",
      "Memory Usage: 930.53 MB\n",
      "Epoch 43, Batch 1400/4948, Batch loss: 0.00012340322427917272\n",
      "Memory Usage: 931.58 MB\n",
      "Epoch 43, Batch 1500/4948, Batch loss: 7.609754538862035e-05\n",
      "Memory Usage: 931.62 MB\n",
      "Epoch 43, Batch 1600/4948, Batch loss: 1.3424922144622542e-05\n",
      "Memory Usage: 931.72 MB\n",
      "Epoch 43, Batch 1700/4948, Batch loss: 0.0003622132644522935\n",
      "Memory Usage: 931.75 MB\n",
      "Epoch 43, Batch 1800/4948, Batch loss: 0.00026371836429461837\n",
      "Memory Usage: 931.53 MB\n",
      "Epoch 43, Batch 1900/4948, Batch loss: 0.004186517558991909\n",
      "Memory Usage: 931.47 MB\n",
      "Epoch 43, Batch 2000/4948, Batch loss: 0.00041709671495482326\n",
      "Memory Usage: 931.56 MB\n",
      "Epoch 43, Batch 2100/4948, Batch loss: 0.0006913288962095976\n",
      "Memory Usage: 931.56 MB\n",
      "Epoch 43, Batch 2200/4948, Batch loss: 0.00010585622658254579\n",
      "Memory Usage: 931.56 MB\n",
      "Epoch 43, Batch 2300/4948, Batch loss: 0.0002325079549336806\n",
      "Memory Usage: 932.62 MB\n",
      "Epoch 43, Batch 2400/4948, Batch loss: 0.00010450436093378812\n",
      "Memory Usage: 932.64 MB\n",
      "Epoch 43, Batch 2500/4948, Batch loss: 4.8888916353462264e-05\n",
      "Memory Usage: 932.64 MB\n",
      "Epoch 43, Batch 2600/4948, Batch loss: 0.00038377728196792305\n",
      "Memory Usage: 932.64 MB\n",
      "Epoch 43, Batch 2700/4948, Batch loss: 0.0003806576714850962\n",
      "Memory Usage: 932.64 MB\n",
      "Epoch 43, Batch 2800/4948, Batch loss: 0.001462784013710916\n",
      "Memory Usage: 932.64 MB\n",
      "Epoch 43, Batch 2900/4948, Batch loss: 0.00020055376808159053\n",
      "Memory Usage: 932.64 MB\n",
      "Epoch 43, Batch 3000/4948, Batch loss: 5.7543948059901595e-05\n",
      "Memory Usage: 932.66 MB\n",
      "Epoch 43, Batch 3100/4948, Batch loss: 0.00012286592391319573\n",
      "Memory Usage: 932.66 MB\n",
      "Epoch 43, Batch 3200/4948, Batch loss: 0.000313144875690341\n",
      "Memory Usage: 932.66 MB\n",
      "Epoch 43, Batch 3300/4948, Batch loss: 0.00014152860967442393\n",
      "Memory Usage: 932.69 MB\n",
      "Epoch 43, Batch 3400/4948, Batch loss: 6.22012839812669e-06\n",
      "Memory Usage: 932.69 MB\n",
      "Epoch 43, Batch 3500/4948, Batch loss: 5.365087781683542e-05\n",
      "Memory Usage: 932.69 MB\n",
      "Epoch 43, Batch 3600/4948, Batch loss: 4.106037158635445e-05\n",
      "Memory Usage: 932.69 MB\n",
      "Epoch 43, Batch 3700/4948, Batch loss: 0.0005484866560436785\n",
      "Memory Usage: 932.70 MB\n",
      "Epoch 43, Batch 3800/4948, Batch loss: 0.00010576890053926036\n",
      "Memory Usage: 932.70 MB\n",
      "Epoch 43, Batch 3900/4948, Batch loss: 0.0001359625020995736\n",
      "Memory Usage: 932.70 MB\n",
      "Epoch 43, Batch 4000/4948, Batch loss: 0.00014889039448462427\n",
      "Memory Usage: 932.70 MB\n",
      "Epoch 43, Batch 4100/4948, Batch loss: 4.543597242445685e-05\n",
      "Memory Usage: 932.70 MB\n",
      "Epoch 43, Batch 4200/4948, Batch loss: 0.0010215031215921044\n",
      "Memory Usage: 932.81 MB\n",
      "Epoch 43, Batch 4300/4948, Batch loss: 0.00014399482461158186\n",
      "Memory Usage: 932.81 MB\n",
      "Epoch 43, Batch 4400/4948, Batch loss: 4.0945988075691275e-06\n",
      "Memory Usage: 932.91 MB\n",
      "Epoch 43, Batch 4500/4948, Batch loss: 0.0009683346725068986\n",
      "Memory Usage: 932.91 MB\n",
      "Epoch 43, Batch 4600/4948, Batch loss: 0.00023928350128699094\n",
      "Memory Usage: 932.91 MB\n",
      "Epoch 43, Batch 4700/4948, Batch loss: 0.0004206589364912361\n",
      "Memory Usage: 932.91 MB\n",
      "Epoch 43, Batch 4800/4948, Batch loss: 0.00011517852544784546\n",
      "Memory Usage: 932.91 MB\n",
      "Epoch 43, Batch 4900/4948, Batch loss: 7.078829366946593e-05\n",
      "Memory Usage: 932.91 MB\n",
      "Epoch 43, Batch 4948/4948, Batch loss: 0.0010437376331537962\n",
      "Memory Usage: 932.94 MB\n",
      "Epoch 43 completed in 61.62 seconds, Total Training Loss: 0.00030825935011806455\n",
      "\n",
      "Epoch 44/100\n",
      "Epoch 44, Batch 100/4948, Batch loss: 0.00017077216762118042\n",
      "Memory Usage: 932.94 MB\n",
      "Epoch 44, Batch 200/4948, Batch loss: 1.5413626897498034e-05\n",
      "Memory Usage: 932.94 MB\n",
      "Epoch 44, Batch 300/4948, Batch loss: 0.0003080628521274775\n",
      "Memory Usage: 932.94 MB\n",
      "Epoch 44, Batch 400/4948, Batch loss: 0.0001152914046542719\n",
      "Memory Usage: 933.69 MB\n",
      "Epoch 44, Batch 500/4948, Batch loss: 5.133285230840556e-05\n",
      "Memory Usage: 933.69 MB\n",
      "Epoch 44, Batch 600/4948, Batch loss: 0.0004719071730505675\n",
      "Memory Usage: 933.69 MB\n",
      "Epoch 44, Batch 700/4948, Batch loss: 5.6730888900347054e-05\n",
      "Memory Usage: 933.69 MB\n",
      "Epoch 44, Batch 800/4948, Batch loss: 0.00013730682258028537\n",
      "Memory Usage: 933.69 MB\n",
      "Epoch 44, Batch 900/4948, Batch loss: 0.0005339773488231003\n",
      "Memory Usage: 933.69 MB\n",
      "Epoch 44, Batch 1000/4948, Batch loss: 8.854208863340318e-05\n",
      "Memory Usage: 933.70 MB\n",
      "Epoch 44, Batch 1100/4948, Batch loss: 0.0008664920460432768\n",
      "Memory Usage: 933.70 MB\n",
      "Epoch 44, Batch 1200/4948, Batch loss: 0.0014880208764225245\n",
      "Memory Usage: 933.70 MB\n",
      "Epoch 44, Batch 1300/4948, Batch loss: 0.00015404884470626712\n",
      "Memory Usage: 933.70 MB\n",
      "Epoch 44, Batch 1400/4948, Batch loss: 0.00012351805344223976\n",
      "Memory Usage: 933.70 MB\n",
      "Epoch 44, Batch 1500/4948, Batch loss: 8.681225881446153e-05\n",
      "Memory Usage: 933.70 MB\n",
      "Epoch 44, Batch 1600/4948, Batch loss: 1.3279808626975864e-05\n",
      "Memory Usage: 934.27 MB\n",
      "Epoch 44, Batch 1700/4948, Batch loss: 0.00047208464820869267\n",
      "Memory Usage: 934.27 MB\n",
      "Epoch 44, Batch 1800/4948, Batch loss: 0.0002625384659040719\n",
      "Memory Usage: 934.27 MB\n",
      "Epoch 44, Batch 1900/4948, Batch loss: 0.006926416885107756\n",
      "Memory Usage: 934.38 MB\n",
      "Epoch 44, Batch 2000/4948, Batch loss: 0.00044359418097883463\n",
      "Memory Usage: 934.39 MB\n",
      "Epoch 44, Batch 2100/4948, Batch loss: 0.0007198234088718891\n",
      "Memory Usage: 934.39 MB\n",
      "Epoch 44, Batch 2200/4948, Batch loss: 0.00010532356100156903\n",
      "Memory Usage: 934.39 MB\n",
      "Epoch 44, Batch 2300/4948, Batch loss: 0.00021632408606819808\n",
      "Memory Usage: 934.91 MB\n",
      "Epoch 44, Batch 2400/4948, Batch loss: 0.00010556318738963455\n",
      "Memory Usage: 934.91 MB\n",
      "Epoch 44, Batch 2500/4948, Batch loss: 4.956721386406571e-05\n",
      "Memory Usage: 934.91 MB\n",
      "Epoch 44, Batch 2600/4948, Batch loss: 0.0003844638413283974\n",
      "Memory Usage: 935.34 MB\n",
      "Epoch 44, Batch 2700/4948, Batch loss: 0.0003795648517552763\n",
      "Memory Usage: 935.34 MB\n",
      "Epoch 44, Batch 2800/4948, Batch loss: 0.0014480017125606537\n",
      "Memory Usage: 935.34 MB\n",
      "Epoch 44, Batch 2900/4948, Batch loss: 0.00019930812413804233\n",
      "Memory Usage: 935.34 MB\n",
      "Epoch 44, Batch 3000/4948, Batch loss: 5.769275958300568e-05\n",
      "Memory Usage: 935.34 MB\n",
      "Epoch 44, Batch 3100/4948, Batch loss: 0.00012712452735286206\n",
      "Memory Usage: 935.34 MB\n",
      "Epoch 44, Batch 3200/4948, Batch loss: 0.0003231316804885864\n",
      "Memory Usage: 935.34 MB\n",
      "Epoch 44, Batch 3300/4948, Batch loss: 0.00014057374210096896\n",
      "Memory Usage: 935.34 MB\n",
      "Epoch 44, Batch 3400/4948, Batch loss: 7.675477718294133e-06\n",
      "Memory Usage: 935.34 MB\n",
      "Epoch 44, Batch 3500/4948, Batch loss: 4.742848614114337e-05\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 44, Batch 3600/4948, Batch loss: 4.200288822175935e-05\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 44, Batch 3700/4948, Batch loss: 0.0005577690317295492\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 44, Batch 3800/4948, Batch loss: 0.00010628615564201027\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 44, Batch 3900/4948, Batch loss: 0.00013661256525665522\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 44, Batch 4000/4948, Batch loss: 0.00014937439118511975\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 44, Batch 4100/4948, Batch loss: 4.513397652772255e-05\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 44, Batch 4200/4948, Batch loss: 0.0010105581022799015\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 44, Batch 4300/4948, Batch loss: 0.00014398731582332402\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 44, Batch 4400/4948, Batch loss: 4.5763786147290375e-06\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 44, Batch 4500/4948, Batch loss: 0.0009709086734801531\n",
      "Memory Usage: 935.36 MB\n",
      "Epoch 44, Batch 4600/4948, Batch loss: 0.00023940614482853562\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 44, Batch 4700/4948, Batch loss: 0.0004162660916335881\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 44, Batch 4800/4948, Batch loss: 0.00010985768312821165\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 44, Batch 4900/4948, Batch loss: 7.084714161464944e-05\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 44, Batch 4948/4948, Batch loss: 0.0010282768635079265\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 44 completed in 60.41 seconds, Total Training Loss: 0.00031091323509854524\n",
      "\n",
      "Epoch 45/100\n",
      "Epoch 45, Batch 100/4948, Batch loss: 0.00016932333528529853\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 200/4948, Batch loss: 1.3517126717488281e-05\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 300/4948, Batch loss: 0.00030964415054768324\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 400/4948, Batch loss: 0.0001155856007244438\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 500/4948, Batch loss: 5.155705730430782e-05\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 600/4948, Batch loss: 0.00046916783321648836\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 700/4948, Batch loss: 8.04008450359106e-05\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 800/4948, Batch loss: 0.00012514933769125491\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 900/4948, Batch loss: 0.0005240986938588321\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 1000/4948, Batch loss: 8.476158836856484e-05\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 1100/4948, Batch loss: 0.0008751808782108128\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 1200/4948, Batch loss: 0.0014852163149043918\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 1300/4948, Batch loss: 0.0001516118209110573\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 1400/4948, Batch loss: 0.00012301161768846214\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 1500/4948, Batch loss: 8.584163879277185e-05\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 1600/4948, Batch loss: 1.332590909441933e-05\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 1700/4948, Batch loss: 0.0004058432823512703\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 1800/4948, Batch loss: 0.0002620176237542182\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 1900/4948, Batch loss: 0.003930190112441778\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 2000/4948, Batch loss: 0.0004521661903709173\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 2100/4948, Batch loss: 0.0007265980239026248\n",
      "Memory Usage: 935.38 MB\n",
      "Epoch 45, Batch 2200/4948, Batch loss: 0.00010478025069460273\n",
      "Memory Usage: 935.39 MB\n",
      "Epoch 45, Batch 2300/4948, Batch loss: 0.00021869812917429954\n",
      "Memory Usage: 935.39 MB\n",
      "Epoch 45, Batch 2400/4948, Batch loss: 0.00010759507131297141\n",
      "Memory Usage: 935.39 MB\n",
      "Epoch 45, Batch 2500/4948, Batch loss: 4.84185729874298e-05\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 2600/4948, Batch loss: 0.00038726473576389253\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 2700/4948, Batch loss: 0.00037110934499651194\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 2800/4948, Batch loss: 0.0014476992655545473\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 2900/4948, Batch loss: 0.00019918927864637226\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 3000/4948, Batch loss: 5.745286398450844e-05\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 3100/4948, Batch loss: 0.00012470240471884608\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 3200/4948, Batch loss: 0.0003202744701411575\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 3300/4948, Batch loss: 0.00013870121620129794\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 3400/4948, Batch loss: 7.999506124178879e-06\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 3500/4948, Batch loss: 4.765890116686933e-05\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 3600/4948, Batch loss: 4.04271122533828e-05\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 3700/4948, Batch loss: 0.0005488860770128667\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 3800/4948, Batch loss: 0.00010720206773839891\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 3900/4948, Batch loss: 0.00014024956908542663\n",
      "Memory Usage: 935.41 MB\n",
      "Epoch 45, Batch 4000/4948, Batch loss: 0.0001484573003835976\n",
      "Memory Usage: 935.42 MB\n",
      "Epoch 45, Batch 4100/4948, Batch loss: 4.6397795813390985e-05\n",
      "Memory Usage: 935.42 MB\n",
      "Epoch 45, Batch 4200/4948, Batch loss: 0.0010209883330389857\n",
      "Memory Usage: 935.42 MB\n",
      "Epoch 45, Batch 4300/4948, Batch loss: 0.00014319051115307957\n",
      "Memory Usage: 935.42 MB\n",
      "Epoch 45, Batch 4400/4948, Batch loss: 3.856880084640579e-06\n",
      "Memory Usage: 935.42 MB\n",
      "Epoch 45, Batch 4500/4948, Batch loss: 0.0009629896958358586\n",
      "Memory Usage: 935.42 MB\n",
      "Epoch 45, Batch 4600/4948, Batch loss: 0.0002380875957896933\n",
      "Memory Usage: 935.42 MB\n",
      "Epoch 45, Batch 4700/4948, Batch loss: 0.0004150884924456477\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 45, Batch 4800/4948, Batch loss: 0.00012572036939673126\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 45, Batch 4900/4948, Batch loss: 7.471045682905242e-05\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 45, Batch 4948/4948, Batch loss: 0.0010732628870755434\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 45 completed in 61.30 seconds, Total Training Loss: 0.00030971303201468217\n",
      "\n",
      "Epoch 46/100\n",
      "Epoch 46, Batch 100/4948, Batch loss: 0.00016940843488555402\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 200/4948, Batch loss: 1.5617735698469914e-05\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 300/4948, Batch loss: 0.0003091174876317382\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 400/4948, Batch loss: 0.00011702601477736607\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 500/4948, Batch loss: 5.143945600138977e-05\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 600/4948, Batch loss: 0.00047438545152544975\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 700/4948, Batch loss: 5.691829210263677e-05\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 800/4948, Batch loss: 0.0001243252627318725\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 900/4948, Batch loss: 0.0005252892733551562\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 1000/4948, Batch loss: 8.52575758472085e-05\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 1100/4948, Batch loss: 0.0008666064823046327\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 1200/4948, Batch loss: 0.0014629303477704525\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 1300/4948, Batch loss: 0.00015491200610995293\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 1400/4948, Batch loss: 0.0001238622353412211\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 1500/4948, Batch loss: 8.088011236395687e-05\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 1600/4948, Batch loss: 1.608776619832497e-05\n",
      "Memory Usage: 936.33 MB\n",
      "Epoch 46, Batch 1700/4948, Batch loss: 0.0004643722204491496\n",
      "Memory Usage: 936.34 MB\n",
      "Epoch 46, Batch 1800/4948, Batch loss: 0.0002624014741741121\n",
      "Memory Usage: 936.34 MB\n",
      "Epoch 46, Batch 1900/4948, Batch loss: 0.003989321645349264\n",
      "Memory Usage: 936.34 MB\n",
      "Epoch 46, Batch 2000/4948, Batch loss: 0.0004488566191866994\n",
      "Memory Usage: 936.34 MB\n",
      "Epoch 46, Batch 2100/4948, Batch loss: 0.0007239374099299312\n",
      "Memory Usage: 936.34 MB\n",
      "Epoch 46, Batch 2200/4948, Batch loss: 0.0001050696155289188\n",
      "Memory Usage: 936.34 MB\n",
      "Epoch 46, Batch 2300/4948, Batch loss: 0.0002230318496003747\n",
      "Memory Usage: 936.34 MB\n",
      "Epoch 46, Batch 2400/4948, Batch loss: 0.0001054624590324238\n",
      "Memory Usage: 860.89 MB\n",
      "Epoch 46, Batch 2500/4948, Batch loss: 4.858466490986757e-05\n",
      "Memory Usage: 867.30 MB\n",
      "Epoch 46, Batch 2600/4948, Batch loss: 0.00038354264688678086\n",
      "Memory Usage: 872.50 MB\n",
      "Epoch 46, Batch 2700/4948, Batch loss: 0.00036448563332669437\n",
      "Memory Usage: 880.34 MB\n",
      "Epoch 46, Batch 2800/4948, Batch loss: 0.0014134618686512113\n",
      "Memory Usage: 885.64 MB\n",
      "Epoch 46, Batch 2900/4948, Batch loss: 0.00019893268472515047\n",
      "Memory Usage: 888.41 MB\n",
      "Epoch 46, Batch 3000/4948, Batch loss: 5.757335020462051e-05\n",
      "Memory Usage: 890.06 MB\n",
      "Epoch 46, Batch 3100/4948, Batch loss: 0.00012312021863181144\n",
      "Memory Usage: 891.80 MB\n",
      "Epoch 46, Batch 3200/4948, Batch loss: 0.00032003934029489756\n",
      "Memory Usage: 893.17 MB\n",
      "Epoch 46, Batch 3300/4948, Batch loss: 0.00014039983216207474\n",
      "Memory Usage: 893.70 MB\n",
      "Epoch 46, Batch 3400/4948, Batch loss: 5.5946748034330085e-06\n",
      "Memory Usage: 893.72 MB\n",
      "Epoch 46, Batch 3500/4948, Batch loss: 5.187452188692987e-05\n",
      "Memory Usage: 894.05 MB\n",
      "Epoch 46, Batch 3600/4948, Batch loss: 4.286818875698373e-05\n",
      "Memory Usage: 895.30 MB\n",
      "Epoch 46, Batch 3700/4948, Batch loss: 0.0005518509424291551\n",
      "Memory Usage: 895.31 MB\n",
      "Epoch 46, Batch 3800/4948, Batch loss: 0.00010625198046909645\n",
      "Memory Usage: 895.34 MB\n",
      "Epoch 46, Batch 3900/4948, Batch loss: 0.00013573192700278014\n",
      "Memory Usage: 895.36 MB\n",
      "Epoch 46, Batch 4000/4948, Batch loss: 0.00014816799375694245\n",
      "Memory Usage: 895.52 MB\n",
      "Epoch 46, Batch 4100/4948, Batch loss: 4.535550033324398e-05\n",
      "Memory Usage: 895.53 MB\n",
      "Epoch 46, Batch 4200/4948, Batch loss: 0.0010184909915551543\n",
      "Memory Usage: 895.55 MB\n",
      "Epoch 46, Batch 4300/4948, Batch loss: 0.00014476911746896803\n",
      "Memory Usage: 896.33 MB\n",
      "Epoch 46, Batch 4400/4948, Batch loss: 3.4880613384302706e-06\n",
      "Memory Usage: 896.33 MB\n",
      "Epoch 46, Batch 4500/4948, Batch loss: 0.0009664427489042282\n",
      "Memory Usage: 896.33 MB\n",
      "Epoch 46, Batch 4600/4948, Batch loss: 0.00023839110508561134\n",
      "Memory Usage: 896.39 MB\n",
      "Epoch 46, Batch 4700/4948, Batch loss: 0.0004149599699303508\n",
      "Memory Usage: 897.45 MB\n",
      "Epoch 46, Batch 4800/4948, Batch loss: 0.00011379476200090721\n",
      "Memory Usage: 897.45 MB\n",
      "Epoch 46, Batch 4900/4948, Batch loss: 7.110444857971743e-05\n",
      "Memory Usage: 897.45 MB\n",
      "Epoch 46, Batch 4948/4948, Batch loss: 0.0010397861478850245\n",
      "Memory Usage: 897.48 MB\n",
      "Epoch 46 completed in 62.25 seconds, Total Training Loss: 0.0003069558450774505\n",
      "\n",
      "Epoch 47/100\n",
      "Epoch 47, Batch 100/4948, Batch loss: 0.000170428553246893\n",
      "Memory Usage: 897.48 MB\n",
      "Epoch 47, Batch 200/4948, Batch loss: 1.8067716155201197e-05\n",
      "Memory Usage: 897.89 MB\n",
      "Epoch 47, Batch 300/4948, Batch loss: 0.0003051805542781949\n",
      "Memory Usage: 897.91 MB\n",
      "Epoch 47, Batch 400/4948, Batch loss: 0.0001150285461335443\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 500/4948, Batch loss: 5.299221083987504e-05\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 600/4948, Batch loss: 0.0004702892038039863\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 700/4948, Batch loss: 5.7197168644052e-05\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 800/4948, Batch loss: 0.00012489862274378538\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 900/4948, Batch loss: 0.000527785683516413\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 1000/4948, Batch loss: 8.534613880328834e-05\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 1100/4948, Batch loss: 0.0008843178511597216\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 1200/4948, Batch loss: 0.001449991948902607\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 1300/4948, Batch loss: 0.00015445803001057357\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 1400/4948, Batch loss: 0.0001238728582393378\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 1500/4948, Batch loss: 8.089959737844765e-05\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 1600/4948, Batch loss: 1.3532502634916455e-05\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 1700/4948, Batch loss: 0.0003863981692120433\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 1800/4948, Batch loss: 0.0002644214837346226\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 1900/4948, Batch loss: 0.0024807413574308157\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 2000/4948, Batch loss: 0.000453505344921723\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 2100/4948, Batch loss: 0.0007091903244145215\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 2200/4948, Batch loss: 0.00010531437146710232\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 2300/4948, Batch loss: 0.00022466799418907613\n",
      "Memory Usage: 897.97 MB\n",
      "Epoch 47, Batch 2400/4948, Batch loss: 0.00010644636495271698\n",
      "Memory Usage: 897.98 MB\n",
      "Epoch 47, Batch 2500/4948, Batch loss: 4.883549263468012e-05\n",
      "Memory Usage: 897.98 MB\n",
      "Epoch 47, Batch 2600/4948, Batch loss: 0.00038543870323337615\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 2700/4948, Batch loss: 0.00038088037399575114\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 2800/4948, Batch loss: 0.0014365942915901542\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 2900/4948, Batch loss: 0.00019997719209641218\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 3000/4948, Batch loss: 5.8449175412533805e-05\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 3100/4948, Batch loss: 0.00012434217205736786\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 3200/4948, Batch loss: 0.0003031920932698995\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 3300/4948, Batch loss: 0.00014039561210665852\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 3400/4948, Batch loss: 5.019142918172292e-06\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 3500/4948, Batch loss: 4.959972284268588e-05\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 3600/4948, Batch loss: 4.061530125909485e-05\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 3700/4948, Batch loss: 0.0005405397387221456\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 3800/4948, Batch loss: 0.00010560412192717195\n",
      "Memory Usage: 882.70 MB\n",
      "Epoch 47, Batch 3900/4948, Batch loss: 0.0001336726127192378\n",
      "Memory Usage: 883.02 MB\n",
      "Epoch 47, Batch 4000/4948, Batch loss: 0.0001486572000430897\n",
      "Memory Usage: 883.02 MB\n",
      "Epoch 47, Batch 4100/4948, Batch loss: 4.597307997755706e-05\n",
      "Memory Usage: 883.02 MB\n",
      "Epoch 47, Batch 4200/4948, Batch loss: 0.000997658702544868\n",
      "Memory Usage: 883.52 MB\n",
      "Epoch 47, Batch 4300/4948, Batch loss: 0.00014405307592824101\n",
      "Memory Usage: 883.52 MB\n",
      "Epoch 47, Batch 4400/4948, Batch loss: 4.102470484212972e-06\n",
      "Memory Usage: 883.52 MB\n",
      "Epoch 47, Batch 4500/4948, Batch loss: 0.0009678753558546305\n",
      "Memory Usage: 883.52 MB\n",
      "Epoch 47, Batch 4600/4948, Batch loss: 0.00023828150006011128\n",
      "Memory Usage: 883.52 MB\n",
      "Epoch 47, Batch 4700/4948, Batch loss: 0.00041647168109193444\n",
      "Memory Usage: 883.52 MB\n",
      "Epoch 47, Batch 4800/4948, Batch loss: 0.00011044081475120038\n",
      "Memory Usage: 883.61 MB\n",
      "Epoch 47, Batch 4900/4948, Batch loss: 6.963617488509044e-05\n",
      "Memory Usage: 883.61 MB\n",
      "Epoch 47, Batch 4948/4948, Batch loss: 0.001025036326609552\n",
      "Memory Usage: 883.64 MB\n",
      "Epoch 47 completed in 62.07 seconds, Total Training Loss: 0.0002972895733343991\n",
      "\n",
      "Epoch 48/100\n",
      "Epoch 48, Batch 100/4948, Batch loss: 0.00017071535694412887\n",
      "Memory Usage: 883.64 MB\n",
      "Epoch 48, Batch 200/4948, Batch loss: 1.5223186892399099e-05\n",
      "Memory Usage: 883.66 MB\n",
      "Epoch 48, Batch 300/4948, Batch loss: 0.00030972680542618036\n",
      "Memory Usage: 883.66 MB\n",
      "Epoch 48, Batch 400/4948, Batch loss: 0.00011578579142224044\n",
      "Memory Usage: 883.67 MB\n",
      "Epoch 48, Batch 500/4948, Batch loss: 5.1244187488919124e-05\n",
      "Memory Usage: 883.67 MB\n",
      "Epoch 48, Batch 600/4948, Batch loss: 0.000491958751808852\n",
      "Memory Usage: 883.67 MB\n",
      "Epoch 48, Batch 700/4948, Batch loss: 5.755151141784154e-05\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 800/4948, Batch loss: 0.00012480077566578984\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 900/4948, Batch loss: 0.0005392677849158645\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 1000/4948, Batch loss: 8.569040801376104e-05\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 1100/4948, Batch loss: 0.0008516297675669193\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 1200/4948, Batch loss: 0.0014375194441527128\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 1300/4948, Batch loss: 0.00015441517462022603\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 1400/4948, Batch loss: 0.0001217425597133115\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 1500/4948, Batch loss: 7.279991405084729e-05\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 1600/4948, Batch loss: 1.4367009498528205e-05\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 1700/4948, Batch loss: 0.00029224558966234326\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 1800/4948, Batch loss: 0.0002626596251502633\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 1900/4948, Batch loss: 0.0022701509296894073\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 2000/4948, Batch loss: 0.0004543308459687978\n",
      "Memory Usage: 883.69 MB\n",
      "Epoch 48, Batch 2100/4948, Batch loss: 0.000677694333717227\n",
      "Memory Usage: 892.67 MB\n",
      "Epoch 48, Batch 2200/4948, Batch loss: 0.00010530377767281607\n",
      "Memory Usage: 899.69 MB\n",
      "Epoch 48, Batch 2300/4948, Batch loss: 0.00022081719362176955\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 2400/4948, Batch loss: 0.00010421661863802001\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 2500/4948, Batch loss: 4.8293550207745284e-05\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 2600/4948, Batch loss: 0.00038467723061330616\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 2700/4948, Batch loss: 0.0003659452486317605\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 2800/4948, Batch loss: 0.0014227064093574882\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 2900/4948, Batch loss: 0.00019981533114332706\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 3000/4948, Batch loss: 5.771835276391357e-05\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 3100/4948, Batch loss: 0.0001252593210665509\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 3200/4948, Batch loss: 0.0003045150369871408\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 3300/4948, Batch loss: 0.00013990260777063668\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 3400/4948, Batch loss: 1.269407403015066e-05\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 3500/4948, Batch loss: 4.6997552999528125e-05\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 3600/4948, Batch loss: 4.002925561508164e-05\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 3700/4948, Batch loss: 0.0005777166807092726\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 3800/4948, Batch loss: 0.00010563922114670277\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 3900/4948, Batch loss: 0.00013570596638601273\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 4000/4948, Batch loss: 0.00014722772175446153\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 4100/4948, Batch loss: 4.594339043251239e-05\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 4200/4948, Batch loss: 0.000996262882836163\n",
      "Memory Usage: 899.70 MB\n",
      "Epoch 48, Batch 4300/4948, Batch loss: 0.00014384015230461955\n",
      "Memory Usage: 899.77 MB\n",
      "Epoch 48, Batch 4400/4948, Batch loss: 3.398863782422268e-06\n",
      "Memory Usage: 899.77 MB\n",
      "Epoch 48, Batch 4500/4948, Batch loss: 0.0009531769319437444\n",
      "Memory Usage: 899.77 MB\n",
      "Epoch 48, Batch 4600/4948, Batch loss: 0.00023552784114144742\n",
      "Memory Usage: 899.77 MB\n",
      "Epoch 48, Batch 4700/4948, Batch loss: 0.00041614819201640785\n",
      "Memory Usage: 899.77 MB\n",
      "Epoch 48, Batch 4800/4948, Batch loss: 0.00011980882118223235\n",
      "Memory Usage: 899.77 MB\n",
      "Epoch 48, Batch 4900/4948, Batch loss: 7.068402919685468e-05\n",
      "Memory Usage: 899.77 MB\n",
      "Epoch 48, Batch 4948/4948, Batch loss: 0.001021506846882403\n",
      "Memory Usage: 899.77 MB\n",
      "Epoch 48 completed in 61.69 seconds, Total Training Loss: 0.00029595018983100244\n",
      "\n",
      "Epoch 49/100\n",
      "Epoch 49, Batch 100/4948, Batch loss: 0.00017040714737959206\n",
      "Memory Usage: 899.77 MB\n",
      "Epoch 49, Batch 200/4948, Batch loss: 1.7405121980118565e-05\n",
      "Memory Usage: 899.77 MB\n",
      "Epoch 49, Batch 300/4948, Batch loss: 0.0003051856183446944\n",
      "Memory Usage: 899.77 MB\n",
      "Epoch 49, Batch 400/4948, Batch loss: 0.00011561538121895865\n",
      "Memory Usage: 899.77 MB\n",
      "Epoch 49, Batch 500/4948, Batch loss: 5.0860122428275645e-05\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 600/4948, Batch loss: 0.0004850068944506347\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 700/4948, Batch loss: 5.023277481086552e-05\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 800/4948, Batch loss: 0.0001278525305679068\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 900/4948, Batch loss: 0.0005438121152110398\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 1000/4948, Batch loss: 8.635748235974461e-05\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 1100/4948, Batch loss: 0.0008557249675504863\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 1200/4948, Batch loss: 0.001416094251908362\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 1300/4948, Batch loss: 0.00015343244012910873\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 1400/4948, Batch loss: 0.00012534450797829777\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 1500/4948, Batch loss: 7.482211367459968e-05\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 1600/4948, Batch loss: 1.3241846318123862e-05\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 1700/4948, Batch loss: 0.00031757858232595026\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 1800/4948, Batch loss: 0.0002637385914567858\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 1900/4948, Batch loss: 0.003763602115213871\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 2000/4948, Batch loss: 0.0004031789430882782\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 2100/4948, Batch loss: 0.0006840048008598387\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 2200/4948, Batch loss: 0.00010638141975505278\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 2300/4948, Batch loss: 0.00022286213061306626\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 2400/4948, Batch loss: 0.00010536264016991481\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 2500/4948, Batch loss: 4.8474692448507994e-05\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 2600/4948, Batch loss: 0.00038608230534009635\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 2700/4948, Batch loss: 0.00037103911745361984\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 2800/4948, Batch loss: 0.0014098675455898046\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 2900/4948, Batch loss: 0.0002000510139623657\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 3000/4948, Batch loss: 5.8855093811871484e-05\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 3100/4948, Batch loss: 0.0001274480891879648\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 3200/4948, Batch loss: 0.00030616766889579594\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 3300/4948, Batch loss: 0.00013970013242214918\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 3400/4948, Batch loss: 6.035190835973481e-06\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 3500/4948, Batch loss: 5.0998896767850965e-05\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 3600/4948, Batch loss: 4.134765185881406e-05\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 3700/4948, Batch loss: 0.0005410973681136966\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 3800/4948, Batch loss: 0.00010741663572844118\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 3900/4948, Batch loss: 0.00013450148981064558\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 4000/4948, Batch loss: 0.00014694270794279873\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 4100/4948, Batch loss: 4.5963806769577786e-05\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 4200/4948, Batch loss: 0.000997609575279057\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 4300/4948, Batch loss: 0.00014537388051394373\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 4400/4948, Batch loss: 4.067403551744064e-06\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 4500/4948, Batch loss: 0.0009491884266026318\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 4600/4948, Batch loss: 0.0002394111070316285\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 4700/4948, Batch loss: 0.00042017956729978323\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 4800/4948, Batch loss: 0.0001136415739892982\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 4900/4948, Batch loss: 7.402469782391563e-05\n",
      "Memory Usage: 899.78 MB\n",
      "Epoch 49, Batch 4948/4948, Batch loss: 0.0010278327390551567\n",
      "Memory Usage: 899.80 MB\n",
      "Epoch 49 completed in 61.83 seconds, Total Training Loss: 0.00030062682184008185\n",
      "\n",
      "Epoch 50/100\n",
      "Epoch 50, Batch 100/4948, Batch loss: 0.00017014617333188653\n",
      "Memory Usage: 899.80 MB\n",
      "Epoch 50, Batch 200/4948, Batch loss: 1.4576796274923254e-05\n",
      "Memory Usage: 899.80 MB\n",
      "Epoch 50, Batch 300/4948, Batch loss: 0.0003084895433858037\n",
      "Memory Usage: 899.80 MB\n",
      "Epoch 50, Batch 400/4948, Batch loss: 0.00011593164526857436\n",
      "Memory Usage: 899.80 MB\n",
      "Epoch 50, Batch 500/4948, Batch loss: 5.173289537196979e-05\n",
      "Memory Usage: 899.80 MB\n",
      "Epoch 50, Batch 600/4948, Batch loss: 0.00046749060857109725\n",
      "Memory Usage: 899.80 MB\n",
      "Epoch 50, Batch 700/4948, Batch loss: 5.30140932823997e-05\n",
      "Memory Usage: 899.80 MB\n",
      "Epoch 50, Batch 800/4948, Batch loss: 0.00012328510638326406\n",
      "Memory Usage: 900.80 MB\n",
      "Epoch 50, Batch 900/4948, Batch loss: 0.0005321062635630369\n",
      "Memory Usage: 900.80 MB\n",
      "Epoch 50, Batch 1000/4948, Batch loss: 8.600757428212091e-05\n",
      "Memory Usage: 900.80 MB\n",
      "Epoch 50, Batch 1100/4948, Batch loss: 0.000867033435497433\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 1200/4948, Batch loss: 0.0014155698008835316\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 1300/4948, Batch loss: 0.00015592016279697418\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 1400/4948, Batch loss: 0.00012563803466036916\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 1500/4948, Batch loss: 7.25176214473322e-05\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 1600/4948, Batch loss: 1.3306636901688762e-05\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 1700/4948, Batch loss: 0.0003814302908722311\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 1800/4948, Batch loss: 0.0002610799274407327\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 1900/4948, Batch loss: 0.003947788849473\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 2000/4948, Batch loss: 0.00042075125384144485\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 2100/4948, Batch loss: 0.0006929175578989089\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 2200/4948, Batch loss: 0.0001042129733832553\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 2300/4948, Batch loss: 0.00022083643125370145\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 2400/4948, Batch loss: 0.00010486680548638105\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 2500/4948, Batch loss: 4.791314131580293e-05\n",
      "Memory Usage: 900.81 MB\n",
      "Epoch 50, Batch 2600/4948, Batch loss: 0.0003827459295280278\n",
      "Memory Usage: 900.83 MB\n",
      "Epoch 50, Batch 2700/4948, Batch loss: 0.00036439491668716073\n",
      "Memory Usage: 900.83 MB\n",
      "Epoch 50, Batch 2800/4948, Batch loss: 0.0013944299425929785\n",
      "Memory Usage: 885.52 MB\n",
      "Epoch 50, Batch 2900/4948, Batch loss: 0.00020085906726308167\n",
      "Memory Usage: 892.08 MB\n",
      "Epoch 50, Batch 3000/4948, Batch loss: 5.9545604017330334e-05\n",
      "Memory Usage: 886.55 MB\n",
      "Epoch 50, Batch 3100/4948, Batch loss: 0.00012745683488901705\n",
      "Memory Usage: 884.98 MB\n",
      "Epoch 50, Batch 3200/4948, Batch loss: 0.0003082093026023358\n",
      "Memory Usage: 888.81 MB\n",
      "Epoch 50, Batch 3300/4948, Batch loss: 0.0001389989338349551\n",
      "Memory Usage: 890.05 MB\n",
      "Epoch 50, Batch 3400/4948, Batch loss: 6.6448569668864366e-06\n",
      "Memory Usage: 893.45 MB\n",
      "Epoch 50, Batch 3500/4948, Batch loss: 4.6842462325003e-05\n",
      "Memory Usage: 893.81 MB\n",
      "Epoch 50, Batch 3600/4948, Batch loss: 4.024199733976275e-05\n",
      "Memory Usage: 863.45 MB\n",
      "Epoch 50, Batch 3700/4948, Batch loss: 0.000560418760869652\n",
      "Memory Usage: 758.77 MB\n",
      "Epoch 50, Batch 3800/4948, Batch loss: 0.0001057358822436072\n",
      "Memory Usage: 758.70 MB\n",
      "Epoch 50, Batch 3900/4948, Batch loss: 0.00013589394802693278\n",
      "Memory Usage: 764.31 MB\n",
      "Epoch 50, Batch 4000/4948, Batch loss: 0.00014814927999395877\n",
      "Memory Usage: 765.69 MB\n",
      "Epoch 50, Batch 4100/4948, Batch loss: 5.077547029941343e-05\n",
      "Memory Usage: 769.73 MB\n",
      "Epoch 50, Batch 4200/4948, Batch loss: 0.0010037677129730582\n",
      "Memory Usage: 769.73 MB\n",
      "Epoch 50, Batch 4300/4948, Batch loss: 0.00014369317796081305\n",
      "Memory Usage: 771.12 MB\n",
      "Epoch 50, Batch 4400/4948, Batch loss: 4.0364579945162404e-06\n",
      "Memory Usage: 779.31 MB\n",
      "Epoch 50, Batch 4500/4948, Batch loss: 0.0009449535864405334\n",
      "Memory Usage: 791.31 MB\n",
      "Epoch 50, Batch 4600/4948, Batch loss: 0.0002395478804828599\n",
      "Memory Usage: 793.78 MB\n",
      "Epoch 50, Batch 4700/4948, Batch loss: 0.0004137523064855486\n",
      "Memory Usage: 793.78 MB\n",
      "Epoch 50, Batch 4800/4948, Batch loss: 0.00011033302871510386\n",
      "Memory Usage: 793.78 MB\n",
      "Epoch 50, Batch 4900/4948, Batch loss: 7.123334216885269e-05\n",
      "Memory Usage: 793.81 MB\n",
      "Epoch 50, Batch 4948/4948, Batch loss: 0.0009997113374993205\n",
      "Memory Usage: 793.81 MB\n",
      "Epoch 50 completed in 62.20 seconds, Total Training Loss: 0.00030130556175244515\n",
      "\n",
      "Epoch 51/100\n",
      "Epoch 51, Batch 100/4948, Batch loss: 0.0001713453239062801\n",
      "Memory Usage: 793.83 MB\n",
      "Epoch 51, Batch 200/4948, Batch loss: 1.820155193854589e-05\n",
      "Memory Usage: 795.89 MB\n",
      "Epoch 51, Batch 300/4948, Batch loss: 0.00030335693736560643\n",
      "Memory Usage: 795.89 MB\n",
      "Epoch 51, Batch 400/4948, Batch loss: 0.00011513243953231722\n",
      "Memory Usage: 795.92 MB\n",
      "Epoch 51, Batch 500/4948, Batch loss: 5.1193801482440904e-05\n",
      "Memory Usage: 795.92 MB\n",
      "Epoch 51, Batch 600/4948, Batch loss: 0.00046763676800765097\n",
      "Memory Usage: 795.94 MB\n",
      "Epoch 51, Batch 700/4948, Batch loss: 5.398040229920298e-05\n",
      "Memory Usage: 795.94 MB\n",
      "Epoch 51, Batch 800/4948, Batch loss: 0.00012370024342089891\n",
      "Memory Usage: 796.39 MB\n",
      "Epoch 51, Batch 900/4948, Batch loss: 0.0005149167263880372\n",
      "Memory Usage: 796.39 MB\n",
      "Epoch 51, Batch 1000/4948, Batch loss: 8.626678754808381e-05\n",
      "Memory Usage: 796.39 MB\n",
      "Epoch 51, Batch 1100/4948, Batch loss: 0.0009165497031062841\n",
      "Memory Usage: 796.39 MB\n",
      "Epoch 51, Batch 1200/4948, Batch loss: 0.0014179981080815196\n",
      "Memory Usage: 796.77 MB\n",
      "Epoch 51, Batch 1300/4948, Batch loss: 0.00015101292228791863\n",
      "Memory Usage: 796.77 MB\n",
      "Epoch 51, Batch 1400/4948, Batch loss: 0.00012309837620705366\n",
      "Memory Usage: 796.77 MB\n",
      "Epoch 51, Batch 1500/4948, Batch loss: 7.462238863809034e-05\n",
      "Memory Usage: 796.77 MB\n",
      "Epoch 51, Batch 1600/4948, Batch loss: 1.734976831357926e-05\n",
      "Memory Usage: 796.77 MB\n",
      "Epoch 51, Batch 1700/4948, Batch loss: 0.0004094727337360382\n",
      "Memory Usage: 796.77 MB\n",
      "Epoch 51, Batch 1800/4948, Batch loss: 0.0002618921862449497\n",
      "Memory Usage: 796.77 MB\n",
      "Epoch 51, Batch 1900/4948, Batch loss: 0.004016484599560499\n",
      "Memory Usage: 796.77 MB\n",
      "Epoch 51, Batch 2000/4948, Batch loss: 0.0004094038740731776\n",
      "Memory Usage: 796.77 MB\n",
      "Epoch 51, Batch 2100/4948, Batch loss: 0.0006803885335102677\n",
      "Memory Usage: 796.78 MB\n",
      "Epoch 51, Batch 2200/4948, Batch loss: 0.00010513842426007614\n",
      "Memory Usage: 796.78 MB\n",
      "Epoch 51, Batch 2300/4948, Batch loss: 0.00022043236822355539\n",
      "Memory Usage: 796.80 MB\n",
      "Epoch 51, Batch 2400/4948, Batch loss: 0.00010463954822625965\n",
      "Memory Usage: 796.80 MB\n",
      "Epoch 51, Batch 2500/4948, Batch loss: 4.869218173553236e-05\n",
      "Memory Usage: 796.80 MB\n",
      "Epoch 51, Batch 2600/4948, Batch loss: 0.0003886806371156126\n",
      "Memory Usage: 796.80 MB\n",
      "Epoch 51, Batch 2700/4948, Batch loss: 0.0003792211937252432\n",
      "Memory Usage: 796.81 MB\n",
      "Epoch 51, Batch 2800/4948, Batch loss: 0.0013856773730367422\n",
      "Memory Usage: 796.84 MB\n",
      "Epoch 51, Batch 2900/4948, Batch loss: 0.00019861181499436498\n",
      "Memory Usage: 796.84 MB\n",
      "Epoch 51, Batch 3000/4948, Batch loss: 5.7613247918197885e-05\n",
      "Memory Usage: 796.84 MB\n",
      "Epoch 51, Batch 3100/4948, Batch loss: 0.0001229963090736419\n",
      "Memory Usage: 796.84 MB\n",
      "Epoch 51, Batch 3200/4948, Batch loss: 0.00032143297721631825\n",
      "Memory Usage: 796.84 MB\n",
      "Epoch 51, Batch 3300/4948, Batch loss: 0.00013962844968773425\n",
      "Memory Usage: 797.62 MB\n",
      "Epoch 51, Batch 3400/4948, Batch loss: 6.228623988135951e-06\n",
      "Memory Usage: 797.62 MB\n",
      "Epoch 51, Batch 3500/4948, Batch loss: 4.564821210806258e-05\n",
      "Memory Usage: 797.62 MB\n",
      "Epoch 51, Batch 3600/4948, Batch loss: 4.197002635919489e-05\n",
      "Memory Usage: 797.62 MB\n",
      "Epoch 51, Batch 3700/4948, Batch loss: 0.0005472565535455942\n",
      "Memory Usage: 797.62 MB\n",
      "Epoch 51, Batch 3800/4948, Batch loss: 0.00010780053708003834\n",
      "Memory Usage: 797.64 MB\n",
      "Epoch 51, Batch 3900/4948, Batch loss: 0.00014111140626482666\n",
      "Memory Usage: 797.64 MB\n",
      "Epoch 51, Batch 4000/4948, Batch loss: 0.00014670167001895607\n",
      "Memory Usage: 797.64 MB\n",
      "Epoch 51, Batch 4100/4948, Batch loss: 4.801032628165558e-05\n",
      "Memory Usage: 797.64 MB\n",
      "Epoch 51, Batch 4200/4948, Batch loss: 0.0009935741545632482\n",
      "Memory Usage: 797.64 MB\n",
      "Epoch 51, Batch 4300/4948, Batch loss: 0.0001450421696063131\n",
      "Memory Usage: 797.64 MB\n",
      "Epoch 51, Batch 4400/4948, Batch loss: 3.3876244742714334e-06\n",
      "Memory Usage: 797.64 MB\n",
      "Epoch 51, Batch 4500/4948, Batch loss: 0.0009553560521453619\n",
      "Memory Usage: 797.64 MB\n",
      "Epoch 51, Batch 4600/4948, Batch loss: 0.0002346747787669301\n",
      "Memory Usage: 797.64 MB\n",
      "Epoch 51, Batch 4700/4948, Batch loss: 0.0004140908713452518\n",
      "Memory Usage: 797.64 MB\n",
      "Epoch 51, Batch 4800/4948, Batch loss: 0.00011213302059331909\n",
      "Memory Usage: 797.64 MB\n",
      "Epoch 51, Batch 4900/4948, Batch loss: 7.368265505647287e-05\n",
      "Memory Usage: 797.66 MB\n",
      "Epoch 51, Batch 4948/4948, Batch loss: 0.0009947887156158686\n",
      "Memory Usage: 797.66 MB\n",
      "Epoch 51 completed in 61.92 seconds, Total Training Loss: 0.0003026679031594405\n",
      "Validation completed in 3.47 seconds, Average Validation Loss: 0.0005016637187070186\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 52/100\n",
      "Epoch 52, Batch 100/4948, Batch loss: 0.00016955917817540467\n",
      "Memory Usage: 935.86 MB\n",
      "Epoch 52, Batch 200/4948, Batch loss: 1.539185359433759e-05\n",
      "Memory Usage: 935.94 MB\n",
      "Epoch 52, Batch 300/4948, Batch loss: 0.0003065891796723008\n",
      "Memory Usage: 935.94 MB\n",
      "Epoch 52, Batch 400/4948, Batch loss: 0.00011494196951389313\n",
      "Memory Usage: 935.94 MB\n",
      "Epoch 52, Batch 500/4948, Batch loss: 5.138530468684621e-05\n",
      "Memory Usage: 935.94 MB\n",
      "Epoch 52, Batch 600/4948, Batch loss: 0.00047821440966799855\n",
      "Memory Usage: 935.94 MB\n",
      "Epoch 52, Batch 700/4948, Batch loss: 5.4940512200118974e-05\n",
      "Memory Usage: 935.94 MB\n",
      "Epoch 52, Batch 800/4948, Batch loss: 0.0001278551935683936\n",
      "Memory Usage: 935.94 MB\n",
      "Epoch 52, Batch 900/4948, Batch loss: 0.0005304095684550703\n",
      "Memory Usage: 935.94 MB\n",
      "Epoch 52, Batch 1000/4948, Batch loss: 8.725463703740388e-05\n",
      "Memory Usage: 936.25 MB\n",
      "Epoch 52, Batch 1100/4948, Batch loss: 0.0008698405581526458\n",
      "Memory Usage: 936.75 MB\n",
      "Epoch 52, Batch 1200/4948, Batch loss: 0.0013954034075140953\n",
      "Memory Usage: 936.75 MB\n",
      "Epoch 52, Batch 1300/4948, Batch loss: 0.0001522615784779191\n",
      "Memory Usage: 936.75 MB\n",
      "Epoch 52, Batch 1400/4948, Batch loss: 0.00012401219282764941\n",
      "Memory Usage: 936.75 MB\n",
      "Epoch 52, Batch 1500/4948, Batch loss: 7.400132017210126e-05\n",
      "Memory Usage: 936.75 MB\n",
      "Epoch 52, Batch 1600/4948, Batch loss: 1.3526183465728536e-05\n",
      "Memory Usage: 936.75 MB\n",
      "Epoch 52, Batch 1700/4948, Batch loss: 0.0004009540716651827\n",
      "Memory Usage: 936.75 MB\n",
      "Epoch 52, Batch 1800/4948, Batch loss: 0.00026175190578214824\n",
      "Memory Usage: 936.75 MB\n",
      "Epoch 52, Batch 1900/4948, Batch loss: 0.003591932589188218\n",
      "Memory Usage: 937.25 MB\n",
      "Epoch 52, Batch 2000/4948, Batch loss: 0.00040541382622905076\n",
      "Memory Usage: 935.98 MB\n",
      "Epoch 52, Batch 2100/4948, Batch loss: 0.0006692360620945692\n",
      "Memory Usage: 936.28 MB\n",
      "Epoch 52, Batch 2200/4948, Batch loss: 0.00010448728426126763\n",
      "Memory Usage: 936.41 MB\n",
      "Epoch 52, Batch 2300/4948, Batch loss: 0.0002171733940485865\n",
      "Memory Usage: 936.41 MB\n",
      "Epoch 52, Batch 2400/4948, Batch loss: 0.00010627255687722936\n",
      "Memory Usage: 936.41 MB\n",
      "Epoch 52, Batch 2500/4948, Batch loss: 4.9727292207535356e-05\n",
      "Memory Usage: 936.94 MB\n",
      "Epoch 52, Batch 2600/4948, Batch loss: 0.00037963679642416537\n",
      "Memory Usage: 937.05 MB\n",
      "Epoch 52, Batch 2700/4948, Batch loss: 0.00037717248778790236\n",
      "Memory Usage: 937.05 MB\n",
      "Epoch 52, Batch 2800/4948, Batch loss: 0.0013730842620134354\n",
      "Memory Usage: 937.05 MB\n",
      "Epoch 52, Batch 2900/4948, Batch loss: 0.00019920113845728338\n",
      "Memory Usage: 937.08 MB\n",
      "Epoch 52, Batch 3000/4948, Batch loss: 5.792481897515245e-05\n",
      "Memory Usage: 937.48 MB\n",
      "Epoch 52, Batch 3100/4948, Batch loss: 0.00012416123354341835\n",
      "Memory Usage: 937.48 MB\n",
      "Epoch 52, Batch 3200/4948, Batch loss: 0.0003047515929210931\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 3300/4948, Batch loss: 0.000138645846163854\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 3400/4948, Batch loss: 8.246077413787134e-06\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 3500/4948, Batch loss: 5.0152888434240595e-05\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 3600/4948, Batch loss: 4.068965426995419e-05\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 3700/4948, Batch loss: 0.0005561018479056656\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 3800/4948, Batch loss: 0.0001068024430423975\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 3900/4948, Batch loss: 0.00015300842642318457\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 4000/4948, Batch loss: 0.00014749099500477314\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 4100/4948, Batch loss: 4.475325476960279e-05\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 4200/4948, Batch loss: 0.0009809103794395924\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 4300/4948, Batch loss: 0.0001449691189918667\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 4400/4948, Batch loss: 3.3895153137564193e-06\n",
      "Memory Usage: 937.62 MB\n",
      "Epoch 52, Batch 4500/4948, Batch loss: 0.0009427139884792268\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 52, Batch 4600/4948, Batch loss: 0.0002423815749352798\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 52, Batch 4700/4948, Batch loss: 0.0004125314299017191\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 52, Batch 4800/4948, Batch loss: 0.00011269968672422692\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 52, Batch 4900/4948, Batch loss: 0.00010873557039303705\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 52, Batch 4948/4948, Batch loss: 0.0009983014315366745\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 52 completed in 63.05 seconds, Total Training Loss: 0.00030089915709756904\n",
      "\n",
      "Epoch 53/100\n",
      "Epoch 53, Batch 100/4948, Batch loss: 0.00017009118164423853\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 200/4948, Batch loss: 1.2671822332777083e-05\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 300/4948, Batch loss: 0.000304576416965574\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 400/4948, Batch loss: 0.00011806330439867452\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 500/4948, Batch loss: 5.119604611536488e-05\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 600/4948, Batch loss: 0.00046997316530905664\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 700/4948, Batch loss: 5.186683119973168e-05\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 800/4948, Batch loss: 0.00014365039533004165\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 900/4948, Batch loss: 0.0005343479570001364\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 1000/4948, Batch loss: 8.66635818965733e-05\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 1100/4948, Batch loss: 0.0008502677665092051\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 1200/4948, Batch loss: 0.0013979739742353559\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 1300/4948, Batch loss: 0.00015354403876699507\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 1400/4948, Batch loss: 0.00012387349852360785\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 1500/4948, Batch loss: 7.295849354704842e-05\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 1600/4948, Batch loss: 1.401995359628927e-05\n",
      "Memory Usage: 937.64 MB\n",
      "Epoch 53, Batch 1700/4948, Batch loss: 0.00029659157735295594\n",
      "Memory Usage: 937.67 MB\n",
      "Epoch 53, Batch 1800/4948, Batch loss: 0.00026375005836598575\n",
      "Memory Usage: 937.67 MB\n",
      "Epoch 53, Batch 1900/4948, Batch loss: 0.0009347558370791376\n",
      "Memory Usage: 937.67 MB\n",
      "Epoch 53, Batch 2000/4948, Batch loss: 0.00044982435065321624\n",
      "Memory Usage: 937.67 MB\n",
      "Epoch 53, Batch 2100/4948, Batch loss: 0.0007379809976555407\n",
      "Memory Usage: 937.67 MB\n",
      "Epoch 53, Batch 2200/4948, Batch loss: 0.00010654026846168563\n",
      "Memory Usage: 937.67 MB\n",
      "Epoch 53, Batch 2300/4948, Batch loss: 0.0002264896029373631\n",
      "Memory Usage: 937.67 MB\n",
      "Epoch 53, Batch 2400/4948, Batch loss: 0.000106125145975966\n",
      "Memory Usage: 937.67 MB\n",
      "Epoch 53, Batch 2500/4948, Batch loss: 4.831579281017184e-05\n",
      "Memory Usage: 937.67 MB\n",
      "Epoch 53, Batch 2600/4948, Batch loss: 0.00038483174284920096\n",
      "Memory Usage: 937.67 MB\n",
      "Epoch 53, Batch 2700/4948, Batch loss: 0.0003971999103669077\n",
      "Memory Usage: 937.67 MB\n",
      "Epoch 53, Batch 2800/4948, Batch loss: 0.0013645002618432045\n",
      "Memory Usage: 937.67 MB\n",
      "Epoch 53, Batch 2900/4948, Batch loss: 0.00020003617100883275\n",
      "Memory Usage: 934.52 MB\n",
      "Epoch 53, Batch 3000/4948, Batch loss: 5.840226367581636e-05\n",
      "Memory Usage: 891.31 MB\n",
      "Epoch 53, Batch 3100/4948, Batch loss: 0.00012377541861496866\n",
      "Memory Usage: 896.00 MB\n",
      "Epoch 53, Batch 3200/4948, Batch loss: 0.0002960102865472436\n",
      "Memory Usage: 903.34 MB\n",
      "Epoch 53, Batch 3300/4948, Batch loss: 0.00014024446136318147\n",
      "Memory Usage: 904.11 MB\n",
      "Epoch 53, Batch 3400/4948, Batch loss: 4.940335202263668e-06\n",
      "Memory Usage: 905.14 MB\n",
      "Epoch 53, Batch 3500/4948, Batch loss: 4.697098120232113e-05\n",
      "Memory Usage: 905.55 MB\n",
      "Epoch 53, Batch 3600/4948, Batch loss: 4.1025050450116396e-05\n",
      "Memory Usage: 906.45 MB\n",
      "Epoch 53, Batch 3700/4948, Batch loss: 0.0005660781753249466\n",
      "Memory Usage: 906.80 MB\n",
      "Epoch 53, Batch 3800/4948, Batch loss: 0.00010581706010270864\n",
      "Memory Usage: 906.81 MB\n",
      "Epoch 53, Batch 3900/4948, Batch loss: 0.00015035906108096242\n",
      "Memory Usage: 906.81 MB\n",
      "Epoch 53, Batch 4000/4948, Batch loss: 0.00014816340990364552\n",
      "Memory Usage: 907.31 MB\n",
      "Epoch 53, Batch 4100/4948, Batch loss: 4.565650306176394e-05\n",
      "Memory Usage: 907.31 MB\n",
      "Epoch 53, Batch 4200/4948, Batch loss: 0.0009859711863100529\n",
      "Memory Usage: 907.98 MB\n",
      "Epoch 53, Batch 4300/4948, Batch loss: 0.0001446799433324486\n",
      "Memory Usage: 909.06 MB\n",
      "Epoch 53, Batch 4400/4948, Batch loss: 3.384373030712595e-06\n",
      "Memory Usage: 909.62 MB\n",
      "Epoch 53, Batch 4500/4948, Batch loss: 0.0009572929120622575\n",
      "Memory Usage: 909.62 MB\n",
      "Epoch 53, Batch 4600/4948, Batch loss: 0.00023676696582697332\n",
      "Memory Usage: 909.62 MB\n",
      "Epoch 53, Batch 4700/4948, Batch loss: 0.00041869960841722786\n",
      "Memory Usage: 910.19 MB\n",
      "Epoch 53, Batch 4800/4948, Batch loss: 0.00012015677930321544\n",
      "Memory Usage: 910.19 MB\n",
      "Epoch 53, Batch 4900/4948, Batch loss: 7.024407386779785e-05\n",
      "Memory Usage: 910.19 MB\n",
      "Epoch 53, Batch 4948/4948, Batch loss: 0.0009880780708044767\n",
      "Memory Usage: 910.19 MB\n",
      "Epoch 53 completed in 64.16 seconds, Total Training Loss: 0.0002903452090709173\n",
      "\n",
      "Epoch 54/100\n",
      "Epoch 54, Batch 100/4948, Batch loss: 0.00017061817925423384\n",
      "Memory Usage: 910.19 MB\n",
      "Epoch 54, Batch 200/4948, Batch loss: 1.833787973737344e-05\n",
      "Memory Usage: 910.19 MB\n",
      "Epoch 54, Batch 300/4948, Batch loss: 0.0003072366234846413\n",
      "Memory Usage: 913.14 MB\n",
      "Epoch 54, Batch 400/4948, Batch loss: 0.00011512856144690886\n",
      "Memory Usage: 913.14 MB\n",
      "Epoch 54, Batch 500/4948, Batch loss: 5.10386707901489e-05\n",
      "Memory Usage: 913.14 MB\n",
      "Epoch 54, Batch 600/4948, Batch loss: 0.0004686423053499311\n",
      "Memory Usage: 913.14 MB\n",
      "Epoch 54, Batch 700/4948, Batch loss: 5.5986125516938046e-05\n",
      "Memory Usage: 913.67 MB\n",
      "Epoch 54, Batch 800/4948, Batch loss: 0.0001265419414266944\n",
      "Memory Usage: 913.67 MB\n",
      "Epoch 54, Batch 900/4948, Batch loss: 0.0005255312426015735\n",
      "Memory Usage: 913.72 MB\n",
      "Epoch 54, Batch 1000/4948, Batch loss: 8.60380387166515e-05\n",
      "Memory Usage: 915.00 MB\n",
      "Epoch 54, Batch 1100/4948, Batch loss: 0.0008771857828833163\n",
      "Memory Usage: 915.00 MB\n",
      "Epoch 54, Batch 1200/4948, Batch loss: 0.0013821902684867382\n",
      "Memory Usage: 900.80 MB\n",
      "Epoch 54, Batch 1300/4948, Batch loss: 0.00015430987696163356\n",
      "Memory Usage: 903.09 MB\n",
      "Epoch 54, Batch 1400/4948, Batch loss: 0.0001252099173143506\n",
      "Memory Usage: 905.98 MB\n",
      "Epoch 54, Batch 1500/4948, Batch loss: 7.272787479450926e-05\n",
      "Memory Usage: 903.64 MB\n",
      "Epoch 54, Batch 1600/4948, Batch loss: 1.3179935194784775e-05\n",
      "Memory Usage: 903.64 MB\n",
      "Epoch 54, Batch 1700/4948, Batch loss: 0.00052610132843256\n",
      "Memory Usage: 903.64 MB\n",
      "Epoch 54, Batch 1800/4948, Batch loss: 0.000261326931649819\n",
      "Memory Usage: 903.67 MB\n",
      "Epoch 54, Batch 1900/4948, Batch loss: 0.003748314455151558\n",
      "Memory Usage: 903.67 MB\n",
      "Epoch 54, Batch 2000/4948, Batch loss: 0.00042887701420113444\n",
      "Memory Usage: 898.72 MB\n",
      "Epoch 54, Batch 2100/4948, Batch loss: 0.0006925849593244493\n",
      "Memory Usage: 897.39 MB\n",
      "Epoch 54, Batch 2200/4948, Batch loss: 0.00010505607497179881\n",
      "Memory Usage: 897.41 MB\n",
      "Epoch 54, Batch 2300/4948, Batch loss: 0.0002248169475933537\n",
      "Memory Usage: 897.61 MB\n",
      "Epoch 54, Batch 2400/4948, Batch loss: 0.00010383213520981371\n",
      "Memory Usage: 898.11 MB\n",
      "Epoch 54, Batch 2500/4948, Batch loss: 4.8980080464389175e-05\n",
      "Memory Usage: 898.11 MB\n",
      "Epoch 54, Batch 2600/4948, Batch loss: 0.00038180628325790167\n",
      "Memory Usage: 898.61 MB\n",
      "Epoch 54, Batch 2700/4948, Batch loss: 0.0003816850367002189\n",
      "Memory Usage: 898.62 MB\n",
      "Epoch 54, Batch 2800/4948, Batch loss: 0.0013567800633609295\n",
      "Memory Usage: 898.70 MB\n",
      "Epoch 54, Batch 2900/4948, Batch loss: 0.00019909554976038635\n",
      "Memory Usage: 899.25 MB\n",
      "Epoch 54, Batch 3000/4948, Batch loss: 5.849478839081712e-05\n",
      "Memory Usage: 899.25 MB\n",
      "Epoch 54, Batch 3100/4948, Batch loss: 0.0001225488813361153\n",
      "Memory Usage: 899.25 MB\n",
      "Epoch 54, Batch 3200/4948, Batch loss: 0.0003176536120008677\n",
      "Memory Usage: 899.38 MB\n",
      "Epoch 54, Batch 3300/4948, Batch loss: 0.00013809825759381056\n",
      "Memory Usage: 899.41 MB\n",
      "Epoch 54, Batch 3400/4948, Batch loss: 5.237237928668037e-06\n",
      "Memory Usage: 899.41 MB\n",
      "Epoch 54, Batch 3500/4948, Batch loss: 6.427810876630247e-05\n",
      "Memory Usage: 899.41 MB\n",
      "Epoch 54, Batch 3600/4948, Batch loss: 4.126137719140388e-05\n",
      "Memory Usage: 899.91 MB\n",
      "Epoch 54, Batch 3700/4948, Batch loss: 0.000553885183762759\n",
      "Memory Usage: 899.91 MB\n",
      "Epoch 54, Batch 3800/4948, Batch loss: 0.0001060640934156254\n",
      "Memory Usage: 899.91 MB\n",
      "Epoch 54, Batch 3900/4948, Batch loss: 0.00013928170665167272\n",
      "Memory Usage: 899.91 MB\n",
      "Epoch 54, Batch 4000/4948, Batch loss: 0.0001478909543948248\n",
      "Memory Usage: 899.91 MB\n",
      "Epoch 54, Batch 4100/4948, Batch loss: 4.7937854105839506e-05\n",
      "Memory Usage: 899.91 MB\n",
      "Epoch 54, Batch 4200/4948, Batch loss: 0.0009875298710539937\n",
      "Memory Usage: 899.91 MB\n",
      "Epoch 54, Batch 4300/4948, Batch loss: 0.00014539322000928223\n",
      "Memory Usage: 899.91 MB\n",
      "Epoch 54, Batch 4400/4948, Batch loss: 3.4694130590651184e-06\n",
      "Memory Usage: 899.91 MB\n",
      "Epoch 54, Batch 4500/4948, Batch loss: 0.0009253206080757082\n",
      "Memory Usage: 899.91 MB\n",
      "Epoch 54, Batch 4600/4948, Batch loss: 0.00023730608518235385\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 54, Batch 4700/4948, Batch loss: 0.00041149515891447663\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 54, Batch 4800/4948, Batch loss: 0.00011272211850155145\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 54, Batch 4900/4948, Batch loss: 8.008388977032155e-05\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 54, Batch 4948/4948, Batch loss: 0.0009823597501963377\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 54 completed in 62.87 seconds, Total Training Loss: 0.00030222845554011966\n",
      "\n",
      "Epoch 55/100\n",
      "Epoch 55, Batch 100/4948, Batch loss: 0.0001705117610981688\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 55, Batch 200/4948, Batch loss: 1.2119767234253231e-05\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 55, Batch 300/4948, Batch loss: 0.0003081027534790337\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 55, Batch 400/4948, Batch loss: 0.00011997020919807255\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 55, Batch 500/4948, Batch loss: 5.092500941827893e-05\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 55, Batch 600/4948, Batch loss: 0.00047044389066286385\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 55, Batch 700/4948, Batch loss: 5.468698509503156e-05\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 55, Batch 800/4948, Batch loss: 0.00012395282101351768\n",
      "Memory Usage: 900.34 MB\n",
      "Epoch 55, Batch 900/4948, Batch loss: 0.0005216566496528685\n",
      "Memory Usage: 900.36 MB\n",
      "Epoch 55, Batch 1000/4948, Batch loss: 8.520356641383842e-05\n",
      "Memory Usage: 900.36 MB\n",
      "Epoch 55, Batch 1100/4948, Batch loss: 0.0008574869716539979\n",
      "Memory Usage: 900.36 MB\n",
      "Epoch 55, Batch 1200/4948, Batch loss: 0.0014064946444705129\n",
      "Memory Usage: 900.36 MB\n",
      "Epoch 55, Batch 1300/4948, Batch loss: 0.0001513233146397397\n",
      "Memory Usage: 900.36 MB\n",
      "Epoch 55, Batch 1400/4948, Batch loss: 0.0001237595424754545\n",
      "Memory Usage: 900.36 MB\n",
      "Epoch 55, Batch 1500/4948, Batch loss: 7.423616625601426e-05\n",
      "Memory Usage: 900.36 MB\n",
      "Epoch 55, Batch 1600/4948, Batch loss: 1.329658607573947e-05\n",
      "Memory Usage: 900.86 MB\n",
      "Epoch 55, Batch 1700/4948, Batch loss: 0.00041871811845339835\n",
      "Memory Usage: 900.88 MB\n",
      "Epoch 55, Batch 1800/4948, Batch loss: 0.00026110396720469\n",
      "Memory Usage: 900.88 MB\n",
      "Epoch 55, Batch 1900/4948, Batch loss: 0.004578058607876301\n",
      "Memory Usage: 900.88 MB\n",
      "Epoch 55, Batch 2000/4948, Batch loss: 0.00041846538078971207\n",
      "Memory Usage: 900.88 MB\n",
      "Epoch 55, Batch 2100/4948, Batch loss: 0.0007017492898739874\n",
      "Memory Usage: 900.88 MB\n",
      "Epoch 55, Batch 2200/4948, Batch loss: 0.00010581569222267717\n",
      "Memory Usage: 900.88 MB\n",
      "Epoch 55, Batch 2300/4948, Batch loss: 0.00021970162924844772\n",
      "Memory Usage: 900.92 MB\n",
      "Epoch 55, Batch 2400/4948, Batch loss: 0.0001066417753463611\n",
      "Memory Usage: 900.92 MB\n",
      "Epoch 55, Batch 2500/4948, Batch loss: 4.8599242290947586e-05\n",
      "Memory Usage: 900.92 MB\n",
      "Epoch 55, Batch 2600/4948, Batch loss: 0.0003807607281487435\n",
      "Memory Usage: 900.94 MB\n",
      "Epoch 55, Batch 2700/4948, Batch loss: 0.0003628803533501923\n",
      "Memory Usage: 900.94 MB\n",
      "Epoch 55, Batch 2800/4948, Batch loss: 0.0014049620367586613\n",
      "Memory Usage: 900.94 MB\n",
      "Epoch 55, Batch 2900/4948, Batch loss: 0.00019777024863287807\n",
      "Memory Usage: 900.94 MB\n",
      "Epoch 55, Batch 3000/4948, Batch loss: 5.778699051006697e-05\n",
      "Memory Usage: 900.94 MB\n",
      "Epoch 55, Batch 3100/4948, Batch loss: 0.0001237864198628813\n",
      "Memory Usage: 900.94 MB\n",
      "Epoch 55, Batch 3200/4948, Batch loss: 0.0003147663956042379\n",
      "Memory Usage: 900.95 MB\n",
      "Epoch 55, Batch 3300/4948, Batch loss: 0.00013737441622652113\n",
      "Memory Usage: 900.95 MB\n",
      "Epoch 55, Batch 3400/4948, Batch loss: 1.2036955922667403e-05\n",
      "Memory Usage: 900.97 MB\n",
      "Epoch 55, Batch 3500/4948, Batch loss: 4.784743578056805e-05\n",
      "Memory Usage: 900.97 MB\n",
      "Epoch 55, Batch 3600/4948, Batch loss: 4.3131905840709805e-05\n",
      "Memory Usage: 901.00 MB\n",
      "Epoch 55, Batch 3700/4948, Batch loss: 0.0005627375212498009\n",
      "Memory Usage: 901.00 MB\n",
      "Epoch 55, Batch 3800/4948, Batch loss: 0.00010741384903667495\n",
      "Memory Usage: 901.00 MB\n",
      "Epoch 55, Batch 3900/4948, Batch loss: 0.00013446065713651478\n",
      "Memory Usage: 901.00 MB\n",
      "Epoch 55, Batch 4000/4948, Batch loss: 0.00014694768469780684\n",
      "Memory Usage: 901.00 MB\n",
      "Epoch 55, Batch 4100/4948, Batch loss: 4.744971374748275e-05\n",
      "Memory Usage: 901.00 MB\n",
      "Epoch 55, Batch 4200/4948, Batch loss: 0.000991437933407724\n",
      "Memory Usage: 901.00 MB\n",
      "Epoch 55, Batch 4300/4948, Batch loss: 0.00014598979032598436\n",
      "Memory Usage: 901.00 MB\n",
      "Epoch 55, Batch 4400/4948, Batch loss: 3.910126906703226e-06\n",
      "Memory Usage: 901.00 MB\n",
      "Epoch 55, Batch 4500/4948, Batch loss: 0.0009250725852325559\n",
      "Memory Usage: 901.02 MB\n",
      "Epoch 55, Batch 4600/4948, Batch loss: 0.00023641322331968695\n",
      "Memory Usage: 901.02 MB\n",
      "Epoch 55, Batch 4700/4948, Batch loss: 0.00041407416574656963\n",
      "Memory Usage: 901.02 MB\n",
      "Epoch 55, Batch 4800/4948, Batch loss: 0.00011367929255357012\n",
      "Memory Usage: 901.02 MB\n",
      "Epoch 55, Batch 4900/4948, Batch loss: 6.976064469199628e-05\n",
      "Memory Usage: 901.02 MB\n",
      "Epoch 55, Batch 4948/4948, Batch loss: 0.000986790400929749\n",
      "Memory Usage: 901.02 MB\n",
      "Epoch 55 completed in 60.86 seconds, Total Training Loss: 0.00030011282079566505\n",
      "\n",
      "Epoch 56/100\n",
      "Epoch 56, Batch 100/4948, Batch loss: 0.00017202379240188748\n",
      "Memory Usage: 901.02 MB\n",
      "Epoch 56, Batch 200/4948, Batch loss: 1.8317166905035265e-05\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 56, Batch 300/4948, Batch loss: 0.00030608277302235365\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 56, Batch 400/4948, Batch loss: 0.0001142024775617756\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 56, Batch 500/4948, Batch loss: 5.11916441610083e-05\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 56, Batch 600/4948, Batch loss: 0.0004696196992881596\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 56, Batch 700/4948, Batch loss: 5.3900235798209906e-05\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 56, Batch 800/4948, Batch loss: 0.00012390837946441025\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 56, Batch 900/4948, Batch loss: 0.0005217514117248356\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 56, Batch 1000/4948, Batch loss: 8.806894038571045e-05\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 56, Batch 1100/4948, Batch loss: 0.0008584840106777847\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 56, Batch 1200/4948, Batch loss: 0.0013834423152729869\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 1300/4948, Batch loss: 0.0001549143489683047\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 1400/4948, Batch loss: 0.0001226222375407815\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 1500/4948, Batch loss: 7.94138468336314e-05\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 1600/4948, Batch loss: 1.3270642739371397e-05\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 1700/4948, Batch loss: 0.00048129510832950473\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 1800/4948, Batch loss: 0.0002597013663034886\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 1900/4948, Batch loss: 0.003998329397290945\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 2000/4948, Batch loss: 0.0004021349304821342\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 2100/4948, Batch loss: 0.000666843552608043\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 2200/4948, Batch loss: 0.00010682335414458066\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 2300/4948, Batch loss: 0.00022137013729661703\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 2400/4948, Batch loss: 0.00010498243500478566\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 2500/4948, Batch loss: 4.828161036130041e-05\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 2600/4948, Batch loss: 0.00038332707481458783\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 2700/4948, Batch loss: 0.0003778711543418467\n",
      "Memory Usage: 901.05 MB\n",
      "Epoch 56, Batch 2800/4948, Batch loss: 0.0013945039827376604\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 2900/4948, Batch loss: 0.00020007931743748486\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 3000/4948, Batch loss: 5.862439502379857e-05\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 3100/4948, Batch loss: 0.00012577605957631022\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 3200/4948, Batch loss: 0.00031628378201276064\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 3300/4948, Batch loss: 0.00013962741650175303\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 3400/4948, Batch loss: 1.1208554496988654e-05\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 3500/4948, Batch loss: 4.5550947106676176e-05\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 3600/4948, Batch loss: 4.228760371915996e-05\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 3700/4948, Batch loss: 0.000553738500457257\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 3800/4948, Batch loss: 0.00010618673695717007\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 3900/4948, Batch loss: 0.0001447715621907264\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 4000/4948, Batch loss: 0.00014891904720570892\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 4100/4948, Batch loss: 4.518741479841992e-05\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 4200/4948, Batch loss: 0.0009857675759121776\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 4300/4948, Batch loss: 0.0001446952810510993\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 4400/4948, Batch loss: 3.7553477341134567e-06\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 4500/4948, Batch loss: 0.0009287263383157551\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 4600/4948, Batch loss: 0.00023639459686819464\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 4700/4948, Batch loss: 0.00042007266893051565\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 4800/4948, Batch loss: 0.00011772756261052564\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 4900/4948, Batch loss: 8.798052294878289e-05\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56, Batch 4948/4948, Batch loss: 0.0009981885086745024\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 56 completed in 61.72 seconds, Total Training Loss: 0.0003003515016772984\n",
      "\n",
      "Epoch 57/100\n",
      "Epoch 57, Batch 100/4948, Batch loss: 0.00016983624664135277\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 200/4948, Batch loss: 1.0543803000473417e-05\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 300/4948, Batch loss: 0.0003086194337811321\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 400/4948, Batch loss: 0.00011733491555787623\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 500/4948, Batch loss: 5.144285023561679e-05\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 600/4948, Batch loss: 0.0004695081152021885\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 700/4948, Batch loss: 5.542101644095965e-05\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 800/4948, Batch loss: 0.00012670551950577646\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 900/4948, Batch loss: 0.0005275739240460098\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 1000/4948, Batch loss: 8.835102926241234e-05\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 1100/4948, Batch loss: 0.0008664620690979064\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 1200/4948, Batch loss: 0.001372161554172635\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 1300/4948, Batch loss: 0.0001532531314296648\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 1400/4948, Batch loss: 0.00012318410153966397\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 1500/4948, Batch loss: 7.400468166451901e-05\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 1600/4948, Batch loss: 1.32621289594681e-05\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 1700/4948, Batch loss: 0.00045741553185507655\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 1800/4948, Batch loss: 0.00026015620096586645\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 1900/4948, Batch loss: 0.0033702163491398096\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 2000/4948, Batch loss: 0.00041465755202807486\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 2100/4948, Batch loss: 0.0006825393647886813\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 2200/4948, Batch loss: 0.00010609939636196941\n",
      "Memory Usage: 901.06 MB\n",
      "Epoch 57, Batch 2300/4948, Batch loss: 0.0002214274718426168\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 2400/4948, Batch loss: 0.000105659244582057\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 2500/4948, Batch loss: 4.843255737796426e-05\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 2600/4948, Batch loss: 0.0003829489869531244\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 2700/4948, Batch loss: 0.0003641348157543689\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 2800/4948, Batch loss: 0.0013875528238713741\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 2900/4948, Batch loss: 0.00019874531426467001\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 3000/4948, Batch loss: 5.763993976870552e-05\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 3100/4948, Batch loss: 0.00012415031960699707\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 3200/4948, Batch loss: 0.0003183402877766639\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 3300/4948, Batch loss: 0.00013858893362339586\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 3400/4948, Batch loss: 8.415917363890912e-06\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 3500/4948, Batch loss: 5.093597428640351e-05\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 3600/4948, Batch loss: 4.09081949328538e-05\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 3700/4948, Batch loss: 0.0005523076397366822\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 3800/4948, Batch loss: 0.00010582071263343096\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 3900/4948, Batch loss: 0.00017067221051547676\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 4000/4948, Batch loss: 0.00014834781177341938\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 4100/4948, Batch loss: 4.608551898854785e-05\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 4200/4948, Batch loss: 0.0009782843990251422\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 4300/4948, Batch loss: 0.00014606391778215766\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 4400/4948, Batch loss: 3.6306776109995553e-06\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 4500/4948, Batch loss: 0.0009242732776328921\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 4600/4948, Batch loss: 0.0002417143841739744\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 4700/4948, Batch loss: 0.0004117726057302207\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 4800/4948, Batch loss: 0.00011646190250758082\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 4900/4948, Batch loss: 7.37715236027725e-05\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57, Batch 4948/4948, Batch loss: 0.0009997946908697486\n",
      "Memory Usage: 901.56 MB\n",
      "Epoch 57 completed in 61.16 seconds, Total Training Loss: 0.00029941771636801716\n",
      "\n",
      "Epoch 58/100\n",
      "Epoch 58, Batch 100/4948, Batch loss: 0.00016956809849943966\n",
      "Memory Usage: 901.58 MB\n",
      "Epoch 58, Batch 200/4948, Batch loss: 1.255139159184182e-05\n",
      "Memory Usage: 901.58 MB\n",
      "Epoch 58, Batch 300/4948, Batch loss: 0.00030559615697711706\n",
      "Memory Usage: 901.27 MB\n",
      "Epoch 58, Batch 400/4948, Batch loss: 0.0001180460603791289\n",
      "Memory Usage: 901.47 MB\n",
      "Epoch 58, Batch 500/4948, Batch loss: 5.1436050853226334e-05\n",
      "Memory Usage: 901.58 MB\n",
      "Epoch 58, Batch 600/4948, Batch loss: 0.0004750564694404602\n",
      "Memory Usage: 888.03 MB\n",
      "Epoch 58, Batch 700/4948, Batch loss: 5.35569415660575e-05\n",
      "Memory Usage: 891.59 MB\n",
      "Epoch 58, Batch 800/4948, Batch loss: 0.00012444630556274205\n",
      "Memory Usage: 880.56 MB\n",
      "Epoch 58, Batch 900/4948, Batch loss: 0.0005323189543560147\n",
      "Memory Usage: 885.41 MB\n",
      "Epoch 58, Batch 1000/4948, Batch loss: 8.52114098961465e-05\n",
      "Memory Usage: 889.48 MB\n",
      "Epoch 58, Batch 1100/4948, Batch loss: 0.0008414442418143153\n",
      "Memory Usage: 890.84 MB\n",
      "Epoch 58, Batch 1200/4948, Batch loss: 0.0013613717164844275\n",
      "Memory Usage: 892.16 MB\n",
      "Epoch 58, Batch 1300/4948, Batch loss: 0.00015041868027765304\n",
      "Memory Usage: 892.16 MB\n",
      "Epoch 58, Batch 1400/4948, Batch loss: 0.00012243328092154115\n",
      "Memory Usage: 892.77 MB\n",
      "Epoch 58, Batch 1500/4948, Batch loss: 7.297666888916865e-05\n",
      "Memory Usage: 893.41 MB\n",
      "Epoch 58, Batch 1600/4948, Batch loss: 1.3277286598167848e-05\n",
      "Memory Usage: 893.41 MB\n",
      "Epoch 58, Batch 1700/4948, Batch loss: 0.000373000861145556\n",
      "Memory Usage: 893.41 MB\n",
      "Epoch 58, Batch 1800/4948, Batch loss: 0.0002587772032711655\n",
      "Memory Usage: 893.41 MB\n",
      "Epoch 58, Batch 1900/4948, Batch loss: 0.0033318090718239546\n",
      "Memory Usage: 893.42 MB\n",
      "Epoch 58, Batch 2000/4948, Batch loss: 0.00041232246439903975\n",
      "Memory Usage: 893.42 MB\n",
      "Epoch 58, Batch 2100/4948, Batch loss: 0.0006888056523166597\n",
      "Memory Usage: 893.42 MB\n",
      "Epoch 58, Batch 2200/4948, Batch loss: 0.00010513129382161424\n",
      "Memory Usage: 893.42 MB\n",
      "Epoch 58, Batch 2300/4948, Batch loss: 0.00021685952378902584\n",
      "Memory Usage: 893.42 MB\n",
      "Epoch 58, Batch 2400/4948, Batch loss: 0.00010545943951001391\n",
      "Memory Usage: 893.42 MB\n",
      "Epoch 58, Batch 2500/4948, Batch loss: 4.992680987925269e-05\n",
      "Memory Usage: 893.42 MB\n",
      "Epoch 58, Batch 2600/4948, Batch loss: 0.0003833179362118244\n",
      "Memory Usage: 893.92 MB\n",
      "Epoch 58, Batch 2700/4948, Batch loss: 0.00035894819302484393\n",
      "Memory Usage: 893.92 MB\n",
      "Epoch 58, Batch 2800/4948, Batch loss: 0.0013596008066087961\n",
      "Memory Usage: 893.92 MB\n",
      "Epoch 58, Batch 2900/4948, Batch loss: 0.00019625388085842133\n",
      "Memory Usage: 893.92 MB\n",
      "Epoch 58, Batch 3000/4948, Batch loss: 5.735652302973904e-05\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 3100/4948, Batch loss: 0.00012529268860816956\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 3200/4948, Batch loss: 0.0003209088172297925\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 3300/4948, Batch loss: 0.00014038788503967226\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 3400/4948, Batch loss: 7.338263458223082e-06\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 3500/4948, Batch loss: 4.861709021497518e-05\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 3600/4948, Batch loss: 4.116475975024514e-05\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 3700/4948, Batch loss: 0.0005595729453489184\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 3800/4948, Batch loss: 0.00011032601469196379\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 3900/4948, Batch loss: 0.00014190361252985895\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 4000/4948, Batch loss: 0.00014735803415533155\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 4100/4948, Batch loss: 4.682475992012769e-05\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 4200/4948, Batch loss: 0.0009777398081496358\n",
      "Memory Usage: 893.97 MB\n",
      "Epoch 58, Batch 4300/4948, Batch loss: 0.0001462827349314466\n",
      "Memory Usage: 892.48 MB\n",
      "Epoch 58, Batch 4400/4948, Batch loss: 3.7860700103919953e-06\n",
      "Memory Usage: 889.41 MB\n",
      "Epoch 58, Batch 4500/4948, Batch loss: 0.0009248557034879923\n",
      "Memory Usage: 889.75 MB\n",
      "Epoch 58, Batch 4600/4948, Batch loss: 0.0002392159658484161\n",
      "Memory Usage: 890.38 MB\n",
      "Epoch 58, Batch 4700/4948, Batch loss: 0.00040944028296507895\n",
      "Memory Usage: 891.19 MB\n",
      "Epoch 58, Batch 4800/4948, Batch loss: 0.00011067621380789205\n",
      "Memory Usage: 891.70 MB\n",
      "Epoch 58, Batch 4900/4948, Batch loss: 7.504305540351197e-05\n",
      "Memory Usage: 892.55 MB\n",
      "Epoch 58, Batch 4948/4948, Batch loss: 0.000973702350165695\n",
      "Memory Usage: 892.55 MB\n",
      "Epoch 58 completed in 61.02 seconds, Total Training Loss: 0.0003005410440619044\n",
      "\n",
      "Epoch 59/100\n",
      "Epoch 59, Batch 100/4948, Batch loss: 0.00016974072786979377\n",
      "Memory Usage: 892.64 MB\n",
      "Epoch 59, Batch 200/4948, Batch loss: 1.7715756257530302e-05\n",
      "Memory Usage: 894.20 MB\n",
      "Epoch 59, Batch 300/4948, Batch loss: 0.0003094452549703419\n",
      "Memory Usage: 894.20 MB\n",
      "Epoch 59, Batch 400/4948, Batch loss: 0.00011519148392835632\n",
      "Memory Usage: 894.22 MB\n",
      "Epoch 59, Batch 500/4948, Batch loss: 5.2300623792689294e-05\n",
      "Memory Usage: 894.22 MB\n",
      "Epoch 59, Batch 600/4948, Batch loss: 0.0004652596835512668\n",
      "Memory Usage: 894.56 MB\n",
      "Epoch 59, Batch 700/4948, Batch loss: 5.53263453184627e-05\n",
      "Memory Usage: 895.42 MB\n",
      "Epoch 59, Batch 800/4948, Batch loss: 0.00012471982336137444\n",
      "Memory Usage: 897.44 MB\n",
      "Epoch 59, Batch 900/4948, Batch loss: 0.0005297861061990261\n",
      "Memory Usage: 899.19 MB\n",
      "Epoch 59, Batch 1000/4948, Batch loss: 8.519942639395595e-05\n",
      "Memory Usage: 899.19 MB\n",
      "Epoch 59, Batch 1100/4948, Batch loss: 0.000833497557323426\n",
      "Memory Usage: 899.19 MB\n",
      "Epoch 59, Batch 1200/4948, Batch loss: 0.0013342075981199741\n",
      "Memory Usage: 899.19 MB\n",
      "Epoch 59, Batch 1300/4948, Batch loss: 0.00015168255777098238\n",
      "Memory Usage: 899.19 MB\n",
      "Epoch 59, Batch 1400/4948, Batch loss: 0.00012377336679492146\n",
      "Memory Usage: 899.19 MB\n",
      "Epoch 59, Batch 1500/4948, Batch loss: 7.542481034761295e-05\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 1600/4948, Batch loss: 1.3208719792601187e-05\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 1700/4948, Batch loss: 0.00032116458169184625\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 1800/4948, Batch loss: 0.0002598652499727905\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 1900/4948, Batch loss: 0.002552356570959091\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 2000/4948, Batch loss: 0.00043319075484760106\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 2100/4948, Batch loss: 0.0006776477093808353\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 2200/4948, Batch loss: 0.0001056639666785486\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 2300/4948, Batch loss: 0.000226350748562254\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 2400/4948, Batch loss: 0.00010753993410617113\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 2500/4948, Batch loss: 4.8306334065273404e-05\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 2600/4948, Batch loss: 0.00037591170985251665\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 2700/4948, Batch loss: 0.00036450327024795115\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 59, Batch 2800/4948, Batch loss: 0.001352997263893485\n",
      "Memory Usage: 899.92 MB\n",
      "Epoch 59, Batch 2900/4948, Batch loss: 0.00020056968787685037\n",
      "Memory Usage: 899.92 MB\n",
      "Epoch 59, Batch 3000/4948, Batch loss: 5.776997932116501e-05\n",
      "Memory Usage: 899.89 MB\n",
      "Epoch 59, Batch 3100/4948, Batch loss: 0.0001272827066713944\n",
      "Memory Usage: 899.89 MB\n",
      "Epoch 59, Batch 3200/4948, Batch loss: 0.00029955667559988797\n",
      "Memory Usage: 899.89 MB\n",
      "Epoch 59, Batch 3300/4948, Batch loss: 0.00013898813631385565\n",
      "Memory Usage: 899.89 MB\n",
      "Epoch 59, Batch 3400/4948, Batch loss: 6.192530690896092e-06\n",
      "Memory Usage: 899.91 MB\n",
      "Epoch 59, Batch 3500/4948, Batch loss: 4.9222366214962676e-05\n",
      "Memory Usage: 899.92 MB\n",
      "Epoch 59, Batch 3600/4948, Batch loss: 4.203000935376622e-05\n",
      "Memory Usage: 899.92 MB\n",
      "Epoch 59, Batch 3700/4948, Batch loss: 0.0005631918902508914\n",
      "Memory Usage: 899.94 MB\n",
      "Epoch 59, Batch 3800/4948, Batch loss: 0.00010707401088438928\n",
      "Memory Usage: 899.94 MB\n",
      "Epoch 59, Batch 3900/4948, Batch loss: 0.00013438647147268057\n",
      "Memory Usage: 899.94 MB\n",
      "Epoch 59, Batch 4000/4948, Batch loss: 0.00014616282714996487\n",
      "Memory Usage: 895.28 MB\n",
      "Epoch 59, Batch 4100/4948, Batch loss: 4.565844210446812e-05\n",
      "Memory Usage: 686.06 MB\n",
      "Epoch 59, Batch 4200/4948, Batch loss: 0.0009714041370898485\n",
      "Memory Usage: 692.88 MB\n",
      "Epoch 59, Batch 4300/4948, Batch loss: 0.0001445196394342929\n",
      "Memory Usage: 693.91 MB\n",
      "Epoch 59, Batch 4400/4948, Batch loss: 3.454294073890196e-06\n",
      "Memory Usage: 697.19 MB\n",
      "Epoch 59, Batch 4500/4948, Batch loss: 0.0009214931633323431\n",
      "Memory Usage: 698.20 MB\n",
      "Epoch 59, Batch 4600/4948, Batch loss: 0.00024584957282058895\n",
      "Memory Usage: 698.67 MB\n",
      "Epoch 59, Batch 4700/4948, Batch loss: 0.00041490059811621904\n",
      "Memory Usage: 699.69 MB\n",
      "Epoch 59, Batch 4800/4948, Batch loss: 0.00011593503586482257\n",
      "Memory Usage: 704.33 MB\n",
      "Epoch 59, Batch 4900/4948, Batch loss: 7.10138920112513e-05\n",
      "Memory Usage: 704.39 MB\n",
      "Epoch 59, Batch 4948/4948, Batch loss: 0.0009893749374896288\n",
      "Memory Usage: 704.39 MB\n",
      "Epoch 59 completed in 61.07 seconds, Total Training Loss: 0.00029097118412484856\n",
      "\n",
      "Epoch 60/100\n",
      "Epoch 60, Batch 100/4948, Batch loss: 0.00017053171177394688\n",
      "Memory Usage: 710.77 MB\n",
      "Epoch 60, Batch 200/4948, Batch loss: 1.4295448636403307e-05\n",
      "Memory Usage: 713.58 MB\n",
      "Epoch 60, Batch 300/4948, Batch loss: 0.0003079121233895421\n",
      "Memory Usage: 714.89 MB\n",
      "Epoch 60, Batch 400/4948, Batch loss: 0.00011343288497300819\n",
      "Memory Usage: 714.89 MB\n",
      "Epoch 60, Batch 500/4948, Batch loss: 5.2189290727255866e-05\n",
      "Memory Usage: 714.89 MB\n",
      "Epoch 60, Batch 600/4948, Batch loss: 0.0004752617096528411\n",
      "Memory Usage: 714.89 MB\n",
      "Epoch 60, Batch 700/4948, Batch loss: 5.453491030493751e-05\n",
      "Memory Usage: 719.12 MB\n",
      "Epoch 60, Batch 800/4948, Batch loss: 0.00012707778660114855\n",
      "Memory Usage: 730.94 MB\n",
      "Epoch 60, Batch 900/4948, Batch loss: 0.0005166397895663977\n",
      "Memory Usage: 742.67 MB\n",
      "Epoch 60, Batch 1000/4948, Batch loss: 8.650006930110976e-05\n",
      "Memory Usage: 754.42 MB\n",
      "Epoch 60, Batch 1100/4948, Batch loss: 0.0008776367176324129\n",
      "Memory Usage: 766.16 MB\n",
      "Epoch 60, Batch 1200/4948, Batch loss: 0.0013292785733938217\n",
      "Memory Usage: 778.44 MB\n",
      "Epoch 60, Batch 1300/4948, Batch loss: 0.000150085223140195\n",
      "Memory Usage: 790.73 MB\n",
      "Epoch 60, Batch 1400/4948, Batch loss: 0.00012233905727043748\n",
      "Memory Usage: 802.38 MB\n",
      "Epoch 60, Batch 1500/4948, Batch loss: 7.396514411084354e-05\n",
      "Memory Usage: 814.16 MB\n",
      "Epoch 60, Batch 1600/4948, Batch loss: 1.320878800470382e-05\n",
      "Memory Usage: 826.14 MB\n",
      "Epoch 60, Batch 1700/4948, Batch loss: 0.00039206931251101196\n",
      "Memory Usage: 837.91 MB\n",
      "Epoch 60, Batch 1800/4948, Batch loss: 0.00025770545471459627\n",
      "Memory Usage: 849.64 MB\n",
      "Epoch 60, Batch 1900/4948, Batch loss: 0.0035254673566669226\n",
      "Memory Usage: 861.34 MB\n",
      "Epoch 60, Batch 2000/4948, Batch loss: 0.00039561704033985734\n",
      "Memory Usage: 873.11 MB\n",
      "Epoch 60, Batch 2100/4948, Batch loss: 0.0006589945405721664\n",
      "Memory Usage: 884.80 MB\n",
      "Epoch 60, Batch 2200/4948, Batch loss: 0.00010457947792019695\n",
      "Memory Usage: 891.73 MB\n",
      "Epoch 60, Batch 2300/4948, Batch loss: 0.00022361877199728042\n",
      "Memory Usage: 891.73 MB\n",
      "Epoch 60, Batch 2400/4948, Batch loss: 0.0001061578223016113\n",
      "Memory Usage: 892.25 MB\n",
      "Epoch 60, Batch 2500/4948, Batch loss: 4.837868982576765e-05\n",
      "Memory Usage: 892.25 MB\n",
      "Epoch 60, Batch 2600/4948, Batch loss: 0.00038607415626756847\n",
      "Memory Usage: 892.27 MB\n",
      "Epoch 60, Batch 2700/4948, Batch loss: 0.0003760538238566369\n",
      "Memory Usage: 892.27 MB\n",
      "Epoch 60, Batch 2800/4948, Batch loss: 0.0013741637812927365\n",
      "Memory Usage: 878.12 MB\n",
      "Epoch 60, Batch 2900/4948, Batch loss: 0.0002005444694077596\n",
      "Memory Usage: 842.09 MB\n",
      "Epoch 60, Batch 3000/4948, Batch loss: 5.8884077589027584e-05\n",
      "Memory Usage: 844.44 MB\n",
      "Epoch 60, Batch 3100/4948, Batch loss: 0.00012724814587272704\n",
      "Memory Usage: 847.45 MB\n",
      "Epoch 60, Batch 3200/4948, Batch loss: 0.0003064716584049165\n",
      "Memory Usage: 850.23 MB\n",
      "Epoch 60, Batch 3300/4948, Batch loss: 0.00013920753553975374\n",
      "Memory Usage: 853.64 MB\n",
      "Epoch 60, Batch 3400/4948, Batch loss: 5.130465069669299e-06\n",
      "Memory Usage: 856.06 MB\n",
      "Epoch 60, Batch 3500/4948, Batch loss: 4.626450754585676e-05\n",
      "Memory Usage: 859.91 MB\n",
      "Epoch 60, Batch 3600/4948, Batch loss: 3.994761573267169e-05\n",
      "Memory Usage: 860.97 MB\n",
      "Epoch 60, Batch 3700/4948, Batch loss: 0.0005608832580037415\n",
      "Memory Usage: 860.97 MB\n",
      "Epoch 60, Batch 3800/4948, Batch loss: 0.00010792961984407157\n",
      "Memory Usage: 861.95 MB\n",
      "Epoch 60, Batch 3900/4948, Batch loss: 0.00013435159053187817\n",
      "Memory Usage: 868.44 MB\n",
      "Epoch 60, Batch 4000/4948, Batch loss: 0.00014782378275413066\n",
      "Memory Usage: 880.17 MB\n",
      "Epoch 60, Batch 4100/4948, Batch loss: 4.7258927224902436e-05\n",
      "Memory Usage: 884.67 MB\n",
      "Epoch 60, Batch 4200/4948, Batch loss: 0.0009718387154862285\n",
      "Memory Usage: 884.67 MB\n",
      "Epoch 60, Batch 4300/4948, Batch loss: 0.00014688407827634364\n",
      "Memory Usage: 884.67 MB\n",
      "Epoch 60, Batch 4400/4948, Batch loss: 3.85608518627123e-06\n",
      "Memory Usage: 884.70 MB\n",
      "Epoch 60, Batch 4500/4948, Batch loss: 0.0009146393858827651\n",
      "Memory Usage: 884.77 MB\n",
      "Epoch 60, Batch 4600/4948, Batch loss: 0.00024317718634847552\n",
      "Memory Usage: 884.77 MB\n",
      "Epoch 60, Batch 4700/4948, Batch loss: 0.0004128108557779342\n",
      "Memory Usage: 884.77 MB\n",
      "Epoch 60, Batch 4800/4948, Batch loss: 0.00011123390140710399\n",
      "Memory Usage: 885.48 MB\n",
      "Epoch 60, Batch 4900/4948, Batch loss: 6.973907875362784e-05\n",
      "Memory Usage: 886.02 MB\n",
      "Epoch 60, Batch 4948/4948, Batch loss: 0.0009739614324644208\n",
      "Memory Usage: 886.03 MB\n",
      "Epoch 60 completed in 60.94 seconds, Total Training Loss: 0.000294088715397119\n",
      "\n",
      "Epoch 61/100\n",
      "Epoch 61, Batch 100/4948, Batch loss: 0.00017086669686250389\n",
      "Memory Usage: 887.03 MB\n",
      "Epoch 61, Batch 200/4948, Batch loss: 1.6775691619841382e-05\n",
      "Memory Usage: 887.03 MB\n",
      "Epoch 61, Batch 300/4948, Batch loss: 0.00030921478173695505\n",
      "Memory Usage: 887.03 MB\n",
      "Epoch 61, Batch 400/4948, Batch loss: 0.00011502730922074988\n",
      "Memory Usage: 887.03 MB\n",
      "Epoch 61, Batch 500/4948, Batch loss: 5.191951277083717e-05\n",
      "Memory Usage: 887.03 MB\n",
      "Epoch 61, Batch 600/4948, Batch loss: 0.00047060695942491293\n",
      "Memory Usage: 887.03 MB\n",
      "Epoch 61, Batch 700/4948, Batch loss: 5.236571450950578e-05\n",
      "Memory Usage: 887.03 MB\n",
      "Epoch 61, Batch 800/4948, Batch loss: 0.00012500493903644383\n",
      "Memory Usage: 887.03 MB\n",
      "Epoch 61, Batch 900/4948, Batch loss: 0.0005189085495658219\n",
      "Memory Usage: 887.03 MB\n",
      "Epoch 61, Batch 1000/4948, Batch loss: 8.541441638953984e-05\n",
      "Memory Usage: 887.03 MB\n",
      "Epoch 61, Batch 1100/4948, Batch loss: 0.0008447915897704661\n",
      "Memory Usage: 887.53 MB\n",
      "Epoch 61, Batch 1200/4948, Batch loss: 0.0013109550345689058\n",
      "Memory Usage: 887.53 MB\n",
      "Epoch 61, Batch 1300/4948, Batch loss: 0.00015224122034851462\n",
      "Memory Usage: 887.53 MB\n",
      "Epoch 61, Batch 1400/4948, Batch loss: 0.00012195980525575578\n",
      "Memory Usage: 887.53 MB\n",
      "Epoch 61, Batch 1500/4948, Batch loss: 7.877142343204468e-05\n",
      "Memory Usage: 887.53 MB\n",
      "Epoch 61, Batch 1600/4948, Batch loss: 2.2657910449197516e-05\n",
      "Memory Usage: 887.53 MB\n",
      "Epoch 61, Batch 1700/4948, Batch loss: 0.0004365869681350887\n",
      "Memory Usage: 887.53 MB\n",
      "Epoch 61, Batch 1800/4948, Batch loss: 0.00026266035274602473\n",
      "Memory Usage: 887.53 MB\n",
      "Epoch 61, Batch 1900/4948, Batch loss: 0.0033175277058035135\n",
      "Memory Usage: 887.53 MB\n",
      "Epoch 61, Batch 2000/4948, Batch loss: 0.0004144453560002148\n",
      "Memory Usage: 887.53 MB\n",
      "Epoch 61, Batch 2100/4948, Batch loss: 0.0006778829265385866\n",
      "Memory Usage: 887.53 MB\n",
      "Epoch 61, Batch 2200/4948, Batch loss: 0.00010516965267015621\n",
      "Memory Usage: 888.05 MB\n",
      "Epoch 61, Batch 2300/4948, Batch loss: 0.0002196191780967638\n",
      "Memory Usage: 888.05 MB\n",
      "Epoch 61, Batch 2400/4948, Batch loss: 0.00010502494842512533\n",
      "Memory Usage: 888.05 MB\n",
      "Epoch 61, Batch 2500/4948, Batch loss: 4.8076406528707594e-05\n",
      "Memory Usage: 888.05 MB\n",
      "Epoch 61, Batch 2600/4948, Batch loss: 0.0003779781109187752\n",
      "Memory Usage: 888.05 MB\n",
      "Epoch 61, Batch 2700/4948, Batch loss: 0.00035849589039571583\n",
      "Memory Usage: 888.11 MB\n",
      "Epoch 61, Batch 2800/4948, Batch loss: 0.0013315995456650853\n",
      "Memory Usage: 791.19 MB\n",
      "Epoch 61, Batch 2900/4948, Batch loss: 0.000198114023078233\n",
      "Memory Usage: 791.53 MB\n",
      "Epoch 61, Batch 3000/4948, Batch loss: 5.849521767231636e-05\n",
      "Memory Usage: 793.03 MB\n",
      "Epoch 61, Batch 3100/4948, Batch loss: 0.00012711375893559307\n",
      "Memory Usage: 792.77 MB\n",
      "Epoch 61, Batch 3200/4948, Batch loss: 0.00031328981276601553\n",
      "Memory Usage: 793.06 MB\n",
      "Epoch 61, Batch 3300/4948, Batch loss: 0.00013961638615000993\n",
      "Memory Usage: 793.08 MB\n",
      "Epoch 61, Batch 3400/4948, Batch loss: 5.675003649230348e-06\n",
      "Memory Usage: 793.11 MB\n",
      "Epoch 61, Batch 3500/4948, Batch loss: 0.0001138002990046516\n",
      "Memory Usage: 793.45 MB\n",
      "Epoch 61, Batch 3600/4948, Batch loss: 4.879005427937955e-05\n",
      "Memory Usage: 793.45 MB\n",
      "Epoch 61, Batch 3700/4948, Batch loss: 0.0005508892936632037\n",
      "Memory Usage: 793.88 MB\n",
      "Epoch 61, Batch 3800/4948, Batch loss: 0.00010685175220714882\n",
      "Memory Usage: 794.39 MB\n",
      "Epoch 61, Batch 3900/4948, Batch loss: 0.00013612030306831002\n",
      "Memory Usage: 791.20 MB\n",
      "Epoch 61, Batch 4000/4948, Batch loss: 0.00014757292228750885\n",
      "Memory Usage: 795.42 MB\n",
      "Epoch 61, Batch 4100/4948, Batch loss: 4.4703952880809084e-05\n",
      "Memory Usage: 795.52 MB\n",
      "Epoch 61, Batch 4200/4948, Batch loss: 0.0009632650762796402\n",
      "Memory Usage: 795.52 MB\n",
      "Epoch 61, Batch 4300/4948, Batch loss: 0.00014801291399635375\n",
      "Memory Usage: 795.53 MB\n",
      "Epoch 61, Batch 4400/4948, Batch loss: 3.3791488931456115e-06\n",
      "Memory Usage: 795.53 MB\n",
      "Epoch 61, Batch 4500/4948, Batch loss: 0.0009139363537542522\n",
      "Memory Usage: 795.59 MB\n",
      "Epoch 61, Batch 4600/4948, Batch loss: 0.0002417439827695489\n",
      "Memory Usage: 795.59 MB\n",
      "Epoch 61, Batch 4700/4948, Batch loss: 0.00041894172318279743\n",
      "Memory Usage: 795.61 MB\n",
      "Epoch 61, Batch 4800/4948, Batch loss: 0.00011062740668421611\n",
      "Memory Usage: 795.61 MB\n",
      "Epoch 61, Batch 4900/4948, Batch loss: 7.960366201587021e-05\n",
      "Memory Usage: 795.61 MB\n",
      "Epoch 61, Batch 4948/4948, Batch loss: 0.001004368532449007\n",
      "Memory Usage: 795.61 MB\n",
      "Epoch 61 completed in 61.81 seconds, Total Training Loss: 0.00029901796862637095\n",
      "Validation completed in 3.43 seconds, Average Validation Loss: 0.0004883282465011614\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 62/100\n",
      "Epoch 62, Batch 100/4948, Batch loss: 0.00017028058937285095\n",
      "Memory Usage: 936.84 MB\n",
      "Epoch 62, Batch 200/4948, Batch loss: 1.0995824595738668e-05\n",
      "Memory Usage: 937.34 MB\n",
      "Epoch 62, Batch 300/4948, Batch loss: 0.00030304971733130515\n",
      "Memory Usage: 937.34 MB\n",
      "Epoch 62, Batch 400/4948, Batch loss: 0.00011470616300357506\n",
      "Memory Usage: 937.34 MB\n",
      "Epoch 62, Batch 500/4948, Batch loss: 5.1548366172937676e-05\n",
      "Memory Usage: 937.34 MB\n",
      "Epoch 62, Batch 600/4948, Batch loss: 0.0004668867331929505\n",
      "Memory Usage: 937.34 MB\n",
      "Epoch 62, Batch 700/4948, Batch loss: 5.6364024203503504e-05\n",
      "Memory Usage: 937.34 MB\n",
      "Epoch 62, Batch 800/4948, Batch loss: 0.000127832216094248\n",
      "Memory Usage: 937.34 MB\n",
      "Epoch 62, Batch 900/4948, Batch loss: 0.0005382340168580413\n",
      "Memory Usage: 937.34 MB\n",
      "Epoch 62, Batch 1000/4948, Batch loss: 8.588954369770363e-05\n",
      "Memory Usage: 937.34 MB\n",
      "Epoch 62, Batch 1100/4948, Batch loss: 0.0008420303929597139\n",
      "Memory Usage: 937.34 MB\n",
      "Epoch 62, Batch 1200/4948, Batch loss: 0.0013077959883958101\n",
      "Memory Usage: 937.83 MB\n",
      "Epoch 62, Batch 1300/4948, Batch loss: 0.00015486098709516227\n",
      "Memory Usage: 937.83 MB\n",
      "Epoch 62, Batch 1400/4948, Batch loss: 0.00012440161663107574\n",
      "Memory Usage: 937.83 MB\n",
      "Epoch 62, Batch 1500/4948, Batch loss: 7.430109690176323e-05\n",
      "Memory Usage: 937.83 MB\n",
      "Epoch 62, Batch 1600/4948, Batch loss: 1.3210843462729827e-05\n",
      "Memory Usage: 937.83 MB\n",
      "Epoch 62, Batch 1700/4948, Batch loss: 0.0003585532249417156\n",
      "Memory Usage: 937.83 MB\n",
      "Epoch 62, Batch 1800/4948, Batch loss: 0.0002596236008685082\n",
      "Memory Usage: 937.83 MB\n",
      "Epoch 62, Batch 1900/4948, Batch loss: 0.003064724849537015\n",
      "Memory Usage: 937.84 MB\n",
      "Epoch 62, Batch 2000/4948, Batch loss: 0.0003960769099649042\n",
      "Memory Usage: 937.84 MB\n",
      "Epoch 62, Batch 2100/4948, Batch loss: 0.0006460021832026541\n",
      "Memory Usage: 937.84 MB\n",
      "Epoch 62, Batch 2200/4948, Batch loss: 0.00010591706086415797\n",
      "Memory Usage: 937.84 MB\n",
      "Epoch 62, Batch 2300/4948, Batch loss: 0.00022231804905459285\n",
      "Memory Usage: 937.84 MB\n",
      "Epoch 62, Batch 2400/4948, Batch loss: 0.0001094707622542046\n",
      "Memory Usage: 937.84 MB\n",
      "Epoch 62, Batch 2500/4948, Batch loss: 4.853610516875051e-05\n",
      "Memory Usage: 937.84 MB\n",
      "Epoch 62, Batch 2600/4948, Batch loss: 0.00037791189970448613\n",
      "Memory Usage: 916.55 MB\n",
      "Epoch 62, Batch 2700/4948, Batch loss: 0.0003703450784087181\n",
      "Memory Usage: 922.50 MB\n",
      "Epoch 62, Batch 2800/4948, Batch loss: 0.0013479702174663544\n",
      "Memory Usage: 928.81 MB\n",
      "Epoch 62, Batch 2900/4948, Batch loss: 0.00019920403428841382\n",
      "Memory Usage: 930.75 MB\n",
      "Epoch 62, Batch 3000/4948, Batch loss: 5.8052864915225655e-05\n",
      "Memory Usage: 934.89 MB\n",
      "Epoch 62, Batch 3100/4948, Batch loss: 0.00012416286335792392\n",
      "Memory Usage: 935.23 MB\n",
      "Epoch 62, Batch 3200/4948, Batch loss: 0.0003038127033505589\n",
      "Memory Usage: 936.88 MB\n",
      "Epoch 62, Batch 3300/4948, Batch loss: 0.00013952363224234432\n",
      "Memory Usage: 937.06 MB\n",
      "Epoch 62, Batch 3400/4948, Batch loss: 1.3016790035180748e-05\n",
      "Memory Usage: 937.06 MB\n",
      "Epoch 62, Batch 3500/4948, Batch loss: 4.650609116652049e-05\n",
      "Memory Usage: 937.06 MB\n",
      "Epoch 62, Batch 3600/4948, Batch loss: 4.0788920159684494e-05\n",
      "Memory Usage: 937.06 MB\n",
      "Epoch 62, Batch 3700/4948, Batch loss: 0.000535550934728235\n",
      "Memory Usage: 937.38 MB\n",
      "Epoch 62, Batch 3800/4948, Batch loss: 0.00010640041728038341\n",
      "Memory Usage: 937.38 MB\n",
      "Epoch 62, Batch 3900/4948, Batch loss: 0.00014339917106553912\n",
      "Memory Usage: 937.38 MB\n",
      "Epoch 62, Batch 4000/4948, Batch loss: 0.00014873854524921626\n",
      "Memory Usage: 937.38 MB\n",
      "Epoch 62, Batch 4100/4948, Batch loss: 4.58973299828358e-05\n",
      "Memory Usage: 937.38 MB\n",
      "Epoch 62, Batch 4200/4948, Batch loss: 0.000975994800683111\n",
      "Memory Usage: 937.38 MB\n",
      "Epoch 62, Batch 4300/4948, Batch loss: 0.0001464764791307971\n",
      "Memory Usage: 937.38 MB\n",
      "Epoch 62, Batch 4400/4948, Batch loss: 3.452659029790084e-06\n",
      "Memory Usage: 937.38 MB\n",
      "Epoch 62, Batch 4500/4948, Batch loss: 0.0009003485320135951\n",
      "Memory Usage: 937.38 MB\n",
      "Epoch 62, Batch 4600/4948, Batch loss: 0.00023717783915344626\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 62, Batch 4700/4948, Batch loss: 0.00041059369686990976\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 62, Batch 4800/4948, Batch loss: 0.00011549959162948653\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 62, Batch 4900/4948, Batch loss: 7.103165989974514e-05\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 62, Batch 4948/4948, Batch loss: 0.0009899483993649483\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 62 completed in 69.92 seconds, Total Training Loss: 0.000296124085835875\n",
      "\n",
      "Epoch 63/100\n",
      "Epoch 63, Batch 100/4948, Batch loss: 0.00017039077647496015\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 200/4948, Batch loss: 1.4576929061149713e-05\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 300/4948, Batch loss: 0.0003092398401349783\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 400/4948, Batch loss: 0.00011367817933205515\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 500/4948, Batch loss: 5.114681334816851e-05\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 600/4948, Batch loss: 0.0004698456032201648\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 700/4948, Batch loss: 6.366275192704052e-05\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 800/4948, Batch loss: 0.00012464028259273618\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 900/4948, Batch loss: 0.0005216291174292564\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 1000/4948, Batch loss: 8.538539259461686e-05\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 1100/4948, Batch loss: 0.0008256605942733586\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 1200/4948, Batch loss: 0.0012961491011083126\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 1300/4948, Batch loss: 0.00015454099047929049\n",
      "Memory Usage: 937.39 MB\n",
      "Epoch 63, Batch 1400/4948, Batch loss: 0.00012175408483017236\n",
      "Memory Usage: 937.41 MB\n",
      "Epoch 63, Batch 1500/4948, Batch loss: 7.330286462092772e-05\n",
      "Memory Usage: 937.41 MB\n",
      "Epoch 63, Batch 1600/4948, Batch loss: 1.3337741620489396e-05\n",
      "Memory Usage: 937.41 MB\n",
      "Epoch 63, Batch 1700/4948, Batch loss: 0.0003895568661391735\n",
      "Memory Usage: 937.41 MB\n",
      "Epoch 63, Batch 1800/4948, Batch loss: 0.0002598676073830575\n",
      "Memory Usage: 937.42 MB\n",
      "Epoch 63, Batch 1900/4948, Batch loss: 0.0034545804373919964\n",
      "Memory Usage: 937.42 MB\n",
      "Epoch 63, Batch 2000/4948, Batch loss: 0.0004045518580824137\n",
      "Memory Usage: 881.84 MB\n",
      "Epoch 63, Batch 2100/4948, Batch loss: 0.0006799122202210128\n",
      "Memory Usage: 895.03 MB\n",
      "Epoch 63, Batch 2200/4948, Batch loss: 0.00010475538874743506\n",
      "Memory Usage: 895.83 MB\n",
      "Epoch 63, Batch 2300/4948, Batch loss: 0.00022223083942662925\n",
      "Memory Usage: 878.62 MB\n",
      "Epoch 63, Batch 2400/4948, Batch loss: 0.00010593757178867236\n",
      "Memory Usage: 880.98 MB\n",
      "Epoch 63, Batch 2500/4948, Batch loss: 4.836936932406388e-05\n",
      "Memory Usage: 723.70 MB\n",
      "Epoch 63, Batch 2600/4948, Batch loss: 0.0003780364932026714\n",
      "Memory Usage: 737.03 MB\n",
      "Epoch 63, Batch 2700/4948, Batch loss: 0.00036388589069247246\n",
      "Memory Usage: 749.83 MB\n",
      "Epoch 63, Batch 2800/4948, Batch loss: 0.0013109503779560328\n",
      "Memory Usage: 766.41 MB\n",
      "Epoch 63, Batch 2900/4948, Batch loss: 0.0001969494769582525\n",
      "Memory Usage: 778.69 MB\n",
      "Epoch 63, Batch 3000/4948, Batch loss: 5.779956336482428e-05\n",
      "Memory Usage: 792.20 MB\n",
      "Epoch 63, Batch 3100/4948, Batch loss: 0.0001235528034158051\n",
      "Memory Usage: 803.98 MB\n",
      "Epoch 63, Batch 3200/4948, Batch loss: 0.00030743249226361513\n",
      "Memory Usage: 815.72 MB\n",
      "Epoch 63, Batch 3300/4948, Batch loss: 0.00013942900113761425\n",
      "Memory Usage: 827.44 MB\n",
      "Epoch 63, Batch 3400/4948, Batch loss: 9.525009772914927e-06\n",
      "Memory Usage: 833.97 MB\n",
      "Epoch 63, Batch 3500/4948, Batch loss: 4.9532241973793134e-05\n",
      "Memory Usage: 841.14 MB\n",
      "Epoch 63, Batch 3600/4948, Batch loss: 4.027145041618496e-05\n",
      "Memory Usage: 857.92 MB\n",
      "Epoch 63, Batch 3700/4948, Batch loss: 0.0005629119696095586\n",
      "Memory Usage: 873.47 MB\n",
      "Epoch 63, Batch 3800/4948, Batch loss: 0.00010618010128382593\n",
      "Memory Usage: 885.28 MB\n",
      "Epoch 63, Batch 3900/4948, Batch loss: 0.00013507052790373564\n",
      "Memory Usage: 891.89 MB\n",
      "Epoch 63, Batch 4000/4948, Batch loss: 0.00014810028369538486\n",
      "Memory Usage: 893.06 MB\n",
      "Epoch 63, Batch 4100/4948, Batch loss: 4.678417826653458e-05\n",
      "Memory Usage: 893.44 MB\n",
      "Epoch 63, Batch 4200/4948, Batch loss: 0.0009723443072289228\n",
      "Memory Usage: 893.45 MB\n",
      "Epoch 63, Batch 4300/4948, Batch loss: 0.0001458925544284284\n",
      "Memory Usage: 893.45 MB\n",
      "Epoch 63, Batch 4400/4948, Batch loss: 3.564859298421652e-06\n",
      "Memory Usage: 893.81 MB\n",
      "Epoch 63, Batch 4500/4948, Batch loss: 0.0009136732551269233\n",
      "Memory Usage: 893.86 MB\n",
      "Epoch 63, Batch 4600/4948, Batch loss: 0.00023832242004573345\n",
      "Memory Usage: 893.86 MB\n",
      "Epoch 63, Batch 4700/4948, Batch loss: 0.0004136638017371297\n",
      "Memory Usage: 894.36 MB\n",
      "Epoch 63, Batch 4800/4948, Batch loss: 0.00011134191299788654\n",
      "Memory Usage: 894.39 MB\n",
      "Epoch 63, Batch 4900/4948, Batch loss: 6.968544039409608e-05\n",
      "Memory Usage: 894.39 MB\n",
      "Epoch 63, Batch 4948/4948, Batch loss: 0.0009478850988671184\n",
      "Memory Usage: 895.33 MB\n",
      "Epoch 63 completed in 68.76 seconds, Total Training Loss: 0.0002938800514078789\n",
      "\n",
      "Epoch 64/100\n",
      "Epoch 64, Batch 100/4948, Batch loss: 0.0001696728722890839\n",
      "Memory Usage: 895.88 MB\n",
      "Epoch 64, Batch 200/4948, Batch loss: 1.810914181987755e-05\n",
      "Memory Usage: 896.28 MB\n",
      "Epoch 64, Batch 300/4948, Batch loss: 0.00030670510022901\n",
      "Memory Usage: 896.30 MB\n",
      "Epoch 64, Batch 400/4948, Batch loss: 0.0001145901478594169\n",
      "Memory Usage: 896.73 MB\n",
      "Epoch 64, Batch 500/4948, Batch loss: 5.130600038683042e-05\n",
      "Memory Usage: 896.73 MB\n",
      "Epoch 64, Batch 600/4948, Batch loss: 0.0004683809238485992\n",
      "Memory Usage: 896.78 MB\n",
      "Epoch 64, Batch 700/4948, Batch loss: 6.373195355990902e-05\n",
      "Memory Usage: 896.78 MB\n",
      "Epoch 64, Batch 800/4948, Batch loss: 0.00012528130901046097\n",
      "Memory Usage: 897.27 MB\n",
      "Epoch 64, Batch 900/4948, Batch loss: 0.0005107195465825498\n",
      "Memory Usage: 897.50 MB\n",
      "Epoch 64, Batch 1000/4948, Batch loss: 8.61255539348349e-05\n",
      "Memory Usage: 897.52 MB\n",
      "Epoch 64, Batch 1100/4948, Batch loss: 0.0008694438147358596\n",
      "Memory Usage: 897.52 MB\n",
      "Epoch 64, Batch 1200/4948, Batch loss: 0.001286997809074819\n",
      "Memory Usage: 897.52 MB\n",
      "Epoch 64, Batch 1300/4948, Batch loss: 0.0001501369843026623\n",
      "Memory Usage: 897.52 MB\n",
      "Epoch 64, Batch 1400/4948, Batch loss: 0.00012212814181111753\n",
      "Memory Usage: 897.52 MB\n",
      "Epoch 64, Batch 1500/4948, Batch loss: 7.696981629123911e-05\n",
      "Memory Usage: 897.52 MB\n",
      "Epoch 64, Batch 1600/4948, Batch loss: 1.3614415365736932e-05\n",
      "Memory Usage: 897.52 MB\n",
      "Epoch 64, Batch 1700/4948, Batch loss: 0.0004177685477770865\n",
      "Memory Usage: 897.55 MB\n",
      "Epoch 64, Batch 1800/4948, Batch loss: 0.0002621382591314614\n",
      "Memory Usage: 897.59 MB\n",
      "Epoch 64, Batch 1900/4948, Batch loss: 0.0032531702890992165\n",
      "Memory Usage: 897.81 MB\n",
      "Epoch 64, Batch 2000/4948, Batch loss: 0.00039473906508646905\n",
      "Memory Usage: 897.83 MB\n",
      "Epoch 64, Batch 2100/4948, Batch loss: 0.000647294451482594\n",
      "Memory Usage: 897.83 MB\n",
      "Epoch 64, Batch 2200/4948, Batch loss: 0.00010480495257070288\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 64, Batch 2300/4948, Batch loss: 0.00022297428222373128\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 64, Batch 2400/4948, Batch loss: 0.00010821646719705313\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 64, Batch 2500/4948, Batch loss: 4.855641964240931e-05\n",
      "Memory Usage: 882.55 MB\n",
      "Epoch 64, Batch 2600/4948, Batch loss: 0.00037903396878391504\n",
      "Memory Usage: 885.48 MB\n",
      "Epoch 64, Batch 2700/4948, Batch loss: 0.00035911801387555897\n",
      "Memory Usage: 893.17 MB\n",
      "Epoch 64, Batch 2800/4948, Batch loss: 0.0013274693628773093\n",
      "Memory Usage: 884.80 MB\n",
      "Epoch 64, Batch 2900/4948, Batch loss: 0.00019829761004075408\n",
      "Memory Usage: 886.20 MB\n",
      "Epoch 64, Batch 3000/4948, Batch loss: 6.051823584130034e-05\n",
      "Memory Usage: 888.91 MB\n",
      "Epoch 64, Batch 3100/4948, Batch loss: 0.00012549049279186875\n",
      "Memory Usage: 892.27 MB\n",
      "Epoch 64, Batch 3200/4948, Batch loss: 0.00030076614348217845\n",
      "Memory Usage: 893.47 MB\n",
      "Epoch 64, Batch 3300/4948, Batch loss: 0.0001395603030687198\n",
      "Memory Usage: 894.64 MB\n",
      "Epoch 64, Batch 3400/4948, Batch loss: 8.778101800999139e-06\n",
      "Memory Usage: 894.89 MB\n",
      "Epoch 64, Batch 3500/4948, Batch loss: 4.675922900787555e-05\n",
      "Memory Usage: 894.89 MB\n",
      "Epoch 64, Batch 3600/4948, Batch loss: 4.0059443563222885e-05\n",
      "Memory Usage: 895.33 MB\n",
      "Epoch 64, Batch 3700/4948, Batch loss: 0.0005540160927921534\n",
      "Memory Usage: 895.33 MB\n",
      "Epoch 64, Batch 3800/4948, Batch loss: 0.00010578009823802859\n",
      "Memory Usage: 895.50 MB\n",
      "Epoch 64, Batch 3900/4948, Batch loss: 0.0001410854165442288\n",
      "Memory Usage: 895.98 MB\n",
      "Epoch 64, Batch 4000/4948, Batch loss: 0.0001469031849410385\n",
      "Memory Usage: 895.98 MB\n",
      "Epoch 64, Batch 4100/4948, Batch loss: 4.651867857319303e-05\n",
      "Memory Usage: 895.98 MB\n",
      "Epoch 64, Batch 4200/4948, Batch loss: 0.0009590108529664576\n",
      "Memory Usage: 896.48 MB\n",
      "Epoch 64, Batch 4300/4948, Batch loss: 0.0001459709892515093\n",
      "Memory Usage: 896.94 MB\n",
      "Epoch 64, Batch 4400/4948, Batch loss: 4.635451205103891e-06\n",
      "Memory Usage: 896.94 MB\n",
      "Epoch 64, Batch 4500/4948, Batch loss: 0.000906344735994935\n",
      "Memory Usage: 897.17 MB\n",
      "Epoch 64, Batch 4600/4948, Batch loss: 0.00023844088718760759\n",
      "Memory Usage: 897.19 MB\n",
      "Epoch 64, Batch 4700/4948, Batch loss: 0.0004100899677723646\n",
      "Memory Usage: 897.23 MB\n",
      "Epoch 64, Batch 4800/4948, Batch loss: 0.00011817507038358599\n",
      "Memory Usage: 897.23 MB\n",
      "Epoch 64, Batch 4900/4948, Batch loss: 7.05736965755932e-05\n",
      "Memory Usage: 897.81 MB\n",
      "Epoch 64, Batch 4948/4948, Batch loss: 0.0009440342546440661\n",
      "Memory Usage: 897.02 MB\n",
      "Epoch 64 completed in 68.19 seconds, Total Training Loss: 0.0002918141680617374\n",
      "\n",
      "Epoch 65/100\n",
      "Epoch 65, Batch 100/4948, Batch loss: 0.00017067193402908742\n",
      "Memory Usage: 897.39 MB\n",
      "Epoch 65, Batch 200/4948, Batch loss: 1.552319008624181e-05\n",
      "Memory Usage: 897.53 MB\n",
      "Epoch 65, Batch 300/4948, Batch loss: 0.00030850645271129906\n",
      "Memory Usage: 897.61 MB\n",
      "Epoch 65, Batch 400/4948, Batch loss: 0.00011349124542903155\n",
      "Memory Usage: 897.73 MB\n",
      "Epoch 65, Batch 500/4948, Batch loss: 5.130452700541355e-05\n",
      "Memory Usage: 897.73 MB\n",
      "Epoch 65, Batch 600/4948, Batch loss: 0.0004720563592854887\n",
      "Memory Usage: 897.73 MB\n",
      "Epoch 65, Batch 700/4948, Batch loss: 5.2652561862487346e-05\n",
      "Memory Usage: 897.73 MB\n",
      "Epoch 65, Batch 800/4948, Batch loss: 0.00014179354184307158\n",
      "Memory Usage: 897.75 MB\n",
      "Epoch 65, Batch 900/4948, Batch loss: 0.0005373009480535984\n",
      "Memory Usage: 897.75 MB\n",
      "Epoch 65, Batch 1000/4948, Batch loss: 8.677898586029187e-05\n",
      "Memory Usage: 897.81 MB\n",
      "Epoch 65, Batch 1100/4948, Batch loss: 0.0008429292356595397\n",
      "Memory Usage: 897.81 MB\n",
      "Epoch 65, Batch 1200/4948, Batch loss: 0.001275207381695509\n",
      "Memory Usage: 897.81 MB\n",
      "Epoch 65, Batch 1300/4948, Batch loss: 0.00015220831846818328\n",
      "Memory Usage: 897.81 MB\n",
      "Epoch 65, Batch 1400/4948, Batch loss: 0.0001226637396030128\n",
      "Memory Usage: 897.81 MB\n",
      "Epoch 65, Batch 1500/4948, Batch loss: 7.323504542000592e-05\n",
      "Memory Usage: 897.81 MB\n",
      "Epoch 65, Batch 1600/4948, Batch loss: 1.3155128726793919e-05\n",
      "Memory Usage: 897.81 MB\n",
      "Epoch 65, Batch 1700/4948, Batch loss: 0.0003373333893250674\n",
      "Memory Usage: 897.81 MB\n",
      "Epoch 65, Batch 1800/4948, Batch loss: 0.0002605948247946799\n",
      "Memory Usage: 897.81 MB\n",
      "Epoch 65, Batch 1900/4948, Batch loss: 0.0026850951835513115\n",
      "Memory Usage: 897.81 MB\n",
      "Epoch 65, Batch 2000/4948, Batch loss: 0.00039653986459597945\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 65, Batch 2100/4948, Batch loss: 0.000656763615552336\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 65, Batch 2200/4948, Batch loss: 0.0001049875863827765\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 65, Batch 2300/4948, Batch loss: 0.00022253711358644068\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 65, Batch 2400/4948, Batch loss: 0.0001053755113389343\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 65, Batch 2500/4948, Batch loss: 4.81976821902208e-05\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 65, Batch 2600/4948, Batch loss: 0.00037861629971303046\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 65, Batch 2700/4948, Batch loss: 0.000355958822183311\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 65, Batch 2800/4948, Batch loss: 0.0012875172542408109\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 65, Batch 2900/4948, Batch loss: 0.00019802484894171357\n",
      "Memory Usage: 897.84 MB\n",
      "Epoch 65, Batch 3000/4948, Batch loss: 5.892667104490101e-05\n",
      "Memory Usage: 897.89 MB\n",
      "Epoch 65, Batch 3100/4948, Batch loss: 0.00012777464871760458\n",
      "Memory Usage: 897.89 MB\n",
      "Epoch 65, Batch 3200/4948, Batch loss: 0.0003024640609510243\n",
      "Memory Usage: 897.89 MB\n",
      "Epoch 65, Batch 3300/4948, Batch loss: 0.0001411033299518749\n",
      "Memory Usage: 897.89 MB\n",
      "Epoch 65, Batch 3400/4948, Batch loss: 5.7962183745985385e-06\n",
      "Memory Usage: 897.89 MB\n",
      "Epoch 65, Batch 3500/4948, Batch loss: 5.0931084842886776e-05\n",
      "Memory Usage: 897.89 MB\n",
      "Epoch 65, Batch 3600/4948, Batch loss: 4.1401381167816e-05\n",
      "Memory Usage: 897.89 MB\n",
      "Epoch 65, Batch 3700/4948, Batch loss: 0.0005403356626629829\n",
      "Memory Usage: 897.89 MB\n",
      "Epoch 65, Batch 3800/4948, Batch loss: 0.00010523194941924885\n",
      "Memory Usage: 897.89 MB\n",
      "Epoch 65, Batch 3900/4948, Batch loss: 0.0001480989740230143\n",
      "Memory Usage: 897.89 MB\n",
      "Epoch 65, Batch 4000/4948, Batch loss: 0.00014636994455941021\n",
      "Memory Usage: 897.89 MB\n",
      "Epoch 65, Batch 4100/4948, Batch loss: 4.5001492253504694e-05\n",
      "Memory Usage: 898.30 MB\n",
      "Epoch 65, Batch 4200/4948, Batch loss: 0.0009700215305201709\n",
      "Memory Usage: 898.31 MB\n",
      "Epoch 65, Batch 4300/4948, Batch loss: 0.00014613193343393505\n",
      "Memory Usage: 898.31 MB\n",
      "Epoch 65, Batch 4400/4948, Batch loss: 3.833920345641673e-06\n",
      "Memory Usage: 898.31 MB\n",
      "Epoch 65, Batch 4500/4948, Batch loss: 0.0009070452651940286\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 65, Batch 4600/4948, Batch loss: 0.00023850135039538145\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 65, Batch 4700/4948, Batch loss: 0.00041823970968835056\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 65, Batch 4800/4948, Batch loss: 0.0001111814781324938\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 65, Batch 4900/4948, Batch loss: 7.843995990697294e-05\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 65, Batch 4948/4948, Batch loss: 0.0009537826990708709\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 65 completed in 66.74 seconds, Total Training Loss: 0.0002930989182917964\n",
      "\n",
      "Epoch 66/100\n",
      "Epoch 66, Batch 100/4948, Batch loss: 0.0001695601240498945\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 200/4948, Batch loss: 1.0921274224529043e-05\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 300/4948, Batch loss: 0.00030793092446401715\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 400/4948, Batch loss: 0.00012061800953233615\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 500/4948, Batch loss: 5.1654245908139274e-05\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 600/4948, Batch loss: 0.00047245045425370336\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 700/4948, Batch loss: 5.6163204135373235e-05\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 800/4948, Batch loss: 0.0001289710053242743\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 900/4948, Batch loss: 0.0005093437503091991\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 1000/4948, Batch loss: 8.633159450255334e-05\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 1100/4948, Batch loss: 0.0008293087012134492\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 1200/4948, Batch loss: 0.001250510336831212\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 1300/4948, Batch loss: 0.0001535636984044686\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 1400/4948, Batch loss: 0.00012325728312134743\n",
      "Memory Usage: 898.36 MB\n",
      "Epoch 66, Batch 1500/4948, Batch loss: 7.33111664885655e-05\n",
      "Memory Usage: 899.36 MB\n",
      "Epoch 66, Batch 1600/4948, Batch loss: 1.3224708709458355e-05\n",
      "Memory Usage: 899.36 MB\n",
      "Epoch 66, Batch 1700/4948, Batch loss: 0.0003417714615352452\n",
      "Memory Usage: 899.36 MB\n",
      "Epoch 66, Batch 1800/4948, Batch loss: 0.00026158217224292457\n",
      "Memory Usage: 899.30 MB\n",
      "Epoch 66, Batch 1900/4948, Batch loss: 0.0038543802220374346\n",
      "Memory Usage: 872.97 MB\n",
      "Epoch 66, Batch 2000/4948, Batch loss: 0.0003942916519008577\n",
      "Memory Usage: 883.48 MB\n",
      "Epoch 66, Batch 2100/4948, Batch loss: 0.0006449339562095702\n",
      "Memory Usage: 888.42 MB\n",
      "Epoch 66, Batch 2200/4948, Batch loss: 0.00010476013994775712\n",
      "Memory Usage: 892.28 MB\n",
      "Epoch 66, Batch 2300/4948, Batch loss: 0.00022504192020278424\n",
      "Memory Usage: 893.14 MB\n",
      "Epoch 66, Batch 2400/4948, Batch loss: 0.00010898512846324593\n",
      "Memory Usage: 895.50 MB\n",
      "Epoch 66, Batch 2500/4948, Batch loss: 4.845603689318523e-05\n",
      "Memory Usage: 896.30 MB\n",
      "Epoch 66, Batch 2600/4948, Batch loss: 0.0003838530683424324\n",
      "Memory Usage: 896.30 MB\n",
      "Epoch 66, Batch 2700/4948, Batch loss: 0.0003665223775897175\n",
      "Memory Usage: 897.30 MB\n",
      "Epoch 66, Batch 2800/4948, Batch loss: 0.0013064853847026825\n",
      "Memory Usage: 897.30 MB\n",
      "Epoch 66, Batch 2900/4948, Batch loss: 0.00019846322538796812\n",
      "Memory Usage: 897.30 MB\n",
      "Epoch 66, Batch 3000/4948, Batch loss: 5.739811240346171e-05\n",
      "Memory Usage: 897.31 MB\n",
      "Epoch 66, Batch 3100/4948, Batch loss: 0.00012587704986799508\n",
      "Memory Usage: 897.58 MB\n",
      "Epoch 66, Batch 3200/4948, Batch loss: 0.0003021456068381667\n",
      "Memory Usage: 898.02 MB\n",
      "Epoch 66, Batch 3300/4948, Batch loss: 0.0001387427473673597\n",
      "Memory Usage: 898.02 MB\n",
      "Epoch 66, Batch 3400/4948, Batch loss: 6.821484930696897e-06\n",
      "Memory Usage: 898.02 MB\n",
      "Epoch 66, Batch 3500/4948, Batch loss: 4.836605512537062e-05\n",
      "Memory Usage: 898.02 MB\n",
      "Epoch 66, Batch 3600/4948, Batch loss: 4.0257549699163064e-05\n",
      "Memory Usage: 898.02 MB\n",
      "Epoch 66, Batch 3700/4948, Batch loss: 0.000559555075597018\n",
      "Memory Usage: 898.02 MB\n",
      "Epoch 66, Batch 3800/4948, Batch loss: 0.00010615634528221563\n",
      "Memory Usage: 898.03 MB\n",
      "Epoch 66, Batch 3900/4948, Batch loss: 0.00013463248615153134\n",
      "Memory Usage: 898.05 MB\n",
      "Epoch 66, Batch 4000/4948, Batch loss: 0.00014687520160805434\n",
      "Memory Usage: 898.06 MB\n",
      "Epoch 66, Batch 4100/4948, Batch loss: 4.552787868306041e-05\n",
      "Memory Usage: 898.06 MB\n",
      "Epoch 66, Batch 4200/4948, Batch loss: 0.000969227054156363\n",
      "Memory Usage: 898.06 MB\n",
      "Epoch 66, Batch 4300/4948, Batch loss: 0.00014665024355053902\n",
      "Memory Usage: 898.41 MB\n",
      "Epoch 66, Batch 4400/4948, Batch loss: 4.513821295404341e-06\n",
      "Memory Usage: 898.69 MB\n",
      "Epoch 66, Batch 4500/4948, Batch loss: 0.0009001119178719819\n",
      "Memory Usage: 898.69 MB\n",
      "Epoch 66, Batch 4600/4948, Batch loss: 0.00023797000176273286\n",
      "Memory Usage: 898.69 MB\n",
      "Epoch 66, Batch 4700/4948, Batch loss: 0.00041131669422611594\n",
      "Memory Usage: 898.69 MB\n",
      "Epoch 66, Batch 4800/4948, Batch loss: 0.0001157169317593798\n",
      "Memory Usage: 898.69 MB\n",
      "Epoch 66, Batch 4900/4948, Batch loss: 7.020761404419318e-05\n",
      "Memory Usage: 898.69 MB\n",
      "Epoch 66, Batch 4948/4948, Batch loss: 0.0009442109148949385\n",
      "Memory Usage: 898.69 MB\n",
      "Epoch 66 completed in 67.25 seconds, Total Training Loss: 0.00029140535197560963\n",
      "\n",
      "Epoch 67/100\n",
      "Epoch 67, Batch 100/4948, Batch loss: 0.00016972539015114307\n",
      "Memory Usage: 898.69 MB\n",
      "Epoch 67, Batch 200/4948, Batch loss: 2.0508490706561133e-05\n",
      "Memory Usage: 898.69 MB\n",
      "Epoch 67, Batch 300/4948, Batch loss: 0.0003038436407223344\n",
      "Memory Usage: 898.69 MB\n",
      "Epoch 67, Batch 400/4948, Batch loss: 0.00011360395001247525\n",
      "Memory Usage: 898.69 MB\n",
      "Epoch 67, Batch 500/4948, Batch loss: 5.175343176233582e-05\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 600/4948, Batch loss: 0.0004727207706309855\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 700/4948, Batch loss: 5.428834992926568e-05\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 800/4948, Batch loss: 0.0001247808540938422\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 900/4948, Batch loss: 0.0005147288902662694\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 1000/4948, Batch loss: 8.581201109336689e-05\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 1100/4948, Batch loss: 0.0008401247323490679\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 1200/4948, Batch loss: 0.0012320955283939838\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 1300/4948, Batch loss: 0.00014986604219302535\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 1400/4948, Batch loss: 0.00012358315871097147\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 1500/4948, Batch loss: 7.346990605583414e-05\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 1600/4948, Batch loss: 1.3137633686710615e-05\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 1700/4948, Batch loss: 0.0003701619280036539\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 1800/4948, Batch loss: 0.0002635018608998507\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 1900/4948, Batch loss: 0.0036526545882225037\n",
      "Memory Usage: 899.06 MB\n",
      "Epoch 67, Batch 2000/4948, Batch loss: 0.00042025893344543874\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 2100/4948, Batch loss: 0.0007003537612035871\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 2200/4948, Batch loss: 0.0001051191211445257\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 2300/4948, Batch loss: 0.00022122259542811662\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 2400/4948, Batch loss: 0.00010550070146564394\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 2500/4948, Batch loss: 4.824894494959153e-05\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 2600/4948, Batch loss: 0.00037980705383233726\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 2700/4948, Batch loss: 0.0003662944072857499\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 2800/4948, Batch loss: 0.0012873976957052946\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 2900/4948, Batch loss: 0.00019855469872709364\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 3000/4948, Batch loss: 5.8472305681789294e-05\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 3100/4948, Batch loss: 0.00012158897879999131\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 3200/4948, Batch loss: 0.000318623409839347\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 3300/4948, Batch loss: 0.00013866715016774833\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 3400/4948, Batch loss: 9.085241799766663e-06\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 3500/4948, Batch loss: 4.7509322030236945e-05\n",
      "Memory Usage: 899.86 MB\n",
      "Epoch 67, Batch 3600/4948, Batch loss: 3.9836697396822274e-05\n",
      "Memory Usage: 883.66 MB\n",
      "Epoch 67, Batch 3700/4948, Batch loss: 0.0005479099927470088\n",
      "Memory Usage: 772.69 MB\n",
      "Epoch 67, Batch 3800/4948, Batch loss: 0.000105067971162498\n",
      "Memory Usage: 778.19 MB\n",
      "Epoch 67, Batch 3900/4948, Batch loss: 0.00015439544222317636\n",
      "Memory Usage: 778.91 MB\n",
      "Epoch 67, Batch 4000/4948, Batch loss: 0.0001462738000554964\n",
      "Memory Usage: 782.75 MB\n",
      "Epoch 67, Batch 4100/4948, Batch loss: 4.555248960969038e-05\n",
      "Memory Usage: 787.67 MB\n",
      "Epoch 67, Batch 4200/4948, Batch loss: 0.000953526352532208\n",
      "Memory Usage: 788.17 MB\n",
      "Epoch 67, Batch 4300/4948, Batch loss: 0.00014507149171549827\n",
      "Memory Usage: 790.70 MB\n",
      "Epoch 67, Batch 4400/4948, Batch loss: 3.3664912280073622e-06\n",
      "Memory Usage: 791.64 MB\n",
      "Epoch 67, Batch 4500/4948, Batch loss: 0.0008874190971255302\n",
      "Memory Usage: 792.30 MB\n",
      "Epoch 67, Batch 4600/4948, Batch loss: 0.00024280311481561512\n",
      "Memory Usage: 793.23 MB\n",
      "Epoch 67, Batch 4700/4948, Batch loss: 0.00041532519389875233\n",
      "Memory Usage: 793.81 MB\n",
      "Epoch 67, Batch 4800/4948, Batch loss: 0.00011058707605116069\n",
      "Memory Usage: 793.84 MB\n",
      "Epoch 67, Batch 4900/4948, Batch loss: 7.452694262610748e-05\n",
      "Memory Usage: 793.84 MB\n",
      "Epoch 67, Batch 4948/4948, Batch loss: 0.0009117282461374998\n",
      "Memory Usage: 793.84 MB\n",
      "Epoch 67 completed in 67.36 seconds, Total Training Loss: 0.0002949882361926575\n",
      "\n",
      "Epoch 68/100\n",
      "Epoch 68, Batch 100/4948, Batch loss: 0.00017078309610951692\n",
      "Memory Usage: 794.05 MB\n",
      "Epoch 68, Batch 200/4948, Batch loss: 1.2783401871274691e-05\n",
      "Memory Usage: 794.05 MB\n",
      "Epoch 68, Batch 300/4948, Batch loss: 0.0003067198267672211\n",
      "Memory Usage: 794.11 MB\n",
      "Epoch 68, Batch 400/4948, Batch loss: 0.00011473204358480871\n",
      "Memory Usage: 795.14 MB\n",
      "Epoch 68, Batch 500/4948, Batch loss: 5.142463487572968e-05\n",
      "Memory Usage: 795.14 MB\n",
      "Epoch 68, Batch 600/4948, Batch loss: 0.00047186133451759815\n",
      "Memory Usage: 795.14 MB\n",
      "Epoch 68, Batch 700/4948, Batch loss: 5.4934280342422426e-05\n",
      "Memory Usage: 795.14 MB\n",
      "Epoch 68, Batch 800/4948, Batch loss: 0.00012560120376292616\n",
      "Memory Usage: 795.14 MB\n",
      "Epoch 68, Batch 900/4948, Batch loss: 0.0005214771954342723\n",
      "Memory Usage: 795.14 MB\n",
      "Epoch 68, Batch 1000/4948, Batch loss: 8.526763849658892e-05\n",
      "Memory Usage: 795.14 MB\n",
      "Epoch 68, Batch 1100/4948, Batch loss: 0.0008271961123682559\n",
      "Memory Usage: 795.14 MB\n",
      "Epoch 68, Batch 1200/4948, Batch loss: 0.0012401826679706573\n",
      "Memory Usage: 795.16 MB\n",
      "Epoch 68, Batch 1300/4948, Batch loss: 0.0001509618159616366\n",
      "Memory Usage: 795.16 MB\n",
      "Epoch 68, Batch 1400/4948, Batch loss: 0.00012264910037629306\n",
      "Memory Usage: 795.66 MB\n",
      "Epoch 68, Batch 1500/4948, Batch loss: 7.501369691453874e-05\n",
      "Memory Usage: 795.66 MB\n",
      "Epoch 68, Batch 1600/4948, Batch loss: 1.319766124652233e-05\n",
      "Memory Usage: 795.66 MB\n",
      "Epoch 68, Batch 1700/4948, Batch loss: 0.00030218317988328636\n",
      "Memory Usage: 795.66 MB\n",
      "Epoch 68, Batch 1800/4948, Batch loss: 0.000264505302766338\n",
      "Memory Usage: 795.66 MB\n",
      "Epoch 68, Batch 1900/4948, Batch loss: 0.000929706497117877\n",
      "Memory Usage: 795.67 MB\n",
      "Epoch 68, Batch 2000/4948, Batch loss: 0.00038237648550421\n",
      "Memory Usage: 795.69 MB\n",
      "Epoch 68, Batch 2100/4948, Batch loss: 0.0006715503404848278\n",
      "Memory Usage: 796.19 MB\n",
      "Epoch 68, Batch 2200/4948, Batch loss: 0.00010520558862481266\n",
      "Memory Usage: 796.20 MB\n",
      "Epoch 68, Batch 2300/4948, Batch loss: 0.0002219400485046208\n",
      "Memory Usage: 796.20 MB\n",
      "Epoch 68, Batch 2400/4948, Batch loss: 0.00011619393626460806\n",
      "Memory Usage: 796.22 MB\n",
      "Epoch 68, Batch 2500/4948, Batch loss: 4.952664312440902e-05\n",
      "Memory Usage: 796.22 MB\n",
      "Epoch 68, Batch 2600/4948, Batch loss: 0.00037311867345124483\n",
      "Memory Usage: 796.73 MB\n",
      "Epoch 68, Batch 2700/4948, Batch loss: 0.00035919490619562566\n",
      "Memory Usage: 797.23 MB\n",
      "Epoch 68, Batch 2800/4948, Batch loss: 0.001288903527893126\n",
      "Memory Usage: 797.27 MB\n",
      "Epoch 68, Batch 2900/4948, Batch loss: 0.00020070362370461226\n",
      "Memory Usage: 797.28 MB\n",
      "Epoch 68, Batch 3000/4948, Batch loss: 5.79968182137236e-05\n",
      "Memory Usage: 797.28 MB\n",
      "Epoch 68, Batch 3100/4948, Batch loss: 0.00012347947631496936\n",
      "Memory Usage: 797.28 MB\n",
      "Epoch 68, Batch 3200/4948, Batch loss: 0.00029776140581816435\n",
      "Memory Usage: 797.28 MB\n",
      "Epoch 68, Batch 3300/4948, Batch loss: 0.00013931244029663503\n",
      "Memory Usage: 797.28 MB\n",
      "Epoch 68, Batch 3400/4948, Batch loss: 4.970063400833169e-06\n",
      "Memory Usage: 797.28 MB\n",
      "Epoch 68, Batch 3500/4948, Batch loss: 0.00014072660997044295\n",
      "Memory Usage: 797.28 MB\n",
      "Epoch 68, Batch 3600/4948, Batch loss: 4.182100383331999e-05\n",
      "Memory Usage: 797.28 MB\n",
      "Epoch 68, Batch 3700/4948, Batch loss: 0.0005885390564799309\n",
      "Memory Usage: 797.28 MB\n",
      "Epoch 68, Batch 3800/4948, Batch loss: 0.00010608931916067377\n",
      "Memory Usage: 797.28 MB\n",
      "Epoch 68, Batch 3900/4948, Batch loss: 0.00014543661382049322\n",
      "Memory Usage: 800.22 MB\n",
      "Epoch 68, Batch 4000/4948, Batch loss: 0.0001480808132328093\n",
      "Memory Usage: 800.22 MB\n",
      "Epoch 68, Batch 4100/4948, Batch loss: 4.5182645408203825e-05\n",
      "Memory Usage: 800.22 MB\n",
      "Epoch 68, Batch 4200/4948, Batch loss: 0.0009485886548645794\n",
      "Memory Usage: 800.22 MB\n",
      "Epoch 68, Batch 4300/4948, Batch loss: 0.00014528771862387657\n",
      "Memory Usage: 800.22 MB\n",
      "Epoch 68, Batch 4400/4948, Batch loss: 3.3844773952296237e-06\n",
      "Memory Usage: 800.22 MB\n",
      "Epoch 68, Batch 4500/4948, Batch loss: 0.0009028640342876315\n",
      "Memory Usage: 800.22 MB\n",
      "Epoch 68, Batch 4600/4948, Batch loss: 0.0002407392457826063\n",
      "Memory Usage: 800.22 MB\n",
      "Epoch 68, Batch 4700/4948, Batch loss: 0.0004118072974961251\n",
      "Memory Usage: 800.23 MB\n",
      "Epoch 68, Batch 4800/4948, Batch loss: 0.00011094669025624171\n",
      "Memory Usage: 800.23 MB\n",
      "Epoch 68, Batch 4900/4948, Batch loss: 7.546761480625719e-05\n",
      "Memory Usage: 800.25 MB\n",
      "Epoch 68, Batch 4948/4948, Batch loss: 0.000946079904679209\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 68 completed in 66.48 seconds, Total Training Loss: 0.00028412139132467316\n",
      "\n",
      "Epoch 69/100\n",
      "Epoch 69, Batch 100/4948, Batch loss: 0.00017133913934230804\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 69, Batch 200/4948, Batch loss: 1.0757785275927745e-05\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 69, Batch 300/4948, Batch loss: 0.0003024877514690161\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 69, Batch 400/4948, Batch loss: 0.00011524176807142794\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 69, Batch 500/4948, Batch loss: 5.1212962716817856e-05\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 69, Batch 600/4948, Batch loss: 0.000472778279799968\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 69, Batch 700/4948, Batch loss: 5.4951768106548116e-05\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 69, Batch 800/4948, Batch loss: 0.0001278222189284861\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 69, Batch 900/4948, Batch loss: 0.0005139561253599823\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 69, Batch 1000/4948, Batch loss: 8.743473154027015e-05\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 69, Batch 1100/4948, Batch loss: 0.0008394732140004635\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 69, Batch 1200/4948, Batch loss: 0.0012170870322734118\n",
      "Memory Usage: 800.27 MB\n",
      "Epoch 69, Batch 1300/4948, Batch loss: 0.0001546037383377552\n",
      "Memory Usage: 800.61 MB\n",
      "Epoch 69, Batch 1400/4948, Batch loss: 0.0001233256043633446\n",
      "Memory Usage: 800.61 MB\n",
      "Epoch 69, Batch 1500/4948, Batch loss: 7.318992720684037e-05\n",
      "Memory Usage: 800.61 MB\n",
      "Epoch 69, Batch 1600/4948, Batch loss: 1.3408884115051478e-05\n",
      "Memory Usage: 800.61 MB\n",
      "Epoch 69, Batch 1700/4948, Batch loss: 0.0004719849966932088\n",
      "Memory Usage: 800.61 MB\n",
      "Epoch 69, Batch 1800/4948, Batch loss: 0.0002618242288008332\n",
      "Memory Usage: 800.61 MB\n",
      "Epoch 69, Batch 1900/4948, Batch loss: 0.002927504712715745\n",
      "Memory Usage: 800.61 MB\n",
      "Epoch 69, Batch 2000/4948, Batch loss: 0.0003957877343054861\n",
      "Memory Usage: 800.62 MB\n",
      "Epoch 69, Batch 2100/4948, Batch loss: 0.0006567075615748763\n",
      "Memory Usage: 800.62 MB\n",
      "Epoch 69, Batch 2200/4948, Batch loss: 0.00010345870396122336\n",
      "Memory Usage: 800.62 MB\n",
      "Epoch 69, Batch 2300/4948, Batch loss: 0.00022918320610187948\n",
      "Memory Usage: 800.62 MB\n",
      "Epoch 69, Batch 2400/4948, Batch loss: 0.00010625566937960684\n",
      "Memory Usage: 800.64 MB\n",
      "Epoch 69, Batch 2500/4948, Batch loss: 4.9336173105984926e-05\n",
      "Memory Usage: 800.64 MB\n",
      "Epoch 69, Batch 2600/4948, Batch loss: 0.00037870879168622196\n",
      "Memory Usage: 800.64 MB\n",
      "Epoch 69, Batch 2700/4948, Batch loss: 0.0003679144138004631\n",
      "Memory Usage: 800.64 MB\n",
      "Epoch 69, Batch 2800/4948, Batch loss: 0.001261890516616404\n",
      "Memory Usage: 800.64 MB\n",
      "Epoch 69, Batch 2900/4948, Batch loss: 0.00019872149277944118\n",
      "Memory Usage: 800.64 MB\n",
      "Epoch 69, Batch 3000/4948, Batch loss: 5.7306813687318936e-05\n",
      "Memory Usage: 800.64 MB\n",
      "Epoch 69, Batch 3100/4948, Batch loss: 0.00012199862976558506\n",
      "Memory Usage: 800.66 MB\n",
      "Epoch 69, Batch 3200/4948, Batch loss: 0.000315026321914047\n",
      "Memory Usage: 800.66 MB\n",
      "Epoch 69, Batch 3300/4948, Batch loss: 0.00014063909475225955\n",
      "Memory Usage: 801.09 MB\n",
      "Epoch 69, Batch 3400/4948, Batch loss: 6.999185643508099e-06\n",
      "Memory Usage: 801.09 MB\n",
      "Epoch 69, Batch 3500/4948, Batch loss: 4.777768117492087e-05\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 3600/4948, Batch loss: 4.148624066147022e-05\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 3700/4948, Batch loss: 0.0005750973359681666\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 3800/4948, Batch loss: 0.00010751653462648392\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 3900/4948, Batch loss: 0.0001615038636373356\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 4000/4948, Batch loss: 0.00014753785217180848\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 4100/4948, Batch loss: 4.936091499985196e-05\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 4200/4948, Batch loss: 0.000939001387450844\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 4300/4948, Batch loss: 0.00014662815374322236\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 4400/4948, Batch loss: 3.4741781291813822e-06\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 4500/4948, Batch loss: 0.0008791071595624089\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 4600/4948, Batch loss: 0.0002442740078549832\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 4700/4948, Batch loss: 0.00040981470374390483\n",
      "Memory Usage: 801.12 MB\n",
      "Epoch 69, Batch 4800/4948, Batch loss: 0.0001251268549822271\n",
      "Memory Usage: 801.69 MB\n",
      "Epoch 69, Batch 4900/4948, Batch loss: 7.932679000077769e-05\n",
      "Memory Usage: 801.69 MB\n",
      "Epoch 69, Batch 4948/4948, Batch loss: 0.0009557778830640018\n",
      "Memory Usage: 801.94 MB\n",
      "Epoch 69 completed in 66.43 seconds, Total Training Loss: 0.0002911461658228087\n",
      "\n",
      "Epoch 70/100\n",
      "Epoch 70, Batch 100/4948, Batch loss: 0.00016936555039137602\n",
      "Memory Usage: 801.94 MB\n",
      "Epoch 70, Batch 200/4948, Batch loss: 1.0563398063823115e-05\n",
      "Memory Usage: 801.94 MB\n",
      "Epoch 70, Batch 300/4948, Batch loss: 0.0003139973268844187\n",
      "Memory Usage: 801.94 MB\n",
      "Epoch 70, Batch 400/4948, Batch loss: 0.0001160092288046144\n",
      "Memory Usage: 801.94 MB\n",
      "Epoch 70, Batch 500/4948, Batch loss: 5.1499318942660466e-05\n",
      "Memory Usage: 801.94 MB\n",
      "Epoch 70, Batch 600/4948, Batch loss: 0.0004647896275855601\n",
      "Memory Usage: 801.94 MB\n",
      "Epoch 70, Batch 700/4948, Batch loss: 5.488764509209432e-05\n",
      "Memory Usage: 801.94 MB\n",
      "Epoch 70, Batch 800/4948, Batch loss: 0.0001233196526300162\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 900/4948, Batch loss: 0.000504590047057718\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 1000/4948, Batch loss: 8.507257734891027e-05\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 1100/4948, Batch loss: 0.0008390898583456874\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 1200/4948, Batch loss: 0.0012205701787024736\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 1300/4948, Batch loss: 0.00014837436901871115\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 1400/4948, Batch loss: 0.00012248061830177903\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 1500/4948, Batch loss: 7.37827576813288e-05\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 1600/4948, Batch loss: 1.3358855539991055e-05\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 1700/4948, Batch loss: 0.00035340272006578743\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 1800/4948, Batch loss: 0.00026012086891569197\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 1900/4948, Batch loss: 0.00392244104295969\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 2000/4948, Batch loss: 0.0003972674603573978\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 2100/4948, Batch loss: 0.0006748242303729057\n",
      "Memory Usage: 802.00 MB\n",
      "Epoch 70, Batch 2200/4948, Batch loss: 0.00010447679233038798\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 2300/4948, Batch loss: 0.00022279779659584165\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 2400/4948, Batch loss: 0.00010724706953624263\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 2500/4948, Batch loss: 4.94032065034844e-05\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 2600/4948, Batch loss: 0.0003832497459370643\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 2700/4948, Batch loss: 0.00037822365993633866\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 2800/4948, Batch loss: 0.001287688035517931\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 2900/4948, Batch loss: 0.00019549689022824168\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 3000/4948, Batch loss: 5.7262557675130665e-05\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 3100/4948, Batch loss: 0.00012213959416840225\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 3200/4948, Batch loss: 0.00032439795904792845\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 3300/4948, Batch loss: 0.0001375487190671265\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 3400/4948, Batch loss: 9.236389814759605e-06\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 3500/4948, Batch loss: 4.6466833737213165e-05\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 3600/4948, Batch loss: 4.233754953020252e-05\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 3700/4948, Batch loss: 0.0005648493533954024\n",
      "Memory Usage: 802.44 MB\n",
      "Epoch 70, Batch 3800/4948, Batch loss: 0.00010491712600924075\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 70, Batch 3900/4948, Batch loss: 0.0001409299293300137\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 70, Batch 4000/4948, Batch loss: 0.0001473841693950817\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 70, Batch 4100/4948, Batch loss: 4.657229874283075e-05\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 70, Batch 4200/4948, Batch loss: 0.0009490528609603643\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 70, Batch 4300/4948, Batch loss: 0.00014677905710414052\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 70, Batch 4400/4948, Batch loss: 3.792268898905604e-06\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 70, Batch 4500/4948, Batch loss: 0.0008801289368420839\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 70, Batch 4600/4948, Batch loss: 0.00023914618941489607\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 70, Batch 4700/4948, Batch loss: 0.0004076296172570437\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 70, Batch 4800/4948, Batch loss: 0.00011236724094487727\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 70, Batch 4900/4948, Batch loss: 7.013275171630085e-05\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 70, Batch 4948/4948, Batch loss: 0.0009179451153613627\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 70 completed in 67.00 seconds, Total Training Loss: 0.0002904240147849191\n",
      "\n",
      "Epoch 71/100\n",
      "Epoch 71, Batch 100/4948, Batch loss: 0.00017035425116773695\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 200/4948, Batch loss: 1.634810359973926e-05\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 300/4948, Batch loss: 0.00030064379097893834\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 400/4948, Batch loss: 0.00011317190364934504\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 500/4948, Batch loss: 5.200142550165765e-05\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 600/4948, Batch loss: 0.00046629292774014175\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 700/4948, Batch loss: 5.309343760018237e-05\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 800/4948, Batch loss: 0.00012392399366945028\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 900/4948, Batch loss: 0.000525747484061867\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 1000/4948, Batch loss: 8.553663064958528e-05\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 1100/4948, Batch loss: 0.0008358510094694793\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 1200/4948, Batch loss: 0.0011962986318394542\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 1300/4948, Batch loss: 0.00015097064897418022\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 1400/4948, Batch loss: 0.00012216456525493413\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 1500/4948, Batch loss: 7.278516568476334e-05\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 1600/4948, Batch loss: 1.3301689250511117e-05\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 1700/4948, Batch loss: 0.0003731720498763025\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 1800/4948, Batch loss: 0.00026089276070706546\n",
      "Memory Usage: 802.47 MB\n",
      "Epoch 71, Batch 1900/4948, Batch loss: 0.004653591196984053\n",
      "Memory Usage: 805.11 MB\n",
      "Epoch 71, Batch 2000/4948, Batch loss: 0.00039743786328472197\n",
      "Memory Usage: 805.11 MB\n",
      "Epoch 71, Batch 2100/4948, Batch loss: 0.0006602902431041002\n",
      "Memory Usage: 805.11 MB\n",
      "Epoch 71, Batch 2200/4948, Batch loss: 0.00010420362377772108\n",
      "Memory Usage: 805.11 MB\n",
      "Epoch 71, Batch 2300/4948, Batch loss: 0.0002249676181236282\n",
      "Memory Usage: 805.11 MB\n",
      "Epoch 71, Batch 2400/4948, Batch loss: 0.00010677967657102272\n",
      "Memory Usage: 805.11 MB\n",
      "Epoch 71, Batch 2500/4948, Batch loss: 4.850324330618605e-05\n",
      "Memory Usage: 805.11 MB\n",
      "Epoch 71, Batch 2600/4948, Batch loss: 0.0003733421035576612\n",
      "Memory Usage: 805.11 MB\n",
      "Epoch 71, Batch 2700/4948, Batch loss: 0.000367215892765671\n",
      "Memory Usage: 805.61 MB\n",
      "Epoch 71, Batch 2800/4948, Batch loss: 0.0012696902267634869\n",
      "Memory Usage: 805.61 MB\n",
      "Epoch 71, Batch 2900/4948, Batch loss: 0.00019712258654180914\n",
      "Memory Usage: 805.61 MB\n",
      "Epoch 71, Batch 3000/4948, Batch loss: 5.7455235946690664e-05\n",
      "Memory Usage: 805.61 MB\n",
      "Epoch 71, Batch 3100/4948, Batch loss: 0.00012512117973528802\n",
      "Memory Usage: 805.62 MB\n",
      "Epoch 71, Batch 3200/4948, Batch loss: 0.00030326738487929106\n",
      "Memory Usage: 805.62 MB\n",
      "Epoch 71, Batch 3300/4948, Batch loss: 0.00013771721569355577\n",
      "Memory Usage: 805.62 MB\n",
      "Epoch 71, Batch 3400/4948, Batch loss: 5.380079983297037e-06\n",
      "Memory Usage: 805.62 MB\n",
      "Epoch 71, Batch 3500/4948, Batch loss: 4.709419954451732e-05\n",
      "Memory Usage: 805.62 MB\n",
      "Epoch 71, Batch 3600/4948, Batch loss: 4.090189395355992e-05\n",
      "Memory Usage: 806.14 MB\n",
      "Epoch 71, Batch 3700/4948, Batch loss: 0.0005607209750451148\n",
      "Memory Usage: 806.64 MB\n",
      "Epoch 71, Batch 3800/4948, Batch loss: 0.00010529218707233667\n",
      "Memory Usage: 806.64 MB\n",
      "Epoch 71, Batch 3900/4948, Batch loss: 0.00013594311894848943\n",
      "Memory Usage: 807.23 MB\n",
      "Epoch 71, Batch 4000/4948, Batch loss: 0.00014626793563365936\n",
      "Memory Usage: 807.23 MB\n",
      "Epoch 71, Batch 4100/4948, Batch loss: 4.617750528268516e-05\n",
      "Memory Usage: 807.58 MB\n",
      "Epoch 71, Batch 4200/4948, Batch loss: 0.0009462485322728753\n",
      "Memory Usage: 807.73 MB\n",
      "Epoch 71, Batch 4300/4948, Batch loss: 0.0001458854676457122\n",
      "Memory Usage: 807.75 MB\n",
      "Epoch 71, Batch 4400/4948, Batch loss: 3.6838393953075865e-06\n",
      "Memory Usage: 807.75 MB\n",
      "Epoch 71, Batch 4500/4948, Batch loss: 0.0008609062642790377\n",
      "Memory Usage: 808.11 MB\n",
      "Epoch 71, Batch 4600/4948, Batch loss: 0.00024134125851560384\n",
      "Memory Usage: 808.11 MB\n",
      "Epoch 71, Batch 4700/4948, Batch loss: 0.0004084170504938811\n",
      "Memory Usage: 808.11 MB\n",
      "Epoch 71, Batch 4800/4948, Batch loss: 0.00011628160427790135\n",
      "Memory Usage: 808.11 MB\n",
      "Epoch 71, Batch 4900/4948, Batch loss: 6.944428605493158e-05\n",
      "Memory Usage: 808.11 MB\n",
      "Epoch 71, Batch 4948/4948, Batch loss: 0.0009224575478583574\n",
      "Memory Usage: 808.12 MB\n",
      "Epoch 71 completed in 68.05 seconds, Total Training Loss: 0.0002894114757119256\n",
      "Validation completed in 3.49 seconds, Average Validation Loss: 0.0004870025698817738\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 72/100\n",
      "Epoch 72, Batch 100/4948, Batch loss: 0.00017021359235513955\n",
      "Memory Usage: 954.02 MB\n",
      "Epoch 72, Batch 200/4948, Batch loss: 1.8427464965498075e-05\n",
      "Memory Usage: 954.02 MB\n",
      "Epoch 72, Batch 300/4948, Batch loss: 0.0003016472328454256\n",
      "Memory Usage: 954.02 MB\n",
      "Epoch 72, Batch 400/4948, Batch loss: 0.0001147663060692139\n",
      "Memory Usage: 954.02 MB\n",
      "Epoch 72, Batch 500/4948, Batch loss: 5.1815910410368815e-05\n",
      "Memory Usage: 954.02 MB\n",
      "Epoch 72, Batch 600/4948, Batch loss: 0.0004728528729174286\n",
      "Memory Usage: 954.02 MB\n",
      "Epoch 72, Batch 700/4948, Batch loss: 7.253344665514305e-05\n",
      "Memory Usage: 954.02 MB\n",
      "Epoch 72, Batch 800/4948, Batch loss: 0.0001254400995094329\n",
      "Memory Usage: 954.02 MB\n",
      "Epoch 72, Batch 900/4948, Batch loss: 0.000522314221598208\n",
      "Memory Usage: 954.02 MB\n",
      "Epoch 72, Batch 1000/4948, Batch loss: 8.841679664328694e-05\n",
      "Memory Usage: 945.11 MB\n",
      "Epoch 72, Batch 1100/4948, Batch loss: 0.0008174257818609476\n",
      "Memory Usage: 951.34 MB\n",
      "Epoch 72, Batch 1200/4948, Batch loss: 0.0011858425568789244\n",
      "Memory Usage: 951.83 MB\n",
      "Epoch 72, Batch 1300/4948, Batch loss: 0.00015395805530715734\n",
      "Memory Usage: 952.39 MB\n",
      "Epoch 72, Batch 1400/4948, Batch loss: 0.0001255619863513857\n",
      "Memory Usage: 952.39 MB\n",
      "Epoch 72, Batch 1500/4948, Batch loss: 7.576295320177451e-05\n",
      "Memory Usage: 952.39 MB\n",
      "Epoch 72, Batch 1600/4948, Batch loss: 1.3277063771965913e-05\n",
      "Memory Usage: 952.41 MB\n",
      "Epoch 72, Batch 1700/4948, Batch loss: 0.00033760970109142363\n",
      "Memory Usage: 952.48 MB\n",
      "Epoch 72, Batch 1800/4948, Batch loss: 0.0002614604600239545\n",
      "Memory Usage: 952.48 MB\n",
      "Epoch 72, Batch 1900/4948, Batch loss: 0.0017436674097552896\n",
      "Memory Usage: 952.86 MB\n",
      "Epoch 72, Batch 2000/4948, Batch loss: 0.0004424861690495163\n",
      "Memory Usage: 952.86 MB\n",
      "Epoch 72, Batch 2100/4948, Batch loss: 0.000667127373162657\n",
      "Memory Usage: 953.59 MB\n",
      "Epoch 72, Batch 2200/4948, Batch loss: 0.00010436734737595543\n",
      "Memory Usage: 953.59 MB\n",
      "Epoch 72, Batch 2300/4948, Batch loss: 0.00022269839246291667\n",
      "Memory Usage: 953.59 MB\n",
      "Epoch 72, Batch 2400/4948, Batch loss: 0.00010640735854394734\n",
      "Memory Usage: 953.59 MB\n",
      "Epoch 72, Batch 2500/4948, Batch loss: 4.8231016990030184e-05\n",
      "Memory Usage: 953.59 MB\n",
      "Epoch 72, Batch 2600/4948, Batch loss: 0.0003790580667555332\n",
      "Memory Usage: 953.59 MB\n",
      "Epoch 72, Batch 2700/4948, Batch loss: 0.00038115333882160485\n",
      "Memory Usage: 953.59 MB\n",
      "Epoch 72, Batch 2800/4948, Batch loss: 0.0012442421866580844\n",
      "Memory Usage: 950.55 MB\n",
      "Epoch 72, Batch 2900/4948, Batch loss: 0.00020040197705384344\n",
      "Memory Usage: 951.45 MB\n",
      "Epoch 72, Batch 3000/4948, Batch loss: 5.991640136926435e-05\n",
      "Memory Usage: 951.72 MB\n",
      "Epoch 72, Batch 3100/4948, Batch loss: 0.00012504034384619445\n",
      "Memory Usage: 949.67 MB\n",
      "Epoch 72, Batch 3200/4948, Batch loss: 0.00029652981902472675\n",
      "Memory Usage: 951.86 MB\n",
      "Epoch 72, Batch 3300/4948, Batch loss: 0.000140872536576353\n",
      "Memory Usage: 952.48 MB\n",
      "Epoch 72, Batch 3400/4948, Batch loss: 9.409100130142178e-06\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 3500/4948, Batch loss: 4.631477713701315e-05\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 3600/4948, Batch loss: 4.130907836952247e-05\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 3700/4948, Batch loss: 0.0005715680890716612\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 3800/4948, Batch loss: 0.00010588948498480022\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 3900/4948, Batch loss: 0.00014855241170153022\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 4000/4948, Batch loss: 0.00014757466851733625\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 4100/4948, Batch loss: 4.55018671345897e-05\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 4200/4948, Batch loss: 0.0009347781306132674\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 4300/4948, Batch loss: 0.00014546231250278652\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 4400/4948, Batch loss: 3.5297980502946302e-06\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 4500/4948, Batch loss: 0.0008755698218010366\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 4600/4948, Batch loss: 0.00024042079166974872\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 4700/4948, Batch loss: 0.00040161414653994143\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 4800/4948, Batch loss: 0.00011050977627746761\n",
      "Memory Usage: 952.75 MB\n",
      "Epoch 72, Batch 4900/4948, Batch loss: 7.027628453215584e-05\n",
      "Memory Usage: 952.00 MB\n",
      "Epoch 72, Batch 4948/4948, Batch loss: 0.0009481635643169284\n",
      "Memory Usage: 953.58 MB\n",
      "Epoch 72 completed in 73.42 seconds, Total Training Loss: 0.00028266546128197164\n",
      "\n",
      "Epoch 73/100\n",
      "Epoch 73, Batch 100/4948, Batch loss: 0.00017087215383071452\n",
      "Memory Usage: 953.88 MB\n",
      "Epoch 73, Batch 200/4948, Batch loss: 1.3246002708910964e-05\n",
      "Memory Usage: 953.94 MB\n",
      "Epoch 73, Batch 300/4948, Batch loss: 0.0003060106246266514\n",
      "Memory Usage: 953.94 MB\n",
      "Epoch 73, Batch 400/4948, Batch loss: 0.00011558572441572323\n",
      "Memory Usage: 953.94 MB\n",
      "Epoch 73, Batch 500/4948, Batch loss: 5.1820901717292145e-05\n",
      "Memory Usage: 953.95 MB\n",
      "Epoch 73, Batch 600/4948, Batch loss: 0.00047923653619363904\n",
      "Memory Usage: 953.95 MB\n",
      "Epoch 73, Batch 700/4948, Batch loss: 5.487167800310999e-05\n",
      "Memory Usage: 941.48 MB\n",
      "Epoch 73, Batch 800/4948, Batch loss: 0.00012805259029846638\n",
      "Memory Usage: 942.12 MB\n",
      "Epoch 73, Batch 900/4948, Batch loss: 0.0005238661542534828\n",
      "Memory Usage: 944.08 MB\n",
      "Epoch 73, Batch 1000/4948, Batch loss: 8.853228064253926e-05\n",
      "Memory Usage: 936.23 MB\n",
      "Epoch 73, Batch 1100/4948, Batch loss: 0.0008215256966650486\n",
      "Memory Usage: 939.19 MB\n",
      "Epoch 73, Batch 1200/4948, Batch loss: 0.0011763592483475804\n",
      "Memory Usage: 939.19 MB\n",
      "Epoch 73, Batch 1300/4948, Batch loss: 0.00015189769328571856\n",
      "Memory Usage: 939.69 MB\n",
      "Epoch 73, Batch 1400/4948, Batch loss: 0.00012453632371034473\n",
      "Memory Usage: 939.69 MB\n",
      "Epoch 73, Batch 1500/4948, Batch loss: 7.526401896029711e-05\n",
      "Memory Usage: 939.69 MB\n",
      "Epoch 73, Batch 1600/4948, Batch loss: 1.332730971626006e-05\n",
      "Memory Usage: 939.69 MB\n",
      "Epoch 73, Batch 1700/4948, Batch loss: 0.0003235412004869431\n",
      "Memory Usage: 941.06 MB\n",
      "Epoch 73, Batch 1800/4948, Batch loss: 0.00026228695060126483\n",
      "Memory Usage: 941.06 MB\n",
      "Epoch 73, Batch 1900/4948, Batch loss: 0.002759092254564166\n",
      "Memory Usage: 941.06 MB\n",
      "Epoch 73, Batch 2000/4948, Batch loss: 0.0003967953962273896\n",
      "Memory Usage: 941.47 MB\n",
      "Epoch 73, Batch 2100/4948, Batch loss: 0.0006398684345185757\n",
      "Memory Usage: 941.47 MB\n",
      "Epoch 73, Batch 2200/4948, Batch loss: 0.00010471743735251948\n",
      "Memory Usage: 941.47 MB\n",
      "Epoch 73, Batch 2300/4948, Batch loss: 0.00022729655029252172\n",
      "Memory Usage: 941.47 MB\n",
      "Epoch 73, Batch 2400/4948, Batch loss: 0.00010367262439103797\n",
      "Memory Usage: 942.03 MB\n",
      "Epoch 73, Batch 2500/4948, Batch loss: 4.8438079829793423e-05\n",
      "Memory Usage: 898.31 MB\n",
      "Epoch 73, Batch 2600/4948, Batch loss: 0.00037633709143847227\n",
      "Memory Usage: 899.81 MB\n",
      "Epoch 73, Batch 2700/4948, Batch loss: 0.00035903064417652786\n",
      "Memory Usage: 900.28 MB\n",
      "Epoch 73, Batch 2800/4948, Batch loss: 0.0012218746123835444\n",
      "Memory Usage: 900.31 MB\n",
      "Epoch 73, Batch 2900/4948, Batch loss: 0.0001982593967113644\n",
      "Memory Usage: 900.31 MB\n",
      "Epoch 73, Batch 3000/4948, Batch loss: 5.778938430012204e-05\n",
      "Memory Usage: 900.31 MB\n",
      "Epoch 73, Batch 3100/4948, Batch loss: 0.00013061660865787417\n",
      "Memory Usage: 900.33 MB\n",
      "Epoch 73, Batch 3200/4948, Batch loss: 0.0002981366706080735\n",
      "Memory Usage: 900.33 MB\n",
      "Epoch 73, Batch 3300/4948, Batch loss: 0.00013779685832560062\n",
      "Memory Usage: 901.45 MB\n",
      "Epoch 73, Batch 3400/4948, Batch loss: 5.149388471181737e-06\n",
      "Memory Usage: 901.89 MB\n",
      "Epoch 73, Batch 3500/4948, Batch loss: 4.5858607336413115e-05\n",
      "Memory Usage: 901.89 MB\n",
      "Epoch 73, Batch 3600/4948, Batch loss: 4.229858677717857e-05\n",
      "Memory Usage: 901.89 MB\n",
      "Epoch 73, Batch 3700/4948, Batch loss: 0.0005712631391361356\n",
      "Memory Usage: 901.95 MB\n",
      "Epoch 73, Batch 3800/4948, Batch loss: 0.0001057115732692182\n",
      "Memory Usage: 901.95 MB\n",
      "Epoch 73, Batch 3900/4948, Batch loss: 0.0001342993346042931\n",
      "Memory Usage: 901.95 MB\n",
      "Epoch 73, Batch 4000/4948, Batch loss: 0.0001473004522267729\n",
      "Memory Usage: 901.95 MB\n",
      "Epoch 73, Batch 4100/4948, Batch loss: 5.176539343665354e-05\n",
      "Memory Usage: 901.95 MB\n",
      "Epoch 73, Batch 4200/4948, Batch loss: 0.0009296914795413613\n",
      "Memory Usage: 902.31 MB\n",
      "Epoch 73, Batch 4300/4948, Batch loss: 0.00014515429211314768\n",
      "Memory Usage: 902.31 MB\n",
      "Epoch 73, Batch 4400/4948, Batch loss: 3.5352788927411893e-06\n",
      "Memory Usage: 902.31 MB\n",
      "Epoch 73, Batch 4500/4948, Batch loss: 0.0008485118160024285\n",
      "Memory Usage: 902.42 MB\n",
      "Epoch 73, Batch 4600/4948, Batch loss: 0.00023833211162127554\n",
      "Memory Usage: 902.42 MB\n",
      "Epoch 73, Batch 4700/4948, Batch loss: 0.00040694940253160894\n",
      "Memory Usage: 902.42 MB\n",
      "Epoch 73, Batch 4800/4948, Batch loss: 0.00011176759289810434\n",
      "Memory Usage: 902.42 MB\n",
      "Epoch 73, Batch 4900/4948, Batch loss: 6.927465437911451e-05\n",
      "Memory Usage: 902.42 MB\n",
      "Epoch 73, Batch 4948/4948, Batch loss: 0.0009594577713869512\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 73 completed in 71.28 seconds, Total Training Loss: 0.0002848835206794848\n",
      "\n",
      "Epoch 74/100\n",
      "Epoch 74, Batch 100/4948, Batch loss: 0.0001710944779915735\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 200/4948, Batch loss: 2.4316586859640665e-05\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 300/4948, Batch loss: 0.000301576656056568\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 400/4948, Batch loss: 0.00011402561358409002\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 500/4948, Batch loss: 5.153184611117467e-05\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 600/4948, Batch loss: 0.00047209381591528654\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 700/4948, Batch loss: 6.733481131959707e-05\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 800/4948, Batch loss: 0.00012508717190939933\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 900/4948, Batch loss: 0.0005048493039794266\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 1000/4948, Batch loss: 8.753099973546341e-05\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 1100/4948, Batch loss: 0.0008179363212548196\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 1200/4948, Batch loss: 0.001186259905807674\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 1300/4948, Batch loss: 0.00015059640281833708\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 1400/4948, Batch loss: 0.0001243215228896588\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 1500/4948, Batch loss: 7.416379230562598e-05\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 1600/4948, Batch loss: 1.3231870070740115e-05\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 1700/4948, Batch loss: 0.0003506680077407509\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 1800/4948, Batch loss: 0.0002614920085761696\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 1900/4948, Batch loss: 0.0034088657703250647\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 2000/4948, Batch loss: 0.0003931750834453851\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 2100/4948, Batch loss: 0.0006625166861340404\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 2200/4948, Batch loss: 0.00010511487926123664\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 2300/4948, Batch loss: 0.00022643238480668515\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 2400/4948, Batch loss: 0.0001056805849657394\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 2500/4948, Batch loss: 4.905726746073924e-05\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 2600/4948, Batch loss: 0.0003814486844930798\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 2700/4948, Batch loss: 0.00037110253470018506\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 2800/4948, Batch loss: 0.0012393990764394403\n",
      "Memory Usage: 903.38 MB\n",
      "Epoch 74, Batch 2900/4948, Batch loss: 0.00019923030049540102\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 3000/4948, Batch loss: 5.722371133742854e-05\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 3100/4948, Batch loss: 0.00012407850590534508\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 3200/4948, Batch loss: 0.00029631107463501394\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 3300/4948, Batch loss: 0.00013883526844438165\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 3400/4948, Batch loss: 6.849426881672116e-06\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 3500/4948, Batch loss: 4.764208279084414e-05\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 3600/4948, Batch loss: 3.999518958153203e-05\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 3700/4948, Batch loss: 0.000560376385692507\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 3800/4948, Batch loss: 0.00010642313282005489\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 3900/4948, Batch loss: 0.0001412312703905627\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 4000/4948, Batch loss: 0.00014716280566062778\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 4100/4948, Batch loss: 4.541571615845896e-05\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 4200/4948, Batch loss: 0.0009318203665316105\n",
      "Memory Usage: 903.39 MB\n",
      "Epoch 74, Batch 4300/4948, Batch loss: 0.00014679547166451812\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 74, Batch 4400/4948, Batch loss: 3.4144816254411126e-06\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 74, Batch 4500/4948, Batch loss: 0.0008736533927731216\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 74, Batch 4600/4948, Batch loss: 0.0002343006490264088\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 74, Batch 4700/4948, Batch loss: 0.0004036680911667645\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 74, Batch 4800/4948, Batch loss: 0.0001154015917563811\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 74, Batch 4900/4948, Batch loss: 6.894180842209607e-05\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 74, Batch 4948/4948, Batch loss: 0.0009250788134522736\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 74 completed in 69.46 seconds, Total Training Loss: 0.0002884317731389205\n",
      "\n",
      "Epoch 75/100\n",
      "Epoch 75, Batch 100/4948, Batch loss: 0.00017180314171127975\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 200/4948, Batch loss: 1.811503170756623e-05\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 300/4948, Batch loss: 0.00030222386703826487\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 400/4948, Batch loss: 0.00011325077502988279\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 500/4948, Batch loss: 5.1478393288562074e-05\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 600/4948, Batch loss: 0.00047410582192242146\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 700/4948, Batch loss: 5.722566129406914e-05\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 800/4948, Batch loss: 0.00012594269355759025\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 900/4948, Batch loss: 0.0005248438683338463\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 1000/4948, Batch loss: 8.567848999518901e-05\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 1100/4948, Batch loss: 0.0008202408789657056\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 1200/4948, Batch loss: 0.0011628406355157495\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 1300/4948, Batch loss: 0.00015423863078467548\n",
      "Memory Usage: 903.80 MB\n",
      "Epoch 75, Batch 1400/4948, Batch loss: 0.00012299073569010943\n",
      "Memory Usage: 904.45 MB\n",
      "Epoch 75, Batch 1500/4948, Batch loss: 7.344967889366671e-05\n",
      "Memory Usage: 904.45 MB\n",
      "Epoch 75, Batch 1600/4948, Batch loss: 1.3291217328514904e-05\n",
      "Memory Usage: 904.45 MB\n",
      "Epoch 75, Batch 1700/4948, Batch loss: 0.0003131433913949877\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 1800/4948, Batch loss: 0.0002640692691784352\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 1900/4948, Batch loss: 0.001494114869274199\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 2000/4948, Batch loss: 0.00039566869963891804\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 2100/4948, Batch loss: 0.0006354914512485266\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 2200/4948, Batch loss: 0.0001047587938955985\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 2300/4948, Batch loss: 0.00021489399659913033\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 2400/4948, Batch loss: 0.00010633088822942227\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 2500/4948, Batch loss: 4.856324812863022e-05\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 2600/4948, Batch loss: 0.0003755924408324063\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 2700/4948, Batch loss: 0.0003754975914489478\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 2800/4948, Batch loss: 0.001211443217471242\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 2900/4948, Batch loss: 0.000199829155462794\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 3000/4948, Batch loss: 5.8453264500712976e-05\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 3100/4948, Batch loss: 0.00012444029562175274\n",
      "Memory Usage: 904.47 MB\n",
      "Epoch 75, Batch 3200/4948, Batch loss: 0.0002956319076474756\n",
      "Memory Usage: 899.97 MB\n",
      "Epoch 75, Batch 3300/4948, Batch loss: 0.00013831375690642744\n",
      "Memory Usage: 899.97 MB\n",
      "Epoch 75, Batch 3400/4948, Batch loss: 5.298844826029381e-06\n",
      "Memory Usage: 899.98 MB\n",
      "Epoch 75, Batch 3500/4948, Batch loss: 6.55993353575468e-05\n",
      "Memory Usage: 899.98 MB\n",
      "Epoch 75, Batch 3600/4948, Batch loss: 4.0129783883458003e-05\n",
      "Memory Usage: 900.45 MB\n",
      "Epoch 75, Batch 3700/4948, Batch loss: 0.000576500257011503\n",
      "Memory Usage: 900.45 MB\n",
      "Epoch 75, Batch 3800/4948, Batch loss: 0.0001061585862771608\n",
      "Memory Usage: 900.45 MB\n",
      "Epoch 75, Batch 3900/4948, Batch loss: 0.00013602996477857232\n",
      "Memory Usage: 900.45 MB\n",
      "Epoch 75, Batch 4000/4948, Batch loss: 0.00014626103802584112\n",
      "Memory Usage: 899.95 MB\n",
      "Epoch 75, Batch 4100/4948, Batch loss: 4.521989103523083e-05\n",
      "Memory Usage: 900.52 MB\n",
      "Epoch 75, Batch 4200/4948, Batch loss: 0.0009218400809913874\n",
      "Memory Usage: 900.52 MB\n",
      "Epoch 75, Batch 4300/4948, Batch loss: 0.00014502655540127307\n",
      "Memory Usage: 900.52 MB\n",
      "Epoch 75, Batch 4400/4948, Batch loss: 5.016780960431788e-06\n",
      "Memory Usage: 900.52 MB\n",
      "Epoch 75, Batch 4500/4948, Batch loss: 0.0008623164612799883\n",
      "Memory Usage: 900.52 MB\n",
      "Epoch 75, Batch 4600/4948, Batch loss: 0.00024123529146891087\n",
      "Memory Usage: 900.53 MB\n",
      "Epoch 75, Batch 4700/4948, Batch loss: 0.0004074333410244435\n",
      "Memory Usage: 900.53 MB\n",
      "Epoch 75, Batch 4800/4948, Batch loss: 0.0001247959880856797\n",
      "Memory Usage: 900.53 MB\n",
      "Epoch 75, Batch 4900/4948, Batch loss: 6.997128366492689e-05\n",
      "Memory Usage: 900.53 MB\n",
      "Epoch 75, Batch 4948/4948, Batch loss: 0.0009110270184464753\n",
      "Memory Usage: 900.53 MB\n",
      "Epoch 75 completed in 69.23 seconds, Total Training Loss: 0.0002802951238569009\n",
      "\n",
      "Epoch 76/100\n",
      "Epoch 76, Batch 100/4948, Batch loss: 0.00017013864999171346\n",
      "Memory Usage: 900.53 MB\n",
      "Epoch 76, Batch 200/4948, Batch loss: 1.0856952030735556e-05\n",
      "Memory Usage: 900.53 MB\n",
      "Epoch 76, Batch 300/4948, Batch loss: 0.0002997300762217492\n",
      "Memory Usage: 900.53 MB\n",
      "Epoch 76, Batch 400/4948, Batch loss: 0.00011555718083400279\n",
      "Memory Usage: 900.53 MB\n",
      "Epoch 76, Batch 500/4948, Batch loss: 5.178737046662718e-05\n",
      "Memory Usage: 900.53 MB\n",
      "Epoch 76, Batch 600/4948, Batch loss: 0.00047075963811948895\n",
      "Memory Usage: 900.53 MB\n",
      "Epoch 76, Batch 700/4948, Batch loss: 5.266914740786888e-05\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 800/4948, Batch loss: 0.00012607585813384503\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 900/4948, Batch loss: 0.0005093177314847708\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 1000/4948, Batch loss: 8.743898797547445e-05\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 1100/4948, Batch loss: 0.0008537776302546263\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 1200/4948, Batch loss: 0.0011347637046128511\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 1300/4948, Batch loss: 0.00015307647117879242\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 1400/4948, Batch loss: 0.00012255723413545638\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 1500/4948, Batch loss: 7.377253496088088e-05\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 1600/4948, Batch loss: 1.3162149116396904e-05\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 1700/4948, Batch loss: 0.00034361667349003255\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 1800/4948, Batch loss: 0.00026204160531051457\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 1900/4948, Batch loss: 0.0035663379821926355\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 2000/4948, Batch loss: 0.00039137419662438333\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 2100/4948, Batch loss: 0.0006576133891940117\n",
      "Memory Usage: 901.03 MB\n",
      "Epoch 76, Batch 2200/4948, Batch loss: 0.00010476729221409187\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 2300/4948, Batch loss: 0.00022982228256296366\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 2400/4948, Batch loss: 0.00010601938265608624\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 2500/4948, Batch loss: 4.926904875901528e-05\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 2600/4948, Batch loss: 0.00037863131728954613\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 2700/4948, Batch loss: 0.0003526649088598788\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 2800/4948, Batch loss: 0.0012464809697121382\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 2900/4948, Batch loss: 0.00019761521252803504\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 3000/4948, Batch loss: 5.812872768728994e-05\n",
      "Memory Usage: 900.62 MB\n",
      "Epoch 76, Batch 3100/4948, Batch loss: 0.00012342298578005284\n",
      "Memory Usage: 901.02 MB\n",
      "Epoch 76, Batch 3200/4948, Batch loss: 0.00030164356576278806\n",
      "Memory Usage: 901.11 MB\n",
      "Epoch 76, Batch 3300/4948, Batch loss: 0.00013712346844840795\n",
      "Memory Usage: 901.11 MB\n",
      "Epoch 76, Batch 3400/4948, Batch loss: 7.319944870687323e-06\n",
      "Memory Usage: 901.11 MB\n",
      "Epoch 76, Batch 3500/4948, Batch loss: 4.7606139560230076e-05\n",
      "Memory Usage: 901.11 MB\n",
      "Epoch 76, Batch 3600/4948, Batch loss: 3.936211214750074e-05\n",
      "Memory Usage: 901.11 MB\n",
      "Epoch 76, Batch 3700/4948, Batch loss: 0.0005672554834745824\n",
      "Memory Usage: 901.11 MB\n",
      "Epoch 76, Batch 3800/4948, Batch loss: 0.0001061480725184083\n",
      "Memory Usage: 901.11 MB\n",
      "Epoch 76, Batch 3900/4948, Batch loss: 0.0001418328465661034\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 4000/4948, Batch loss: 0.00014838873175904155\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 4100/4948, Batch loss: 4.5589640649268404e-05\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 4200/4948, Batch loss: 0.00091937166871503\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 4300/4948, Batch loss: 0.0001470547722419724\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 4400/4948, Batch loss: 3.461397909632069e-06\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 4500/4948, Batch loss: 0.0008751620189286768\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 4600/4948, Batch loss: 0.0002423532132524997\n",
      "Memory Usage: 901.22 MB\n",
      "Epoch 76, Batch 4700/4948, Batch loss: 0.00040825389442034066\n",
      "Memory Usage: 901.27 MB\n",
      "Epoch 76, Batch 4800/4948, Batch loss: 0.000111156034108717\n",
      "Memory Usage: 901.27 MB\n",
      "Epoch 76, Batch 4900/4948, Batch loss: 7.21963369869627e-05\n",
      "Memory Usage: 901.27 MB\n",
      "Epoch 76, Batch 4948/4948, Batch loss: 0.0009271807502955198\n",
      "Memory Usage: 901.27 MB\n",
      "Epoch 76 completed in 68.87 seconds, Total Training Loss: 0.0002877160953529706\n",
      "\n",
      "Epoch 77/100\n",
      "Epoch 77, Batch 100/4948, Batch loss: 0.0001719740394037217\n",
      "Memory Usage: 901.27 MB\n",
      "Epoch 77, Batch 200/4948, Batch loss: 1.0838063644769136e-05\n",
      "Memory Usage: 901.27 MB\n",
      "Epoch 77, Batch 300/4948, Batch loss: 0.00030185480136424303\n",
      "Memory Usage: 901.27 MB\n",
      "Epoch 77, Batch 400/4948, Batch loss: 0.00012207250983919948\n",
      "Memory Usage: 901.27 MB\n",
      "Epoch 77, Batch 500/4948, Batch loss: 5.1712409913307056e-05\n",
      "Memory Usage: 901.27 MB\n",
      "Epoch 77, Batch 600/4948, Batch loss: 0.00046841177390888333\n",
      "Memory Usage: 901.27 MB\n",
      "Epoch 77, Batch 700/4948, Batch loss: 5.737242827308364e-05\n",
      "Memory Usage: 901.27 MB\n",
      "Epoch 77, Batch 800/4948, Batch loss: 0.0001323187752859667\n",
      "Memory Usage: 899.47 MB\n",
      "Epoch 77, Batch 900/4948, Batch loss: 0.0005144145688973367\n",
      "Memory Usage: 899.47 MB\n",
      "Epoch 77, Batch 1000/4948, Batch loss: 8.635583071736619e-05\n",
      "Memory Usage: 900.28 MB\n",
      "Epoch 77, Batch 1100/4948, Batch loss: 0.0008321386994794011\n",
      "Memory Usage: 900.36 MB\n",
      "Epoch 77, Batch 1200/4948, Batch loss: 0.0011443556286394596\n",
      "Memory Usage: 897.53 MB\n",
      "Epoch 77, Batch 1300/4948, Batch loss: 0.0001510010042693466\n",
      "Memory Usage: 899.47 MB\n",
      "Epoch 77, Batch 1400/4948, Batch loss: 0.00012166587839601561\n",
      "Memory Usage: 899.47 MB\n",
      "Epoch 77, Batch 1500/4948, Batch loss: 7.786091737216339e-05\n",
      "Memory Usage: 899.55 MB\n",
      "Epoch 77, Batch 1600/4948, Batch loss: 1.4226944585971069e-05\n",
      "Memory Usage: 823.69 MB\n",
      "Epoch 77, Batch 1700/4948, Batch loss: 0.0003331410407554358\n",
      "Memory Usage: 750.59 MB\n",
      "Epoch 77, Batch 1800/4948, Batch loss: 0.00026157681713812053\n",
      "Memory Usage: 767.81 MB\n",
      "Epoch 77, Batch 1900/4948, Batch loss: 0.003374615916982293\n",
      "Memory Usage: 780.69 MB\n",
      "Epoch 77, Batch 2000/4948, Batch loss: 0.000392591260606423\n",
      "Memory Usage: 792.98 MB\n",
      "Epoch 77, Batch 2100/4948, Batch loss: 0.0006668905261904001\n",
      "Memory Usage: 713.03 MB\n",
      "Epoch 77, Batch 2200/4948, Batch loss: 0.00010341659799451008\n",
      "Memory Usage: 557.45 MB\n",
      "Epoch 77, Batch 2300/4948, Batch loss: 0.00022438795713242143\n",
      "Memory Usage: 452.55 MB\n",
      "Epoch 77, Batch 2400/4948, Batch loss: 0.00010611258039716631\n",
      "Memory Usage: 467.12 MB\n",
      "Epoch 77, Batch 2500/4948, Batch loss: 4.953966708853841e-05\n",
      "Memory Usage: 480.08 MB\n",
      "Epoch 77, Batch 2600/4948, Batch loss: 0.00037890049861744046\n",
      "Memory Usage: 493.61 MB\n",
      "Epoch 77, Batch 2700/4948, Batch loss: 0.00036174411070533097\n",
      "Memory Usage: 494.50 MB\n",
      "Epoch 77, Batch 2800/4948, Batch loss: 0.0012175309238955379\n",
      "Memory Usage: 500.97 MB\n",
      "Epoch 77, Batch 2900/4948, Batch loss: 0.0001975494233192876\n",
      "Memory Usage: 476.12 MB\n",
      "Epoch 77, Batch 3000/4948, Batch loss: 5.789347778772935e-05\n",
      "Memory Usage: 488.73 MB\n",
      "Epoch 77, Batch 3100/4948, Batch loss: 0.00012155345029896125\n",
      "Memory Usage: 501.03 MB\n",
      "Epoch 77, Batch 3200/4948, Batch loss: 0.00032175390515476465\n",
      "Memory Usage: 415.70 MB\n",
      "Epoch 77, Batch 3300/4948, Batch loss: 0.00013727623445447534\n",
      "Memory Usage: 428.06 MB\n",
      "Epoch 77, Batch 3400/4948, Batch loss: 6.12869325777865e-06\n",
      "Memory Usage: 440.16 MB\n",
      "Epoch 77, Batch 3500/4948, Batch loss: 4.804217678611167e-05\n",
      "Memory Usage: 451.31 MB\n",
      "Epoch 77, Batch 3600/4948, Batch loss: 4.0472339605912566e-05\n",
      "Memory Usage: 429.11 MB\n",
      "Epoch 77, Batch 3700/4948, Batch loss: 0.0005479694227688015\n",
      "Memory Usage: 432.56 MB\n",
      "Epoch 77, Batch 3800/4948, Batch loss: 0.00010791842214530334\n",
      "Memory Usage: 451.59 MB\n",
      "Epoch 77, Batch 3900/4948, Batch loss: 0.00016035878797993064\n",
      "Memory Usage: 464.09 MB\n",
      "Epoch 77, Batch 4000/4948, Batch loss: 0.00014910101890563965\n",
      "Memory Usage: 477.44 MB\n",
      "Epoch 77, Batch 4100/4948, Batch loss: 4.429053660714999e-05\n",
      "Memory Usage: 489.77 MB\n",
      "Epoch 77, Batch 4200/4948, Batch loss: 0.0009090459207072854\n",
      "Memory Usage: 502.77 MB\n",
      "Epoch 77, Batch 4300/4948, Batch loss: 0.00014552347420249134\n",
      "Memory Usage: 514.88 MB\n",
      "Epoch 77, Batch 4400/4948, Batch loss: 3.7453062304848572e-06\n",
      "Memory Usage: 527.02 MB\n",
      "Epoch 77, Batch 4500/4948, Batch loss: 0.000860315456520766\n",
      "Memory Usage: 539.31 MB\n",
      "Epoch 77, Batch 4600/4948, Batch loss: 0.00023550301557406783\n",
      "Memory Usage: 552.23 MB\n",
      "Epoch 77, Batch 4700/4948, Batch loss: 0.00040201834053732455\n",
      "Memory Usage: 564.16 MB\n",
      "Epoch 77, Batch 4800/4948, Batch loss: 0.00013100674550514668\n",
      "Memory Usage: 576.02 MB\n",
      "Epoch 77, Batch 4900/4948, Batch loss: 7.249054760904983e-05\n",
      "Memory Usage: 588.11 MB\n",
      "Epoch 77, Batch 4948/4948, Batch loss: 0.0009517666767351329\n",
      "Memory Usage: 593.73 MB\n",
      "Epoch 77 completed in 400.10 seconds, Total Training Loss: 0.00028770909187380797\n",
      "\n",
      "Epoch 78/100\n",
      "Epoch 78, Batch 100/4948, Batch loss: 0.00016896681336220354\n",
      "Memory Usage: 605.70 MB\n",
      "Epoch 78, Batch 200/4948, Batch loss: 1.5830113625270315e-05\n",
      "Memory Usage: 617.28 MB\n",
      "Epoch 78, Batch 300/4948, Batch loss: 0.00030415039509534836\n",
      "Memory Usage: 629.64 MB\n",
      "Epoch 78, Batch 400/4948, Batch loss: 0.00011420492955949157\n",
      "Memory Usage: 637.09 MB\n",
      "Epoch 78, Batch 500/4948, Batch loss: 5.2364819566719234e-05\n",
      "Memory Usage: 616.27 MB\n",
      "Epoch 78, Batch 600/4948, Batch loss: 0.000470629776827991\n",
      "Memory Usage: 627.92 MB\n",
      "Epoch 78, Batch 700/4948, Batch loss: 5.225157656241208e-05\n",
      "Memory Usage: 637.83 MB\n",
      "Epoch 78, Batch 800/4948, Batch loss: 0.00012630030687432736\n",
      "Memory Usage: 645.41 MB\n",
      "Epoch 78, Batch 900/4948, Batch loss: 0.0005134583916515112\n",
      "Memory Usage: 657.53 MB\n",
      "Epoch 78, Batch 1000/4948, Batch loss: 8.535498636774719e-05\n",
      "Memory Usage: 669.14 MB\n",
      "Epoch 78, Batch 1100/4948, Batch loss: 0.0008151452639140189\n",
      "Memory Usage: 554.95 MB\n",
      "Epoch 78, Batch 1200/4948, Batch loss: 0.0011357314651831985\n",
      "Memory Usage: 563.33 MB\n",
      "Epoch 78, Batch 1300/4948, Batch loss: 0.00015426456229761243\n",
      "Memory Usage: 548.22 MB\n",
      "Epoch 78, Batch 1400/4948, Batch loss: 0.0001240494748344645\n",
      "Memory Usage: 562.55 MB\n",
      "Epoch 78, Batch 1500/4948, Batch loss: 7.29249368305318e-05\n",
      "Memory Usage: 577.75 MB\n",
      "Epoch 78, Batch 1600/4948, Batch loss: 1.5112016626517288e-05\n",
      "Memory Usage: 593.06 MB\n",
      "Epoch 78, Batch 1700/4948, Batch loss: 0.00035426527028903365\n",
      "Memory Usage: 606.25 MB\n",
      "Epoch 78, Batch 1800/4948, Batch loss: 0.00025992587325163186\n",
      "Memory Usage: 625.17 MB\n",
      "Epoch 78, Batch 1900/4948, Batch loss: 0.0028907929081469774\n",
      "Memory Usage: 640.84 MB\n",
      "Epoch 78, Batch 2000/4948, Batch loss: 0.00039954445674084127\n",
      "Memory Usage: 648.72 MB\n",
      "Epoch 78, Batch 2100/4948, Batch loss: 0.0006656096666119993\n",
      "Memory Usage: 656.73 MB\n",
      "Epoch 78, Batch 2200/4948, Batch loss: 0.00010471262794453651\n",
      "Memory Usage: 669.22 MB\n",
      "Epoch 78, Batch 2300/4948, Batch loss: 0.00021720539371017367\n",
      "Memory Usage: 682.62 MB\n",
      "Epoch 78, Batch 2400/4948, Batch loss: 0.00011020995589205995\n",
      "Memory Usage: 695.61 MB\n",
      "Epoch 78, Batch 2500/4948, Batch loss: 4.840039036935195e-05\n",
      "Memory Usage: 707.39 MB\n",
      "Epoch 78, Batch 2600/4948, Batch loss: 0.00037394434912130237\n",
      "Memory Usage: 719.19 MB\n",
      "Epoch 78, Batch 2700/4948, Batch loss: 0.00036016618832945824\n",
      "Memory Usage: 730.95 MB\n",
      "Epoch 78, Batch 2800/4948, Batch loss: 0.0012081756722182035\n",
      "Memory Usage: 742.77 MB\n",
      "Epoch 78, Batch 2900/4948, Batch loss: 0.00020364526426419616\n",
      "Memory Usage: 754.62 MB\n",
      "Epoch 78, Batch 3000/4948, Batch loss: 5.805433829664253e-05\n",
      "Memory Usage: 766.38 MB\n",
      "Epoch 78, Batch 3100/4948, Batch loss: 0.00012726610293611884\n",
      "Memory Usage: 778.20 MB\n",
      "Epoch 78, Batch 3200/4948, Batch loss: 0.000293624383630231\n",
      "Memory Usage: 774.56 MB\n",
      "Epoch 78, Batch 3300/4948, Batch loss: 0.00013740903523284942\n",
      "Memory Usage: 770.73 MB\n",
      "Epoch 78, Batch 3400/4948, Batch loss: 5.4322708820109256e-06\n",
      "Memory Usage: 764.53 MB\n",
      "Epoch 78, Batch 3500/4948, Batch loss: 4.679454650613479e-05\n",
      "Memory Usage: 778.77 MB\n",
      "Epoch 78, Batch 3600/4948, Batch loss: 4.035477468278259e-05\n",
      "Memory Usage: 791.86 MB\n",
      "Epoch 78, Batch 3700/4948, Batch loss: 0.0005612591630779207\n",
      "Memory Usage: 793.94 MB\n",
      "Epoch 78, Batch 3800/4948, Batch loss: 0.00010713638039305806\n",
      "Memory Usage: 793.95 MB\n",
      "Epoch 78, Batch 3900/4948, Batch loss: 0.00014020634989719838\n",
      "Memory Usage: 794.00 MB\n",
      "Epoch 78, Batch 4000/4948, Batch loss: 0.00014881064998917282\n",
      "Memory Usage: 794.02 MB\n",
      "Epoch 78, Batch 4100/4948, Batch loss: 4.47877318947576e-05\n",
      "Memory Usage: 794.02 MB\n",
      "Epoch 78, Batch 4200/4948, Batch loss: 0.0009155540610663593\n",
      "Memory Usage: 794.05 MB\n",
      "Epoch 78, Batch 4300/4948, Batch loss: 0.00014495247160084546\n",
      "Memory Usage: 794.08 MB\n",
      "Epoch 78, Batch 4400/4948, Batch loss: 3.5869657040166203e-06\n",
      "Memory Usage: 794.08 MB\n",
      "Epoch 78, Batch 4500/4948, Batch loss: 0.0008602472953498363\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 78, Batch 4600/4948, Batch loss: 0.0002442558470647782\n",
      "Memory Usage: 794.02 MB\n",
      "Epoch 78, Batch 4700/4948, Batch loss: 0.0004143088881392032\n",
      "Memory Usage: 794.36 MB\n",
      "Epoch 78, Batch 4800/4948, Batch loss: 0.0001254662493010983\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 78, Batch 4900/4948, Batch loss: 7.310930959647521e-05\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 78, Batch 4948/4948, Batch loss: 0.0009244811371900141\n",
      "Memory Usage: 795.47 MB\n",
      "Epoch 78 completed in 82.97 seconds, Total Training Loss: 0.000286745528788978\n",
      "\n",
      "Epoch 79/100\n",
      "Epoch 79, Batch 100/4948, Batch loss: 0.00016958652122411877\n",
      "Memory Usage: 795.55 MB\n",
      "Epoch 79, Batch 200/4948, Batch loss: 1.1378078852430917e-05\n",
      "Memory Usage: 795.77 MB\n",
      "Epoch 79, Batch 300/4948, Batch loss: 0.00029936261125840247\n",
      "Memory Usage: 776.94 MB\n",
      "Epoch 79, Batch 400/4948, Batch loss: 0.00011653928959276527\n",
      "Memory Usage: 771.52 MB\n",
      "Epoch 79, Batch 500/4948, Batch loss: 5.146196053829044e-05\n",
      "Memory Usage: 774.55 MB\n",
      "Epoch 79, Batch 600/4948, Batch loss: 0.00047596736112609506\n",
      "Memory Usage: 772.58 MB\n",
      "Epoch 79, Batch 700/4948, Batch loss: 5.618776776827872e-05\n",
      "Memory Usage: 739.81 MB\n",
      "Epoch 79, Batch 800/4948, Batch loss: 0.00013543915702030063\n",
      "Memory Usage: 741.39 MB\n",
      "Epoch 79, Batch 900/4948, Batch loss: 0.0005236489814706147\n",
      "Memory Usage: 743.38 MB\n",
      "Epoch 79, Batch 1000/4948, Batch loss: 9.006948675960302e-05\n",
      "Memory Usage: 744.88 MB\n",
      "Epoch 79, Batch 1100/4948, Batch loss: 0.0008036090875975788\n",
      "Memory Usage: 746.39 MB\n",
      "Epoch 79, Batch 1200/4948, Batch loss: 0.0011199068976566195\n",
      "Memory Usage: 746.98 MB\n",
      "Epoch 79, Batch 1300/4948, Batch loss: 0.00015320430975407362\n",
      "Memory Usage: 747.55 MB\n",
      "Epoch 79, Batch 1400/4948, Batch loss: 0.0001248046464752406\n",
      "Memory Usage: 747.50 MB\n",
      "Epoch 79, Batch 1500/4948, Batch loss: 7.627439481439069e-05\n",
      "Memory Usage: 747.75 MB\n",
      "Epoch 79, Batch 1600/4948, Batch loss: 1.3239327927294653e-05\n",
      "Memory Usage: 747.75 MB\n",
      "Epoch 79, Batch 1700/4948, Batch loss: 0.00031758646946400404\n",
      "Memory Usage: 748.25 MB\n",
      "Epoch 79, Batch 1800/4948, Batch loss: 0.00026622010045684874\n",
      "Memory Usage: 748.25 MB\n",
      "Epoch 79, Batch 1900/4948, Batch loss: 0.0031746216118335724\n",
      "Memory Usage: 748.19 MB\n",
      "Epoch 79, Batch 2000/4948, Batch loss: 0.00040575346793048084\n",
      "Memory Usage: 748.20 MB\n",
      "Epoch 79, Batch 2100/4948, Batch loss: 0.0006973558920435607\n",
      "Memory Usage: 748.17 MB\n",
      "Epoch 79, Batch 2200/4948, Batch loss: 0.00010453609138494357\n",
      "Memory Usage: 748.17 MB\n",
      "Epoch 79, Batch 2300/4948, Batch loss: 0.00022959051420912147\n",
      "Memory Usage: 748.17 MB\n",
      "Epoch 79, Batch 2400/4948, Batch loss: 0.00010577186912996694\n",
      "Memory Usage: 748.17 MB\n",
      "Epoch 79, Batch 2500/4948, Batch loss: 4.8704168875701725e-05\n",
      "Memory Usage: 748.17 MB\n",
      "Epoch 79, Batch 2600/4948, Batch loss: 0.00037605324178002775\n",
      "Memory Usage: 748.17 MB\n",
      "Epoch 79, Batch 2700/4948, Batch loss: 0.0003537310112733394\n",
      "Memory Usage: 748.72 MB\n",
      "Epoch 79, Batch 2800/4948, Batch loss: 0.0012507257051765919\n",
      "Memory Usage: 748.72 MB\n",
      "Epoch 79, Batch 2900/4948, Batch loss: 0.000198855806956999\n",
      "Memory Usage: 748.72 MB\n",
      "Epoch 79, Batch 3000/4948, Batch loss: 5.750811396865174e-05\n",
      "Memory Usage: 748.73 MB\n",
      "Epoch 79, Batch 3100/4948, Batch loss: 0.00012302213872317225\n",
      "Memory Usage: 748.73 MB\n",
      "Epoch 79, Batch 3200/4948, Batch loss: 0.00030667788814753294\n",
      "Memory Usage: 748.73 MB\n",
      "Epoch 79, Batch 3300/4948, Batch loss: 0.00013727876648772508\n",
      "Memory Usage: 748.73 MB\n",
      "Epoch 79, Batch 3400/4948, Batch loss: 8.701349543116521e-06\n",
      "Memory Usage: 748.73 MB\n",
      "Epoch 79, Batch 3500/4948, Batch loss: 4.60521514469292e-05\n",
      "Memory Usage: 748.75 MB\n",
      "Epoch 79, Batch 3600/4948, Batch loss: 4.0433649701299146e-05\n",
      "Memory Usage: 748.77 MB\n",
      "Epoch 79, Batch 3700/4948, Batch loss: 0.000576083199121058\n",
      "Memory Usage: 748.77 MB\n",
      "Epoch 79, Batch 3800/4948, Batch loss: 0.00010572095197858289\n",
      "Memory Usage: 749.08 MB\n",
      "Epoch 79, Batch 3900/4948, Batch loss: 0.00013569145812653005\n",
      "Memory Usage: 749.08 MB\n",
      "Epoch 79, Batch 4000/4948, Batch loss: 0.00014687454677186906\n",
      "Memory Usage: 749.12 MB\n",
      "Epoch 79, Batch 4100/4948, Batch loss: 4.838473614654504e-05\n",
      "Memory Usage: 749.12 MB\n",
      "Epoch 79, Batch 4200/4948, Batch loss: 0.0009159841574728489\n",
      "Memory Usage: 749.12 MB\n",
      "Epoch 79, Batch 4300/4948, Batch loss: 0.0001462285581510514\n",
      "Memory Usage: 749.12 MB\n",
      "Epoch 79, Batch 4400/4948, Batch loss: 3.6091112178837648e-06\n",
      "Memory Usage: 749.17 MB\n",
      "Epoch 79, Batch 4500/4948, Batch loss: 0.00086388667114079\n",
      "Memory Usage: 749.17 MB\n",
      "Epoch 79, Batch 4600/4948, Batch loss: 0.00023855084145907313\n",
      "Memory Usage: 749.17 MB\n",
      "Epoch 79, Batch 4700/4948, Batch loss: 0.0004043626831844449\n",
      "Memory Usage: 749.17 MB\n",
      "Epoch 79, Batch 4800/4948, Batch loss: 0.00011259787424933165\n",
      "Memory Usage: 749.19 MB\n",
      "Epoch 79, Batch 4900/4948, Batch loss: 6.967484659980983e-05\n",
      "Memory Usage: 751.36 MB\n",
      "Epoch 79, Batch 4948/4948, Batch loss: 0.0009197089239023626\n",
      "Memory Usage: 757.05 MB\n",
      "Epoch 79 completed in 70.21 seconds, Total Training Loss: 0.00028741407174438034\n",
      "\n",
      "Epoch 80/100\n",
      "Epoch 80, Batch 100/4948, Batch loss: 0.00016974375466816127\n",
      "Memory Usage: 769.02 MB\n",
      "Epoch 80, Batch 200/4948, Batch loss: 1.7500413378002122e-05\n",
      "Memory Usage: 780.64 MB\n",
      "Epoch 80, Batch 300/4948, Batch loss: 0.0003017110575456172\n",
      "Memory Usage: 790.75 MB\n",
      "Epoch 80, Batch 400/4948, Batch loss: 0.00011538169928826392\n",
      "Memory Usage: 790.75 MB\n",
      "Epoch 80, Batch 500/4948, Batch loss: 5.2071693062316626e-05\n",
      "Memory Usage: 790.75 MB\n",
      "Epoch 80, Batch 600/4948, Batch loss: 0.000472154380986467\n",
      "Memory Usage: 790.75 MB\n",
      "Epoch 80, Batch 700/4948, Batch loss: 8.370927389478311e-05\n",
      "Memory Usage: 790.75 MB\n",
      "Epoch 80, Batch 800/4948, Batch loss: 0.00012764746497850865\n",
      "Memory Usage: 790.75 MB\n",
      "Epoch 80, Batch 900/4948, Batch loss: 0.0004988631699234247\n",
      "Memory Usage: 790.77 MB\n",
      "Epoch 80, Batch 1000/4948, Batch loss: 8.626389899291098e-05\n",
      "Memory Usage: 790.77 MB\n",
      "Epoch 80, Batch 1100/4948, Batch loss: 0.0008324458613060415\n",
      "Memory Usage: 791.33 MB\n",
      "Epoch 80, Batch 1200/4948, Batch loss: 0.0011014750925824046\n",
      "Memory Usage: 791.33 MB\n",
      "Epoch 80, Batch 1300/4948, Batch loss: 0.00015904901374597102\n",
      "Memory Usage: 791.33 MB\n",
      "Epoch 80, Batch 1400/4948, Batch loss: 0.00012393665383569896\n",
      "Memory Usage: 791.33 MB\n",
      "Epoch 80, Batch 1500/4948, Batch loss: 7.474228186765686e-05\n",
      "Memory Usage: 791.33 MB\n",
      "Epoch 80, Batch 1600/4948, Batch loss: 1.3254992154543288e-05\n",
      "Memory Usage: 791.33 MB\n",
      "Epoch 80, Batch 1700/4948, Batch loss: 0.0003037554561160505\n",
      "Memory Usage: 791.33 MB\n",
      "Epoch 80, Batch 1800/4948, Batch loss: 0.00026083068223670125\n",
      "Memory Usage: 791.33 MB\n",
      "Epoch 80, Batch 1900/4948, Batch loss: 0.0009077470749616623\n",
      "Memory Usage: 791.33 MB\n",
      "Epoch 80, Batch 2000/4948, Batch loss: 0.0003847592743113637\n",
      "Memory Usage: 791.33 MB\n",
      "Epoch 80, Batch 2100/4948, Batch loss: 0.0006576981395483017\n",
      "Memory Usage: 791.33 MB\n",
      "Epoch 80, Batch 2200/4948, Batch loss: 0.00010339781874790788\n",
      "Memory Usage: 791.33 MB\n",
      "Epoch 80, Batch 2300/4948, Batch loss: 0.00021984230261296034\n",
      "Memory Usage: 791.44 MB\n",
      "Epoch 80, Batch 2400/4948, Batch loss: 0.00010803467012010515\n",
      "Memory Usage: 791.44 MB\n",
      "Epoch 80, Batch 2500/4948, Batch loss: 5.012690235162154e-05\n",
      "Memory Usage: 791.44 MB\n",
      "Epoch 80, Batch 2600/4948, Batch loss: 0.00037406253977678716\n",
      "Memory Usage: 791.44 MB\n",
      "Epoch 80, Batch 2700/4948, Batch loss: 0.00036210156395100057\n",
      "Memory Usage: 792.53 MB\n",
      "Epoch 80, Batch 2800/4948, Batch loss: 0.0012065863702446222\n",
      "Memory Usage: 792.53 MB\n",
      "Epoch 80, Batch 2900/4948, Batch loss: 0.00020212927483953536\n",
      "Memory Usage: 792.53 MB\n",
      "Epoch 80, Batch 3000/4948, Batch loss: 5.953514846623875e-05\n",
      "Memory Usage: 792.55 MB\n",
      "Epoch 80, Batch 3100/4948, Batch loss: 0.00012311952013988048\n",
      "Memory Usage: 792.55 MB\n",
      "Epoch 80, Batch 3200/4948, Batch loss: 0.00029316204017959535\n",
      "Memory Usage: 792.55 MB\n",
      "Epoch 80, Batch 3300/4948, Batch loss: 0.00013987165584694594\n",
      "Memory Usage: 792.55 MB\n",
      "Epoch 80, Batch 3400/4948, Batch loss: 7.7846252679592e-06\n",
      "Memory Usage: 792.55 MB\n",
      "Epoch 80, Batch 3500/4948, Batch loss: 4.5290809794096276e-05\n",
      "Memory Usage: 792.55 MB\n",
      "Epoch 80, Batch 3600/4948, Batch loss: 4.1040562791749835e-05\n",
      "Memory Usage: 792.55 MB\n",
      "Epoch 80, Batch 3700/4948, Batch loss: 0.0006277539650909603\n",
      "Memory Usage: 792.56 MB\n",
      "Epoch 80, Batch 3800/4948, Batch loss: 0.00010566338460193947\n",
      "Memory Usage: 792.56 MB\n",
      "Epoch 80, Batch 3900/4948, Batch loss: 0.00013708586629945785\n",
      "Memory Usage: 792.56 MB\n",
      "Epoch 80, Batch 4000/4948, Batch loss: 0.0001474447053624317\n",
      "Memory Usage: 792.56 MB\n",
      "Epoch 80, Batch 4100/4948, Batch loss: 4.773406544700265e-05\n",
      "Memory Usage: 792.56 MB\n",
      "Epoch 80, Batch 4200/4948, Batch loss: 0.0009047670755535364\n",
      "Memory Usage: 792.56 MB\n",
      "Epoch 80, Batch 4300/4948, Batch loss: 0.00014628829376306385\n",
      "Memory Usage: 792.56 MB\n",
      "Epoch 80, Batch 4400/4948, Batch loss: 3.5916380056733033e-06\n",
      "Memory Usage: 792.56 MB\n",
      "Epoch 80, Batch 4500/4948, Batch loss: 0.0008484393474645913\n",
      "Memory Usage: 792.92 MB\n",
      "Epoch 80, Batch 4600/4948, Batch loss: 0.00023799740301910788\n",
      "Memory Usage: 793.77 MB\n",
      "Epoch 80, Batch 4700/4948, Batch loss: 0.0004091280570719391\n",
      "Memory Usage: 793.77 MB\n",
      "Epoch 80, Batch 4800/4948, Batch loss: 0.00011191438534297049\n",
      "Memory Usage: 793.77 MB\n",
      "Epoch 80, Batch 4900/4948, Batch loss: 7.022565841907635e-05\n",
      "Memory Usage: 793.77 MB\n",
      "Epoch 80, Batch 4948/4948, Batch loss: 0.0009224237874150276\n",
      "Memory Usage: 793.77 MB\n",
      "Epoch 80 completed in 68.94 seconds, Total Training Loss: 0.0002765192723630712\n",
      "\n",
      "Epoch 81/100\n",
      "Epoch 81, Batch 100/4948, Batch loss: 0.00017065316205844283\n",
      "Memory Usage: 793.77 MB\n",
      "Epoch 81, Batch 200/4948, Batch loss: 1.7991591448662803e-05\n",
      "Memory Usage: 793.77 MB\n",
      "Epoch 81, Batch 300/4948, Batch loss: 0.00030194653663784266\n",
      "Memory Usage: 793.77 MB\n",
      "Epoch 81, Batch 400/4948, Batch loss: 0.00011405297846067697\n",
      "Memory Usage: 791.98 MB\n",
      "Epoch 81, Batch 500/4948, Batch loss: 5.127770782564767e-05\n",
      "Memory Usage: 793.77 MB\n",
      "Epoch 81, Batch 600/4948, Batch loss: 0.0004693310474976897\n",
      "Memory Usage: 793.77 MB\n",
      "Epoch 81, Batch 700/4948, Batch loss: 5.637493813992478e-05\n",
      "Memory Usage: 793.77 MB\n",
      "Epoch 81, Batch 800/4948, Batch loss: 0.0001250652567250654\n",
      "Memory Usage: 793.78 MB\n",
      "Epoch 81, Batch 900/4948, Batch loss: 0.0005121241556480527\n",
      "Memory Usage: 793.78 MB\n",
      "Epoch 81, Batch 1000/4948, Batch loss: 8.555482054362074e-05\n",
      "Memory Usage: 793.78 MB\n",
      "Epoch 81, Batch 1100/4948, Batch loss: 0.0008246779907494783\n",
      "Memory Usage: 793.78 MB\n",
      "Epoch 81, Batch 1200/4948, Batch loss: 0.0011211021337658167\n",
      "Memory Usage: 793.94 MB\n",
      "Epoch 81, Batch 1300/4948, Batch loss: 0.0001510307629359886\n",
      "Memory Usage: 794.56 MB\n",
      "Epoch 81, Batch 1400/4948, Batch loss: 0.00012194018927402794\n",
      "Memory Usage: 794.56 MB\n",
      "Epoch 81, Batch 1500/4948, Batch loss: 7.427155651384965e-05\n",
      "Memory Usage: 794.56 MB\n",
      "Epoch 81, Batch 1600/4948, Batch loss: 1.4220495359040797e-05\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 81, Batch 1700/4948, Batch loss: 0.0004289036151021719\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 81, Batch 1800/4948, Batch loss: 0.0002597627171780914\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 81, Batch 1900/4948, Batch loss: 0.0019822423346340656\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 81, Batch 2000/4948, Batch loss: 0.0003827796899713576\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 81, Batch 2100/4948, Batch loss: 0.0006507107173092663\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 81, Batch 2200/4948, Batch loss: 0.0001046203396981582\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 81, Batch 2300/4948, Batch loss: 0.00021968872169964015\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 81, Batch 2400/4948, Batch loss: 0.0001053044616128318\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 81, Batch 2500/4948, Batch loss: 5.0108930736314505e-05\n",
      "Memory Usage: 794.58 MB\n",
      "Epoch 81, Batch 2600/4948, Batch loss: 0.0003763898857869208\n",
      "Memory Usage: 794.59 MB\n",
      "Epoch 81, Batch 2700/4948, Batch loss: 0.0003732980403583497\n",
      "Memory Usage: 794.59 MB\n",
      "Epoch 81, Batch 2800/4948, Batch loss: 0.0012265278492122889\n",
      "Memory Usage: 794.59 MB\n",
      "Epoch 81, Batch 2900/4948, Batch loss: 0.00020057572692167014\n",
      "Memory Usage: 794.59 MB\n",
      "Epoch 81, Batch 3000/4948, Batch loss: 5.893253910471685e-05\n",
      "Memory Usage: 794.59 MB\n",
      "Epoch 81, Batch 3100/4948, Batch loss: 0.00012253742897883058\n",
      "Memory Usage: 794.59 MB\n",
      "Epoch 81, Batch 3200/4948, Batch loss: 0.000305265566566959\n",
      "Memory Usage: 794.59 MB\n",
      "Epoch 81, Batch 3300/4948, Batch loss: 0.00013834975834470242\n",
      "Memory Usage: 794.59 MB\n",
      "Epoch 81, Batch 3400/4948, Batch loss: 9.856054930423852e-06\n",
      "Memory Usage: 794.59 MB\n",
      "Epoch 81, Batch 3500/4948, Batch loss: 4.717083356808871e-05\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 81, Batch 3600/4948, Batch loss: 4.108931898372248e-05\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 81, Batch 3700/4948, Batch loss: 0.0005739982007071376\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 81, Batch 3800/4948, Batch loss: 0.00010906178795266896\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 81, Batch 3900/4948, Batch loss: 0.00015148929378483444\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 81, Batch 4000/4948, Batch loss: 0.00014841422671452165\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 81, Batch 4100/4948, Batch loss: 4.538178473012522e-05\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 81, Batch 4200/4948, Batch loss: 0.0008961797575466335\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 81, Batch 4300/4948, Batch loss: 0.00014614844985771924\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 81, Batch 4400/4948, Batch loss: 3.492069026833633e-06\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 81, Batch 4500/4948, Batch loss: 0.0008618352003395557\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 81, Batch 4600/4948, Batch loss: 0.00023321958724409342\n",
      "Memory Usage: 794.94 MB\n",
      "Epoch 81, Batch 4700/4948, Batch loss: 0.00040452671237289906\n",
      "Memory Usage: 794.95 MB\n",
      "Epoch 81, Batch 4800/4948, Batch loss: 0.00011193813406862319\n",
      "Memory Usage: 794.98 MB\n",
      "Epoch 81, Batch 4900/4948, Batch loss: 6.94619448040612e-05\n",
      "Memory Usage: 794.98 MB\n",
      "Epoch 81, Batch 4948/4948, Batch loss: 0.000920899910852313\n",
      "Memory Usage: 794.98 MB\n",
      "Epoch 81 completed in 69.42 seconds, Total Training Loss: 0.0002869123875635088\n",
      "Validation completed in 3.53 seconds, Average Validation Loss: 0.0004867953417208358\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 82/100\n",
      "Epoch 82, Batch 100/4948, Batch loss: 0.00016954541206359863\n",
      "Memory Usage: 926.19 MB\n",
      "Epoch 82, Batch 200/4948, Batch loss: 2.070138907583896e-05\n",
      "Memory Usage: 926.67 MB\n",
      "Epoch 82, Batch 300/4948, Batch loss: 0.0003012788947671652\n",
      "Memory Usage: 926.70 MB\n",
      "Epoch 82, Batch 400/4948, Batch loss: 0.00011254253331571817\n",
      "Memory Usage: 926.70 MB\n",
      "Epoch 82, Batch 500/4948, Batch loss: 5.124283779878169e-05\n",
      "Memory Usage: 926.72 MB\n",
      "Epoch 82, Batch 600/4948, Batch loss: 0.00047374010318890214\n",
      "Memory Usage: 926.73 MB\n",
      "Epoch 82, Batch 700/4948, Batch loss: 5.354428867576644e-05\n",
      "Memory Usage: 926.66 MB\n",
      "Epoch 82, Batch 800/4948, Batch loss: 0.00012723983672913164\n",
      "Memory Usage: 926.66 MB\n",
      "Epoch 82, Batch 900/4948, Batch loss: 0.0005245347856543958\n",
      "Memory Usage: 926.73 MB\n",
      "Epoch 82, Batch 1000/4948, Batch loss: 8.522826101398095e-05\n",
      "Memory Usage: 927.73 MB\n",
      "Epoch 82, Batch 1100/4948, Batch loss: 0.0007806202629581094\n",
      "Memory Usage: 927.94 MB\n",
      "Epoch 82, Batch 1200/4948, Batch loss: 0.0011057191295549273\n",
      "Memory Usage: 927.94 MB\n",
      "Epoch 82, Batch 1300/4948, Batch loss: 0.0001473146112402901\n",
      "Memory Usage: 928.16 MB\n",
      "Epoch 82, Batch 1400/4948, Batch loss: 0.00012172331480542198\n",
      "Memory Usage: 928.94 MB\n",
      "Epoch 82, Batch 1500/4948, Batch loss: 7.474660378647968e-05\n",
      "Memory Usage: 928.94 MB\n",
      "Epoch 82, Batch 1600/4948, Batch loss: 1.3709174709219951e-05\n",
      "Memory Usage: 929.41 MB\n",
      "Epoch 82, Batch 1700/4948, Batch loss: 0.00030403403798118234\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 82, Batch 1800/4948, Batch loss: 0.00025927380193024874\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 82, Batch 1900/4948, Batch loss: 0.00212091114372015\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 82, Batch 2000/4948, Batch loss: 0.000394652655813843\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 82, Batch 2100/4948, Batch loss: 0.0006526641664095223\n",
      "Memory Usage: 929.00 MB\n",
      "Epoch 82, Batch 2200/4948, Batch loss: 0.00010439397738082334\n",
      "Memory Usage: 929.44 MB\n",
      "Epoch 82, Batch 2300/4948, Batch loss: 0.0002235822903458029\n",
      "Memory Usage: 929.73 MB\n",
      "Epoch 82, Batch 2400/4948, Batch loss: 0.00010471022687852383\n",
      "Memory Usage: 929.78 MB\n",
      "Epoch 82, Batch 2500/4948, Batch loss: 4.8259847972076386e-05\n",
      "Memory Usage: 929.81 MB\n",
      "Epoch 82, Batch 2600/4948, Batch loss: 0.000376120995497331\n",
      "Memory Usage: 929.81 MB\n",
      "Epoch 82, Batch 2700/4948, Batch loss: 0.00035481451777741313\n",
      "Memory Usage: 929.81 MB\n",
      "Epoch 82, Batch 2800/4948, Batch loss: 0.0011670577805489302\n",
      "Memory Usage: 929.83 MB\n",
      "Epoch 82, Batch 2900/4948, Batch loss: 0.00020157733524683863\n",
      "Memory Usage: 929.86 MB\n",
      "Epoch 82, Batch 3000/4948, Batch loss: 5.74947553104721e-05\n",
      "Memory Usage: 929.86 MB\n",
      "Epoch 82, Batch 3100/4948, Batch loss: 0.00012435078679118305\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 82, Batch 3200/4948, Batch loss: 0.00028423010371625423\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 82, Batch 3300/4948, Batch loss: 0.0001382401242153719\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 82, Batch 3400/4948, Batch loss: 8.788115337665658e-06\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 82, Batch 3500/4948, Batch loss: 4.702894511865452e-05\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 82, Batch 3600/4948, Batch loss: 4.071960938745178e-05\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 82, Batch 3700/4948, Batch loss: 0.0005617915885522962\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 82, Batch 3800/4948, Batch loss: 0.00010592352191451937\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 82, Batch 3900/4948, Batch loss: 0.0001484066597186029\n",
      "Memory Usage: 929.89 MB\n",
      "Epoch 82, Batch 4000/4948, Batch loss: 0.0001461685315007344\n",
      "Memory Usage: 929.89 MB\n",
      "Epoch 82, Batch 4100/4948, Batch loss: 4.475788591662422e-05\n",
      "Memory Usage: 929.89 MB\n",
      "Epoch 82, Batch 4200/4948, Batch loss: 0.0008984640007838607\n",
      "Memory Usage: 929.89 MB\n",
      "Epoch 82, Batch 4300/4948, Batch loss: 0.00014490670582745224\n",
      "Memory Usage: 929.89 MB\n",
      "Epoch 82, Batch 4400/4948, Batch loss: 4.049234121339396e-06\n",
      "Memory Usage: 929.89 MB\n",
      "Epoch 82, Batch 4500/4948, Batch loss: 0.0008294419967569411\n",
      "Memory Usage: 929.89 MB\n",
      "Epoch 82, Batch 4600/4948, Batch loss: 0.00023533405328635126\n",
      "Memory Usage: 929.89 MB\n",
      "Epoch 82, Batch 4700/4948, Batch loss: 0.0004232152714394033\n",
      "Memory Usage: 929.89 MB\n",
      "Epoch 82, Batch 4800/4948, Batch loss: 0.00012554656132124364\n",
      "Memory Usage: 929.89 MB\n",
      "Epoch 82, Batch 4900/4948, Batch loss: 7.487585389753804e-05\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 82, Batch 4948/4948, Batch loss: 0.0009515161509625614\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 82 completed in 64.56 seconds, Total Training Loss: 0.0002803092741143488\n",
      "\n",
      "Epoch 83/100\n",
      "Epoch 83, Batch 100/4948, Batch loss: 0.0001699557324172929\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 200/4948, Batch loss: 1.1411512787162792e-05\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 300/4948, Batch loss: 0.00030209094984456897\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 400/4948, Batch loss: 0.00011312423157505691\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 500/4948, Batch loss: 5.1201306632719934e-05\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 600/4948, Batch loss: 0.00047263846499845386\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 700/4948, Batch loss: 5.155711551196873e-05\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 800/4948, Batch loss: 0.00012998099555261433\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 900/4948, Batch loss: 0.0005318272160366178\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 1000/4948, Batch loss: 8.622480527264997e-05\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 1100/4948, Batch loss: 0.0007980773807503283\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 1200/4948, Batch loss: 0.0011099799303337932\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 1300/4948, Batch loss: 0.00014677805302198976\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 1400/4948, Batch loss: 0.0001237073156516999\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 1500/4948, Batch loss: 7.273494702531025e-05\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 1600/4948, Batch loss: 1.5793108104844578e-05\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 1700/4948, Batch loss: 0.00030910890200175345\n",
      "Memory Usage: 929.91 MB\n",
      "Epoch 83, Batch 1800/4948, Batch loss: 0.0002612565294839442\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 1900/4948, Batch loss: 0.0009096831199713051\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 2000/4948, Batch loss: 0.0003741648397408426\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 2100/4948, Batch loss: 0.0006389992195181549\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 83, Batch 2200/4948, Batch loss: 0.00010323442256776616\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 83, Batch 2300/4948, Batch loss: 0.00022476637968793511\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 83, Batch 2400/4948, Batch loss: 0.00010591318277874961\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 83, Batch 2500/4948, Batch loss: 4.8622623580740765e-05\n",
      "Memory Usage: 929.88 MB\n",
      "Epoch 83, Batch 2600/4948, Batch loss: 0.0003723291156347841\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 2700/4948, Batch loss: 0.0003820719721261412\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 2800/4948, Batch loss: 0.0011680827010422945\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 2900/4948, Batch loss: 0.0002028125018114224\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 3000/4948, Batch loss: 5.977246109978296e-05\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 3100/4948, Batch loss: 0.0001231774513144046\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 3200/4948, Batch loss: 0.00028835685225203633\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 3300/4948, Batch loss: 0.00014006531273480505\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 3400/4948, Batch loss: 5.860242254129844e-06\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 3500/4948, Batch loss: 4.960952355759218e-05\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 3600/4948, Batch loss: 4.000000990345143e-05\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 3700/4948, Batch loss: 0.0005998457781970501\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 3800/4948, Batch loss: 0.00010492825822439045\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 3900/4948, Batch loss: 0.00014462570834439248\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 83, Batch 4000/4948, Batch loss: 0.00014859075599815696\n",
      "Memory Usage: 929.95 MB\n",
      "Epoch 83, Batch 4100/4948, Batch loss: 4.623207132681273e-05\n",
      "Memory Usage: 929.95 MB\n",
      "Epoch 83, Batch 4200/4948, Batch loss: 0.0008899078820832074\n",
      "Memory Usage: 929.97 MB\n",
      "Epoch 83, Batch 4300/4948, Batch loss: 0.0001454199809813872\n",
      "Memory Usage: 929.97 MB\n",
      "Epoch 83, Batch 4400/4948, Batch loss: 4.298443855077494e-06\n",
      "Memory Usage: 929.97 MB\n",
      "Epoch 83, Batch 4500/4948, Batch loss: 0.000843519635964185\n",
      "Memory Usage: 929.98 MB\n",
      "Epoch 83, Batch 4600/4948, Batch loss: 0.00023515618522651494\n",
      "Memory Usage: 929.98 MB\n",
      "Epoch 83, Batch 4700/4948, Batch loss: 0.0004091066075488925\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 83, Batch 4800/4948, Batch loss: 0.0001228271285071969\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 83, Batch 4900/4948, Batch loss: 7.823002670193091e-05\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 83, Batch 4948/4948, Batch loss: 0.0009085905621759593\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 83 completed in 64.90 seconds, Total Training Loss: 0.0002763548249652692\n",
      "\n",
      "Epoch 84/100\n",
      "Epoch 84, Batch 100/4948, Batch loss: 0.0001684625312918797\n",
      "Memory Usage: 930.02 MB\n",
      "Epoch 84, Batch 200/4948, Batch loss: 1.0885780284297653e-05\n",
      "Memory Usage: 930.02 MB\n",
      "Epoch 84, Batch 300/4948, Batch loss: 0.000300981686450541\n",
      "Memory Usage: 929.36 MB\n",
      "Epoch 84, Batch 400/4948, Batch loss: 0.00011731052654795349\n",
      "Memory Usage: 929.36 MB\n",
      "Epoch 84, Batch 500/4948, Batch loss: 5.137899279361591e-05\n",
      "Memory Usage: 929.36 MB\n",
      "Epoch 84, Batch 600/4948, Batch loss: 0.00046974033466540277\n",
      "Memory Usage: 929.36 MB\n",
      "Epoch 84, Batch 700/4948, Batch loss: 5.215526471147314e-05\n",
      "Memory Usage: 929.36 MB\n",
      "Epoch 84, Batch 800/4948, Batch loss: 0.0001349380036117509\n",
      "Memory Usage: 929.36 MB\n",
      "Epoch 84, Batch 900/4948, Batch loss: 0.0005343254888430238\n",
      "Memory Usage: 929.48 MB\n",
      "Epoch 84, Batch 1000/4948, Batch loss: 8.610096119809896e-05\n",
      "Memory Usage: 929.48 MB\n",
      "Epoch 84, Batch 1100/4948, Batch loss: 0.0008000846137292683\n",
      "Memory Usage: 929.48 MB\n",
      "Epoch 84, Batch 1200/4948, Batch loss: 0.0010903767542913556\n",
      "Memory Usage: 929.48 MB\n",
      "Epoch 84, Batch 1300/4948, Batch loss: 0.00014993196236900985\n",
      "Memory Usage: 929.48 MB\n",
      "Epoch 84, Batch 1400/4948, Batch loss: 0.00012306822463870049\n",
      "Memory Usage: 929.48 MB\n",
      "Epoch 84, Batch 1500/4948, Batch loss: 7.343231845879927e-05\n",
      "Memory Usage: 929.48 MB\n",
      "Epoch 84, Batch 1600/4948, Batch loss: 1.3469866644300055e-05\n",
      "Memory Usage: 929.27 MB\n",
      "Epoch 84, Batch 1700/4948, Batch loss: 0.0003507286310195923\n",
      "Memory Usage: 929.47 MB\n",
      "Epoch 84, Batch 1800/4948, Batch loss: 0.0002611056843306869\n",
      "Memory Usage: 929.66 MB\n",
      "Epoch 84, Batch 1900/4948, Batch loss: 0.003236936405301094\n",
      "Memory Usage: 929.66 MB\n",
      "Epoch 84, Batch 2000/4948, Batch loss: 0.00038645751192234457\n",
      "Memory Usage: 929.66 MB\n",
      "Epoch 84, Batch 2100/4948, Batch loss: 0.0006208450649864972\n",
      "Memory Usage: 929.66 MB\n",
      "Epoch 84, Batch 2200/4948, Batch loss: 0.00010597716027405113\n",
      "Memory Usage: 929.66 MB\n",
      "Epoch 84, Batch 2300/4948, Batch loss: 0.00022099680791143328\n",
      "Memory Usage: 929.72 MB\n",
      "Epoch 84, Batch 2400/4948, Batch loss: 0.00010627417213981971\n",
      "Memory Usage: 929.72 MB\n",
      "Epoch 84, Batch 2500/4948, Batch loss: 4.954415271640755e-05\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 2600/4948, Batch loss: 0.00037412374513223767\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 2700/4948, Batch loss: 0.00035297730937600136\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 2800/4948, Batch loss: 0.0011970499763265252\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 2900/4948, Batch loss: 0.00019923278887290508\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 3000/4948, Batch loss: 5.7435507187619805e-05\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 3100/4948, Batch loss: 0.00012964960478711873\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 3200/4948, Batch loss: 0.0002929729816969484\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 3300/4948, Batch loss: 0.00013709496124647558\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 3400/4948, Batch loss: 6.994771865720395e-06\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 3500/4948, Batch loss: 4.7618563257856295e-05\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 3600/4948, Batch loss: 3.969887620769441e-05\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 3700/4948, Batch loss: 0.0005851298337802291\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 3800/4948, Batch loss: 0.00010551404557190835\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 3900/4948, Batch loss: 0.00015378237003460526\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 4000/4948, Batch loss: 0.0001461382198613137\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 4100/4948, Batch loss: 4.753014582092874e-05\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 4200/4948, Batch loss: 0.0009018653654493392\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 4300/4948, Batch loss: 0.0001468924747314304\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 4400/4948, Batch loss: 3.806128233918571e-06\n",
      "Memory Usage: 930.00 MB\n",
      "Epoch 84, Batch 4500/4948, Batch loss: 0.0008292390848509967\n",
      "Memory Usage: 930.02 MB\n",
      "Epoch 84, Batch 4600/4948, Batch loss: 0.000232774400501512\n",
      "Memory Usage: 930.02 MB\n",
      "Epoch 84, Batch 4700/4948, Batch loss: 0.00041042969678528607\n",
      "Memory Usage: 930.02 MB\n",
      "Epoch 84, Batch 4800/4948, Batch loss: 0.0001191896153613925\n",
      "Memory Usage: 930.02 MB\n",
      "Epoch 84, Batch 4900/4948, Batch loss: 7.716250547673553e-05\n",
      "Memory Usage: 930.02 MB\n",
      "Epoch 84, Batch 4948/4948, Batch loss: 0.0009052040986716747\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 84 completed in 64.07 seconds, Total Training Loss: 0.00028351113796158624\n",
      "\n",
      "Epoch 85/100\n",
      "Epoch 85, Batch 100/4948, Batch loss: 0.00016774109099060297\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 200/4948, Batch loss: 1.065843207470607e-05\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 300/4948, Batch loss: 0.0002996576367877424\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 400/4948, Batch loss: 0.00011294188880128786\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 500/4948, Batch loss: 5.153836536919698e-05\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 600/4948, Batch loss: 0.00046745024155825377\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 700/4948, Batch loss: 6.232710438780487e-05\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 800/4948, Batch loss: 0.00013085863611195236\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 900/4948, Batch loss: 0.0004998850054107606\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 1000/4948, Batch loss: 8.783173689153045e-05\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 1100/4948, Batch loss: 0.0007993131293915212\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 1200/4948, Batch loss: 0.0010977338533848524\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 1300/4948, Batch loss: 0.000152094493387267\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 1400/4948, Batch loss: 0.00012420689745340496\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 1500/4948, Batch loss: 7.265517342602834e-05\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 1600/4948, Batch loss: 1.5878349586273544e-05\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 1700/4948, Batch loss: 0.00030801311368122697\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 1800/4948, Batch loss: 0.0002590247895568609\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 1900/4948, Batch loss: 0.002072088187560439\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 2000/4948, Batch loss: 0.00039480271516367793\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 2100/4948, Batch loss: 0.0006477523711510003\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 2200/4948, Batch loss: 0.00010404954809928313\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 2300/4948, Batch loss: 0.00021752755856141448\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 2400/4948, Batch loss: 0.00010745330655481666\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 2500/4948, Batch loss: 4.8553272790741175e-05\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 2600/4948, Batch loss: 0.0003751856565941125\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 2700/4948, Batch loss: 0.000357597105903551\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 2800/4948, Batch loss: 0.0011575235985219479\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 2900/4948, Batch loss: 0.0001976682833628729\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 3000/4948, Batch loss: 5.853074253536761e-05\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 3100/4948, Batch loss: 0.00012678363418672234\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 3200/4948, Batch loss: 0.0002804503310471773\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 3300/4948, Batch loss: 0.00013918611512053758\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 3400/4948, Batch loss: 9.99211442831438e-06\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 3500/4948, Batch loss: 4.745228579849936e-05\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 3600/4948, Batch loss: 4.029120464110747e-05\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 3700/4948, Batch loss: 0.0005660241586156189\n",
      "Memory Usage: 930.03 MB\n",
      "Epoch 85, Batch 3800/4948, Batch loss: 0.0001053103624144569\n",
      "Memory Usage: 915.97 MB\n",
      "Epoch 85, Batch 3900/4948, Batch loss: 0.0001340672024525702\n",
      "Memory Usage: 922.59 MB\n",
      "Epoch 85, Batch 4000/4948, Batch loss: 0.0001468142436351627\n",
      "Memory Usage: 925.09 MB\n",
      "Epoch 85, Batch 4100/4948, Batch loss: 4.618531238520518e-05\n",
      "Memory Usage: 925.38 MB\n",
      "Epoch 85, Batch 4200/4948, Batch loss: 0.000891053001396358\n",
      "Memory Usage: 926.06 MB\n",
      "Epoch 85, Batch 4300/4948, Batch loss: 0.00014588236808776855\n",
      "Memory Usage: 926.06 MB\n",
      "Epoch 85, Batch 4400/4948, Batch loss: 3.735981408681255e-06\n",
      "Memory Usage: 926.06 MB\n",
      "Epoch 85, Batch 4500/4948, Batch loss: 0.0008439444354735315\n",
      "Memory Usage: 926.55 MB\n",
      "Epoch 85, Batch 4600/4948, Batch loss: 0.00023759773466736078\n",
      "Memory Usage: 926.55 MB\n",
      "Epoch 85, Batch 4700/4948, Batch loss: 0.0004075399483554065\n",
      "Memory Usage: 926.55 MB\n",
      "Epoch 85, Batch 4800/4948, Batch loss: 0.0001227418688358739\n",
      "Memory Usage: 926.55 MB\n",
      "Epoch 85, Batch 4900/4948, Batch loss: 7.295292743947357e-05\n",
      "Memory Usage: 926.55 MB\n",
      "Epoch 85, Batch 4948/4948, Batch loss: 0.0009263491374440491\n",
      "Memory Usage: 926.55 MB\n",
      "Epoch 85 completed in 64.85 seconds, Total Training Loss: 0.0002784343680548026\n",
      "\n",
      "Epoch 86/100\n",
      "Epoch 86, Batch 100/4948, Batch loss: 0.00016900977061595768\n",
      "Memory Usage: 926.55 MB\n",
      "Epoch 86, Batch 200/4948, Batch loss: 1.4213804206519853e-05\n",
      "Memory Usage: 926.56 MB\n",
      "Epoch 86, Batch 300/4948, Batch loss: 0.00029687557253055274\n",
      "Memory Usage: 926.56 MB\n",
      "Epoch 86, Batch 400/4948, Batch loss: 0.00011149653437314555\n",
      "Memory Usage: 926.56 MB\n",
      "Epoch 86, Batch 500/4948, Batch loss: 5.088058242108673e-05\n",
      "Memory Usage: 926.56 MB\n",
      "Epoch 86, Batch 600/4948, Batch loss: 0.00048004495329223573\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 700/4948, Batch loss: 5.753600999014452e-05\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 800/4948, Batch loss: 0.00012522944598458707\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 900/4948, Batch loss: 0.0005326932296156883\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 1000/4948, Batch loss: 8.61881417222321e-05\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 1100/4948, Batch loss: 0.0007922510267235339\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 1200/4948, Batch loss: 0.0010646713199093938\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 1300/4948, Batch loss: 0.00015228643314912915\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 1400/4948, Batch loss: 0.00012176002201158553\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 1500/4948, Batch loss: 7.412203558487818e-05\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 1600/4948, Batch loss: 1.3411571671895217e-05\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 1700/4948, Batch loss: 0.0002975742390844971\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 1800/4948, Batch loss: 0.00025851739337667823\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 1900/4948, Batch loss: 0.0025860366877168417\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 2000/4948, Batch loss: 0.00038175846566446126\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 2100/4948, Batch loss: 0.0006212738226167858\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 2200/4948, Batch loss: 0.00010433900752104819\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 2300/4948, Batch loss: 0.00021808332530781627\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 2400/4948, Batch loss: 0.00010573863255558535\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 2500/4948, Batch loss: 4.831548721995205e-05\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 2600/4948, Batch loss: 0.00037880337913520634\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 2700/4948, Batch loss: 0.00035140596446581185\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 2800/4948, Batch loss: 0.0011838952777907252\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 2900/4948, Batch loss: 0.00019963074009865522\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 3000/4948, Batch loss: 5.803058229503222e-05\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 3100/4948, Batch loss: 0.00012351064651738852\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 3200/4948, Batch loss: 0.00028268140158616006\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 3300/4948, Batch loss: 0.00013955164467915893\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 3400/4948, Batch loss: 5.758699899161002e-06\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 3500/4948, Batch loss: 4.764585173688829e-05\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 3600/4948, Batch loss: 3.9894435758469626e-05\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 3700/4948, Batch loss: 0.0005532094510272145\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 3800/4948, Batch loss: 0.00010562625539023429\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 3900/4948, Batch loss: 0.00013867561938241124\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 4000/4948, Batch loss: 0.00014773342991247773\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 4100/4948, Batch loss: 4.457376053323969e-05\n",
      "Memory Usage: 926.69 MB\n",
      "Epoch 86, Batch 4200/4948, Batch loss: 0.0008860051748342812\n",
      "Memory Usage: 926.83 MB\n",
      "Epoch 86, Batch 4300/4948, Batch loss: 0.0001461527863284573\n",
      "Memory Usage: 926.83 MB\n",
      "Epoch 86, Batch 4400/4948, Batch loss: 3.6290878142608562e-06\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 86, Batch 4500/4948, Batch loss: 0.0008345730020664632\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 86, Batch 4600/4948, Batch loss: 0.00023478054208680987\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 86, Batch 4700/4948, Batch loss: 0.0004103665123693645\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 86, Batch 4800/4948, Batch loss: 0.00013058006879873574\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 86, Batch 4900/4948, Batch loss: 7.319753058254719e-05\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 86, Batch 4948/4948, Batch loss: 0.0008964220760390162\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 86 completed in 64.69 seconds, Total Training Loss: 0.00028112228765692185\n",
      "\n",
      "Epoch 87/100\n",
      "Epoch 87, Batch 100/4948, Batch loss: 0.00016942803631536663\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 200/4948, Batch loss: 1.0539929462538566e-05\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 300/4948, Batch loss: 0.00030289895948953927\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 400/4948, Batch loss: 0.0001169452298199758\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 500/4948, Batch loss: 5.1439412345644087e-05\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 600/4948, Batch loss: 0.00047546159476041794\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 700/4948, Batch loss: 5.474213685374707e-05\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 800/4948, Batch loss: 0.00012580968905240297\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 900/4948, Batch loss: 0.0005020812386646867\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 1000/4948, Batch loss: 8.533464279025793e-05\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 1100/4948, Batch loss: 0.0008058846578933299\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 1200/4948, Batch loss: 0.001049733255058527\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 1300/4948, Batch loss: 0.00014881318202242255\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 1400/4948, Batch loss: 0.00012134997814428061\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 1500/4948, Batch loss: 7.275131065398455e-05\n",
      "Memory Usage: 926.84 MB\n",
      "Epoch 87, Batch 1600/4948, Batch loss: 1.3727488294534851e-05\n",
      "Memory Usage: 925.56 MB\n",
      "Epoch 87, Batch 1700/4948, Batch loss: 0.00034568895353004336\n",
      "Memory Usage: 926.09 MB\n",
      "Epoch 87, Batch 1800/4948, Batch loss: 0.00025940954219549894\n",
      "Memory Usage: 926.23 MB\n",
      "Epoch 87, Batch 1900/4948, Batch loss: 0.0038570223841816187\n",
      "Memory Usage: 926.30 MB\n",
      "Epoch 87, Batch 2000/4948, Batch loss: 0.000411275279475376\n",
      "Memory Usage: 926.64 MB\n",
      "Epoch 87, Batch 2100/4948, Batch loss: 0.0006742340046912432\n",
      "Memory Usage: 926.64 MB\n",
      "Epoch 87, Batch 2200/4948, Batch loss: 0.00010378034494351596\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 2300/4948, Batch loss: 0.00022117084881756455\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 2400/4948, Batch loss: 0.0001055992252076976\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 2500/4948, Batch loss: 4.8613066610414535e-05\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 2600/4948, Batch loss: 0.00037393742240965366\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 2700/4948, Batch loss: 0.0003572012938093394\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 2800/4948, Batch loss: 0.0011759598273783922\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 2900/4948, Batch loss: 0.0001976274506887421\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 3000/4948, Batch loss: 5.7646520872367546e-05\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 3100/4948, Batch loss: 0.00012185353989480063\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 3200/4948, Batch loss: 0.0003103180497419089\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 3300/4948, Batch loss: 0.0001387048396281898\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 3400/4948, Batch loss: 8.028311640373431e-06\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 3500/4948, Batch loss: 4.516577610047534e-05\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 3600/4948, Batch loss: 4.069103306392208e-05\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 3700/4948, Batch loss: 0.00056362000759691\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 3800/4948, Batch loss: 0.00010551737068453804\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 3900/4948, Batch loss: 0.0001510090660303831\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 4000/4948, Batch loss: 0.00014639493019785732\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 4100/4948, Batch loss: 4.6324905270012096e-05\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 4200/4948, Batch loss: 0.0008917740196920931\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 4300/4948, Batch loss: 0.0001465895038563758\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 4400/4948, Batch loss: 3.614172555899131e-06\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 4500/4948, Batch loss: 0.0008210382075048983\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 4600/4948, Batch loss: 0.00023346638772636652\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 4700/4948, Batch loss: 0.00041031299042515457\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 4800/4948, Batch loss: 0.00012120589963160455\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 4900/4948, Batch loss: 7.841719343559816e-05\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87, Batch 4948/4948, Batch loss: 0.0008655229466967285\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 87 completed in 64.50 seconds, Total Training Loss: 0.0002832872815319381\n",
      "\n",
      "Epoch 88/100\n",
      "Epoch 88, Batch 100/4948, Batch loss: 0.00016981280350591987\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 88, Batch 200/4948, Batch loss: 1.0577421562629752e-05\n",
      "Memory Usage: 926.77 MB\n",
      "Epoch 88, Batch 300/4948, Batch loss: 0.0003009155043400824\n",
      "Memory Usage: 927.45 MB\n",
      "Epoch 88, Batch 400/4948, Batch loss: 0.00011708949750754982\n",
      "Memory Usage: 927.45 MB\n",
      "Epoch 88, Batch 500/4948, Batch loss: 5.087501631351188e-05\n",
      "Memory Usage: 927.45 MB\n",
      "Epoch 88, Batch 600/4948, Batch loss: 0.0004708057676907629\n",
      "Memory Usage: 927.45 MB\n",
      "Epoch 88, Batch 700/4948, Batch loss: 6.891792872920632e-05\n",
      "Memory Usage: 927.45 MB\n",
      "Epoch 88, Batch 800/4948, Batch loss: 0.0001281391887459904\n",
      "Memory Usage: 927.45 MB\n",
      "Epoch 88, Batch 900/4948, Batch loss: 0.0005344912060536444\n",
      "Memory Usage: 927.45 MB\n",
      "Epoch 88, Batch 1000/4948, Batch loss: 8.522161078872159e-05\n",
      "Memory Usage: 927.45 MB\n",
      "Epoch 88, Batch 1100/4948, Batch loss: 0.000774762243963778\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 1200/4948, Batch loss: 0.0010472859721630812\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 1300/4948, Batch loss: 0.00015515449922531843\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 1400/4948, Batch loss: 0.00012155295553384349\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 1500/4948, Batch loss: 7.256070239236578e-05\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 1600/4948, Batch loss: 1.333777436229866e-05\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 1700/4948, Batch loss: 0.00030750740552321076\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 1800/4948, Batch loss: 0.00026032832101918757\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 1900/4948, Batch loss: 0.0021787588484585285\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 2000/4948, Batch loss: 0.000381196616217494\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 2100/4948, Batch loss: 0.0006378455436788499\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 2200/4948, Batch loss: 0.00010441099584568292\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 2300/4948, Batch loss: 0.000221117734326981\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 2400/4948, Batch loss: 0.00010692055366234854\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 2500/4948, Batch loss: 4.816149885300547e-05\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 2600/4948, Batch loss: 0.00037770261405967176\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 2700/4948, Batch loss: 0.0003502155013848096\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 2800/4948, Batch loss: 0.0011694635031744838\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 2900/4948, Batch loss: 0.00020582940487656742\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 3000/4948, Batch loss: 5.901205804548226e-05\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 3100/4948, Batch loss: 0.0001288060302613303\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 3200/4948, Batch loss: 0.0002866221184376627\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 3300/4948, Batch loss: 0.00013806244533043355\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 3400/4948, Batch loss: 7.948401616886258e-06\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 3500/4948, Batch loss: 4.9468708311906084e-05\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 3600/4948, Batch loss: 4.052578515256755e-05\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 3700/4948, Batch loss: 0.0005761855281889439\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 3800/4948, Batch loss: 0.00010517545888433233\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 3900/4948, Batch loss: 0.0001343748881481588\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 4000/4948, Batch loss: 0.0001481441140640527\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 4100/4948, Batch loss: 4.49616345576942e-05\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 4200/4948, Batch loss: 0.0008955666562542319\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 88, Batch 4300/4948, Batch loss: 0.00014608082710765302\n",
      "Memory Usage: 927.48 MB\n",
      "Epoch 88, Batch 4400/4948, Batch loss: 4.3371051106078085e-06\n",
      "Memory Usage: 927.48 MB\n",
      "Epoch 88, Batch 4500/4948, Batch loss: 0.0008237949223257601\n",
      "Memory Usage: 927.48 MB\n",
      "Epoch 88, Batch 4600/4948, Batch loss: 0.00023333902936428785\n",
      "Memory Usage: 927.48 MB\n",
      "Epoch 88, Batch 4700/4948, Batch loss: 0.0004060023929923773\n",
      "Memory Usage: 927.48 MB\n",
      "Epoch 88, Batch 4800/4948, Batch loss: 0.00011432815517764539\n",
      "Memory Usage: 928.00 MB\n",
      "Epoch 88, Batch 4900/4948, Batch loss: 7.425063813570887e-05\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 88, Batch 4948/4948, Batch loss: 0.0008933325880207121\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 88 completed in 64.06 seconds, Total Training Loss: 0.0002779935698480787\n",
      "\n",
      "Epoch 89/100\n",
      "Epoch 89, Batch 100/4948, Batch loss: 0.00016881388728506863\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 200/4948, Batch loss: 1.2103040717192926e-05\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 300/4948, Batch loss: 0.0002996097318828106\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 400/4948, Batch loss: 0.00011412029562052339\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 500/4948, Batch loss: 5.137130574439652e-05\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 600/4948, Batch loss: 0.00047726219054311514\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 700/4948, Batch loss: 5.414421684690751e-05\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 800/4948, Batch loss: 0.0001251701614819467\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 900/4948, Batch loss: 0.0005128856282681227\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 1000/4948, Batch loss: 8.54097306728363e-05\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 1100/4948, Batch loss: 0.0007883493090048432\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 1200/4948, Batch loss: 0.0010521115036681294\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 1300/4948, Batch loss: 0.00014858263602945954\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 1400/4948, Batch loss: 0.0001230563793797046\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 1500/4948, Batch loss: 7.516550977015868e-05\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 1600/4948, Batch loss: 1.3165931704861578e-05\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 1700/4948, Batch loss: 0.0003062134201172739\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 1800/4948, Batch loss: 0.0002611936361063272\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 1900/4948, Batch loss: 0.0009435114916414022\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 2000/4948, Batch loss: 0.00037512643029913306\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 2100/4948, Batch loss: 0.0006155189475975931\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 2200/4948, Batch loss: 0.00010355030826758593\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 2300/4948, Batch loss: 0.00022409798111766577\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 2400/4948, Batch loss: 0.00010942268272629008\n",
      "Memory Usage: 928.25 MB\n",
      "Epoch 89, Batch 2500/4948, Batch loss: 4.9957183364313096e-05\n",
      "Memory Usage: 865.55 MB\n",
      "Epoch 89, Batch 2600/4948, Batch loss: 0.00037630004226230085\n",
      "Memory Usage: 794.72 MB\n",
      "Epoch 89, Batch 2700/4948, Batch loss: 0.00036245063529349864\n",
      "Memory Usage: 703.05 MB\n",
      "Epoch 89, Batch 2800/4948, Batch loss: 0.001134646125137806\n",
      "Memory Usage: 715.52 MB\n",
      "Epoch 89, Batch 2900/4948, Batch loss: 0.00019901887571904808\n",
      "Memory Usage: 713.41 MB\n",
      "Epoch 89, Batch 3000/4948, Batch loss: 5.990442878101021e-05\n",
      "Memory Usage: 730.42 MB\n",
      "Epoch 89, Batch 3100/4948, Batch loss: 0.00012338983651716262\n",
      "Memory Usage: 746.75 MB\n",
      "Epoch 89, Batch 3200/4948, Batch loss: 0.00028368874336592853\n",
      "Memory Usage: 758.59 MB\n",
      "Epoch 89, Batch 3300/4948, Batch loss: 0.00013998580106999725\n",
      "Memory Usage: 770.91 MB\n",
      "Epoch 89, Batch 3400/4948, Batch loss: 5.579364824370714e-06\n",
      "Memory Usage: 783.31 MB\n",
      "Epoch 89, Batch 3500/4948, Batch loss: 4.58524446003139e-05\n",
      "Memory Usage: 784.59 MB\n",
      "Epoch 89, Batch 3600/4948, Batch loss: 4.0860180888557807e-05\n",
      "Memory Usage: 785.20 MB\n",
      "Epoch 89, Batch 3700/4948, Batch loss: 0.0005725403898395598\n",
      "Memory Usage: 785.20 MB\n",
      "Epoch 89, Batch 3800/4948, Batch loss: 0.00010801100142998621\n",
      "Memory Usage: 783.11 MB\n",
      "Epoch 89, Batch 3900/4948, Batch loss: 0.00014074993669055402\n",
      "Memory Usage: 785.20 MB\n",
      "Epoch 89, Batch 4000/4948, Batch loss: 0.00014656090934295207\n",
      "Memory Usage: 785.20 MB\n",
      "Epoch 89, Batch 4100/4948, Batch loss: 4.595217978931032e-05\n",
      "Memory Usage: 784.27 MB\n",
      "Epoch 89, Batch 4200/4948, Batch loss: 0.0008863095426931977\n",
      "Memory Usage: 785.16 MB\n",
      "Epoch 89, Batch 4300/4948, Batch loss: 0.00014594401000067592\n",
      "Memory Usage: 785.66 MB\n",
      "Epoch 89, Batch 4400/4948, Batch loss: 3.6987551084166626e-06\n",
      "Memory Usage: 785.66 MB\n",
      "Epoch 89, Batch 4500/4948, Batch loss: 0.0008207477512769401\n",
      "Memory Usage: 785.66 MB\n",
      "Epoch 89, Batch 4600/4948, Batch loss: 0.000234829552937299\n",
      "Memory Usage: 785.66 MB\n",
      "Epoch 89, Batch 4700/4948, Batch loss: 0.0004139608354307711\n",
      "Memory Usage: 785.66 MB\n",
      "Epoch 89, Batch 4800/4948, Batch loss: 0.000127240433357656\n",
      "Memory Usage: 785.66 MB\n",
      "Epoch 89, Batch 4900/4948, Batch loss: 7.670902414247394e-05\n",
      "Memory Usage: 785.67 MB\n",
      "Epoch 89, Batch 4948/4948, Batch loss: 0.0008876652573235333\n",
      "Memory Usage: 785.67 MB\n",
      "Epoch 89 completed in 63.84 seconds, Total Training Loss: 0.00027223934128167796\n",
      "\n",
      "Epoch 90/100\n",
      "Epoch 90, Batch 100/4948, Batch loss: 0.00016915604646783322\n",
      "Memory Usage: 785.69 MB\n",
      "Epoch 90, Batch 200/4948, Batch loss: 1.065922242560191e-05\n",
      "Memory Usage: 785.69 MB\n",
      "Epoch 90, Batch 300/4948, Batch loss: 0.0003011436783708632\n",
      "Memory Usage: 785.69 MB\n",
      "Epoch 90, Batch 400/4948, Batch loss: 0.0001162004773505032\n",
      "Memory Usage: 785.69 MB\n",
      "Epoch 90, Batch 500/4948, Batch loss: 5.1025443099206313e-05\n",
      "Memory Usage: 785.69 MB\n",
      "Epoch 90, Batch 600/4948, Batch loss: 0.0004636211378965527\n",
      "Memory Usage: 785.84 MB\n",
      "Epoch 90, Batch 700/4948, Batch loss: 5.553867595153861e-05\n",
      "Memory Usage: 786.41 MB\n",
      "Epoch 90, Batch 800/4948, Batch loss: 0.0001249500783160329\n",
      "Memory Usage: 786.41 MB\n",
      "Epoch 90, Batch 900/4948, Batch loss: 0.0005284405197016895\n",
      "Memory Usage: 786.41 MB\n",
      "Epoch 90, Batch 1000/4948, Batch loss: 8.551219798391685e-05\n",
      "Memory Usage: 786.41 MB\n",
      "Epoch 90, Batch 1100/4948, Batch loss: 0.000770572863984853\n",
      "Memory Usage: 786.41 MB\n",
      "Epoch 90, Batch 1200/4948, Batch loss: 0.0010390395764261484\n",
      "Memory Usage: 786.41 MB\n",
      "Epoch 90, Batch 1300/4948, Batch loss: 0.00015610019909217954\n",
      "Memory Usage: 786.41 MB\n",
      "Epoch 90, Batch 1400/4948, Batch loss: 0.00012123212945880368\n",
      "Memory Usage: 786.44 MB\n",
      "Epoch 90, Batch 1500/4948, Batch loss: 7.467107207048684e-05\n",
      "Memory Usage: 786.47 MB\n",
      "Epoch 90, Batch 1600/4948, Batch loss: 1.3775465959042776e-05\n",
      "Memory Usage: 789.69 MB\n",
      "Epoch 90, Batch 1700/4948, Batch loss: 0.00032699923031032085\n",
      "Memory Usage: 801.52 MB\n",
      "Epoch 90, Batch 1800/4948, Batch loss: 0.00026329379761591554\n",
      "Memory Usage: 813.30 MB\n",
      "Epoch 90, Batch 1900/4948, Batch loss: 0.0026433151215314865\n",
      "Memory Usage: 825.03 MB\n",
      "Epoch 90, Batch 2000/4948, Batch loss: 0.00038854635204188526\n",
      "Memory Usage: 836.80 MB\n",
      "Epoch 90, Batch 2100/4948, Batch loss: 0.0006090649985708296\n",
      "Memory Usage: 849.06 MB\n",
      "Epoch 90, Batch 2200/4948, Batch loss: 0.00010391211981186643\n",
      "Memory Usage: 860.91 MB\n",
      "Epoch 90, Batch 2300/4948, Batch loss: 0.0002218994195573032\n",
      "Memory Usage: 872.69 MB\n",
      "Epoch 90, Batch 2400/4948, Batch loss: 0.0001060813301592134\n",
      "Memory Usage: 874.02 MB\n",
      "Epoch 90, Batch 2500/4948, Batch loss: 4.896993050351739e-05\n",
      "Memory Usage: 887.83 MB\n",
      "Epoch 90, Batch 2600/4948, Batch loss: 0.00037022048491053283\n",
      "Memory Usage: 895.38 MB\n",
      "Epoch 90, Batch 2700/4948, Batch loss: 0.00033953023375943303\n",
      "Memory Usage: 871.86 MB\n",
      "Epoch 90, Batch 2800/4948, Batch loss: 0.0011254679411649704\n",
      "Memory Usage: 875.70 MB\n",
      "Epoch 90, Batch 2900/4948, Batch loss: 0.00019636275828815997\n",
      "Memory Usage: 885.89 MB\n",
      "Epoch 90, Batch 3000/4948, Batch loss: 5.775513272965327e-05\n",
      "Memory Usage: 889.55 MB\n",
      "Epoch 90, Batch 3100/4948, Batch loss: 0.00012160997721366584\n",
      "Memory Usage: 889.55 MB\n",
      "Epoch 90, Batch 3200/4948, Batch loss: 0.0002755011082626879\n",
      "Memory Usage: 890.61 MB\n",
      "Epoch 90, Batch 3300/4948, Batch loss: 0.00013888046669308096\n",
      "Memory Usage: 890.61 MB\n",
      "Epoch 90, Batch 3400/4948, Batch loss: 7.472377092199167e-06\n",
      "Memory Usage: 891.61 MB\n",
      "Epoch 90, Batch 3500/4948, Batch loss: 4.62956159026362e-05\n",
      "Memory Usage: 891.61 MB\n",
      "Epoch 90, Batch 3600/4948, Batch loss: 4.016217644675635e-05\n",
      "Memory Usage: 891.69 MB\n",
      "Epoch 90, Batch 3700/4948, Batch loss: 0.0005806119297631085\n",
      "Memory Usage: 892.19 MB\n",
      "Epoch 90, Batch 3800/4948, Batch loss: 0.00010833459964487702\n",
      "Memory Usage: 893.89 MB\n",
      "Epoch 90, Batch 3900/4948, Batch loss: 0.00013594081974588335\n",
      "Memory Usage: 893.89 MB\n",
      "Epoch 90, Batch 4000/4948, Batch loss: 0.00014852953609079123\n",
      "Memory Usage: 893.89 MB\n",
      "Epoch 90, Batch 4100/4948, Batch loss: 4.481905489228666e-05\n",
      "Memory Usage: 894.39 MB\n",
      "Epoch 90, Batch 4200/4948, Batch loss: 0.0008673801785334945\n",
      "Memory Usage: 894.39 MB\n",
      "Epoch 90, Batch 4300/4948, Batch loss: 0.00014683320478070527\n",
      "Memory Usage: 894.95 MB\n",
      "Epoch 90, Batch 4400/4948, Batch loss: 3.416239906073315e-06\n",
      "Memory Usage: 894.97 MB\n",
      "Epoch 90, Batch 4500/4948, Batch loss: 0.0008147953194566071\n",
      "Memory Usage: 894.97 MB\n",
      "Epoch 90, Batch 4600/4948, Batch loss: 0.00022930574778001755\n",
      "Memory Usage: 895.41 MB\n",
      "Epoch 90, Batch 4700/4948, Batch loss: 0.0004090713046025485\n",
      "Memory Usage: 895.41 MB\n",
      "Epoch 90, Batch 4800/4948, Batch loss: 0.00011018272925866768\n",
      "Memory Usage: 895.23 MB\n",
      "Epoch 90, Batch 4900/4948, Batch loss: 7.184161222539842e-05\n",
      "Memory Usage: 895.78 MB\n",
      "Epoch 90, Batch 4948/4948, Batch loss: 0.0008739445474930108\n",
      "Memory Usage: 895.78 MB\n",
      "Epoch 90 completed in 63.37 seconds, Total Training Loss: 0.000278385400066233\n",
      "\n",
      "Epoch 91/100\n",
      "Epoch 91, Batch 100/4948, Batch loss: 0.00017078110249713063\n",
      "Memory Usage: 895.91 MB\n",
      "Epoch 91, Batch 200/4948, Batch loss: 1.358524605166167e-05\n",
      "Memory Usage: 895.91 MB\n",
      "Epoch 91, Batch 300/4948, Batch loss: 0.000296534679364413\n",
      "Memory Usage: 896.75 MB\n",
      "Epoch 91, Batch 400/4948, Batch loss: 0.00011882962280651554\n",
      "Memory Usage: 896.75 MB\n",
      "Epoch 91, Batch 500/4948, Batch loss: 5.180917651159689e-05\n",
      "Memory Usage: 896.75 MB\n",
      "Epoch 91, Batch 600/4948, Batch loss: 0.00047448326949961483\n",
      "Memory Usage: 897.25 MB\n",
      "Epoch 91, Batch 700/4948, Batch loss: 5.2078012231504545e-05\n",
      "Memory Usage: 878.42 MB\n",
      "Epoch 91, Batch 800/4948, Batch loss: 0.0001259881246369332\n",
      "Memory Usage: 889.02 MB\n",
      "Epoch 91, Batch 900/4948, Batch loss: 0.0005110157071612775\n",
      "Memory Usage: 890.69 MB\n",
      "Epoch 91, Batch 1000/4948, Batch loss: 8.534684457117692e-05\n",
      "Memory Usage: 891.27 MB\n",
      "Epoch 91, Batch 1100/4948, Batch loss: 0.0007787701324559748\n",
      "Memory Usage: 892.03 MB\n",
      "Epoch 91, Batch 1200/4948, Batch loss: 0.0010167659493163228\n",
      "Memory Usage: 893.16 MB\n",
      "Epoch 91, Batch 1300/4948, Batch loss: 0.00015374246868304908\n",
      "Memory Usage: 893.30 MB\n",
      "Epoch 91, Batch 1400/4948, Batch loss: 0.00012134356802562252\n",
      "Memory Usage: 893.30 MB\n",
      "Epoch 91, Batch 1500/4948, Batch loss: 7.373734115390107e-05\n",
      "Memory Usage: 893.30 MB\n",
      "Epoch 91, Batch 1600/4948, Batch loss: 1.3823096196574625e-05\n",
      "Memory Usage: 893.30 MB\n",
      "Epoch 91, Batch 1700/4948, Batch loss: 0.0003133210993837565\n",
      "Memory Usage: 893.36 MB\n",
      "Epoch 91, Batch 1800/4948, Batch loss: 0.00026014994364231825\n",
      "Memory Usage: 893.36 MB\n",
      "Epoch 91, Batch 1900/4948, Batch loss: 0.0016480255872011185\n",
      "Memory Usage: 893.36 MB\n",
      "Epoch 91, Batch 2000/4948, Batch loss: 0.00040539150359109044\n",
      "Memory Usage: 894.77 MB\n",
      "Epoch 91, Batch 2100/4948, Batch loss: 0.0006390055641531944\n",
      "Memory Usage: 895.20 MB\n",
      "Epoch 91, Batch 2200/4948, Batch loss: 0.000103787962871138\n",
      "Memory Usage: 895.20 MB\n",
      "Epoch 91, Batch 2300/4948, Batch loss: 0.00022663112031295896\n",
      "Memory Usage: 895.20 MB\n",
      "Epoch 91, Batch 2400/4948, Batch loss: 0.0001046726101776585\n",
      "Memory Usage: 895.22 MB\n",
      "Epoch 91, Batch 2500/4948, Batch loss: 4.863423964707181e-05\n",
      "Memory Usage: 895.22 MB\n",
      "Epoch 91, Batch 2600/4948, Batch loss: 0.00037554537993855774\n",
      "Memory Usage: 895.23 MB\n",
      "Epoch 91, Batch 2700/4948, Batch loss: 0.00034054971183650196\n",
      "Memory Usage: 895.25 MB\n",
      "Epoch 91, Batch 2800/4948, Batch loss: 0.0011243405751883984\n",
      "Memory Usage: 895.25 MB\n",
      "Epoch 91, Batch 2900/4948, Batch loss: 0.00019865098875015974\n",
      "Memory Usage: 895.25 MB\n",
      "Epoch 91, Batch 3000/4948, Batch loss: 5.858762233401649e-05\n",
      "Memory Usage: 895.25 MB\n",
      "Epoch 91, Batch 3100/4948, Batch loss: 0.00012881973816547543\n",
      "Memory Usage: 895.25 MB\n",
      "Epoch 91, Batch 3200/4948, Batch loss: 0.0002849699812941253\n",
      "Memory Usage: 895.25 MB\n",
      "Epoch 91, Batch 3300/4948, Batch loss: 0.00013848472735844553\n",
      "Memory Usage: 895.25 MB\n",
      "Epoch 91, Batch 3400/4948, Batch loss: 7.04986450728029e-06\n",
      "Memory Usage: 895.77 MB\n",
      "Epoch 91, Batch 3500/4948, Batch loss: 4.771137173520401e-05\n",
      "Memory Usage: 895.77 MB\n",
      "Epoch 91, Batch 3600/4948, Batch loss: 4.002071000286378e-05\n",
      "Memory Usage: 895.77 MB\n",
      "Epoch 91, Batch 3700/4948, Batch loss: 0.0005885385908186436\n",
      "Memory Usage: 895.77 MB\n",
      "Epoch 91, Batch 3800/4948, Batch loss: 0.00010601089161355048\n",
      "Memory Usage: 895.77 MB\n",
      "Epoch 91, Batch 3900/4948, Batch loss: 0.00014080530672799796\n",
      "Memory Usage: 895.77 MB\n",
      "Epoch 91, Batch 4000/4948, Batch loss: 0.0001483546511735767\n",
      "Memory Usage: 895.77 MB\n",
      "Epoch 91, Batch 4100/4948, Batch loss: 4.452714347280562e-05\n",
      "Memory Usage: 895.77 MB\n",
      "Epoch 91, Batch 4200/4948, Batch loss: 0.0008649566443637013\n",
      "Memory Usage: 896.14 MB\n",
      "Epoch 91, Batch 4300/4948, Batch loss: 0.00014581946015823632\n",
      "Memory Usage: 896.14 MB\n",
      "Epoch 91, Batch 4400/4948, Batch loss: 3.7176619116507936e-06\n",
      "Memory Usage: 896.14 MB\n",
      "Epoch 91, Batch 4500/4948, Batch loss: 0.0008080290863290429\n",
      "Memory Usage: 896.14 MB\n",
      "Epoch 91, Batch 4600/4948, Batch loss: 0.00023664016043767333\n",
      "Memory Usage: 893.14 MB\n",
      "Epoch 91, Batch 4700/4948, Batch loss: 0.0004119503719266504\n",
      "Memory Usage: 894.48 MB\n",
      "Epoch 91, Batch 4800/4948, Batch loss: 0.00013080025382805616\n",
      "Memory Usage: 895.31 MB\n",
      "Epoch 91, Batch 4900/4948, Batch loss: 7.301176810869947e-05\n",
      "Memory Usage: 896.14 MB\n",
      "Epoch 91, Batch 4948/4948, Batch loss: 0.0009041044977493584\n",
      "Memory Usage: 896.52 MB\n",
      "Epoch 91 completed in 64.43 seconds, Total Training Loss: 0.0002736277646336515\n",
      "Validation completed in 3.51 seconds, Average Validation Loss: 0.0005063575435153713\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 92/100\n",
      "Epoch 92, Batch 100/4948, Batch loss: 0.000167750651598908\n",
      "Memory Usage: 926.88 MB\n",
      "Epoch 92, Batch 200/4948, Batch loss: 1.2365645488898735e-05\n",
      "Memory Usage: 926.89 MB\n",
      "Epoch 92, Batch 300/4948, Batch loss: 0.0003007242630701512\n",
      "Memory Usage: 927.16 MB\n",
      "Epoch 92, Batch 400/4948, Batch loss: 0.00011745994561351836\n",
      "Memory Usage: 929.03 MB\n",
      "Epoch 92, Batch 500/4948, Batch loss: 5.190855517867021e-05\n",
      "Memory Usage: 929.03 MB\n",
      "Epoch 92, Batch 600/4948, Batch loss: 0.0004706534091383219\n",
      "Memory Usage: 929.03 MB\n",
      "Epoch 92, Batch 700/4948, Batch loss: 5.625730045721866e-05\n",
      "Memory Usage: 929.16 MB\n",
      "Epoch 92, Batch 800/4948, Batch loss: 0.00012869335478171706\n",
      "Memory Usage: 927.75 MB\n",
      "Epoch 92, Batch 900/4948, Batch loss: 0.0004961322410963476\n",
      "Memory Usage: 929.06 MB\n",
      "Epoch 92, Batch 1000/4948, Batch loss: 8.792078006081283e-05\n",
      "Memory Usage: 929.06 MB\n",
      "Epoch 92, Batch 1100/4948, Batch loss: 0.00081189617048949\n",
      "Memory Usage: 929.06 MB\n",
      "Epoch 92, Batch 1200/4948, Batch loss: 0.0010372033575549722\n",
      "Memory Usage: 929.06 MB\n",
      "Epoch 92, Batch 1300/4948, Batch loss: 0.00015480353613384068\n",
      "Memory Usage: 929.06 MB\n",
      "Epoch 92, Batch 1400/4948, Batch loss: 0.0001224110455950722\n",
      "Memory Usage: 929.06 MB\n",
      "Epoch 92, Batch 1500/4948, Batch loss: 7.622793782502413e-05\n",
      "Memory Usage: 929.09 MB\n",
      "Epoch 92, Batch 1600/4948, Batch loss: 1.3262024367577396e-05\n",
      "Memory Usage: 929.09 MB\n",
      "Epoch 92, Batch 1700/4948, Batch loss: 0.0002981828583870083\n",
      "Memory Usage: 929.09 MB\n",
      "Epoch 92, Batch 1800/4948, Batch loss: 0.0002626267960295081\n",
      "Memory Usage: 929.09 MB\n",
      "Epoch 92, Batch 1900/4948, Batch loss: 0.001110563869588077\n",
      "Memory Usage: 929.09 MB\n",
      "Epoch 92, Batch 2000/4948, Batch loss: 0.0003708094300236553\n",
      "Memory Usage: 929.09 MB\n",
      "Epoch 92, Batch 2100/4948, Batch loss: 0.0006086064386181533\n",
      "Memory Usage: 929.09 MB\n",
      "Epoch 92, Batch 2200/4948, Batch loss: 0.00010326996562071145\n",
      "Memory Usage: 929.09 MB\n",
      "Epoch 92, Batch 2300/4948, Batch loss: 0.00021769515296909958\n",
      "Memory Usage: 929.14 MB\n",
      "Epoch 92, Batch 2400/4948, Batch loss: 0.00010515844769543037\n",
      "Memory Usage: 929.16 MB\n",
      "Epoch 92, Batch 2500/4948, Batch loss: 4.900426574749872e-05\n",
      "Memory Usage: 929.16 MB\n",
      "Epoch 92, Batch 2600/4948, Batch loss: 0.0003708629810716957\n",
      "Memory Usage: 929.16 MB\n",
      "Epoch 92, Batch 2700/4948, Batch loss: 0.0003641103394329548\n",
      "Memory Usage: 929.17 MB\n",
      "Epoch 92, Batch 2800/4948, Batch loss: 0.0011347844265401363\n",
      "Memory Usage: 929.92 MB\n",
      "Epoch 92, Batch 2900/4948, Batch loss: 0.00020669119840022177\n",
      "Memory Usage: 926.81 MB\n",
      "Epoch 92, Batch 3000/4948, Batch loss: 5.8854166127275676e-05\n",
      "Memory Usage: 927.47 MB\n",
      "Epoch 92, Batch 3100/4948, Batch loss: 0.00012545171193778515\n",
      "Memory Usage: 917.61 MB\n",
      "Epoch 92, Batch 3200/4948, Batch loss: 0.0002778914349619299\n",
      "Memory Usage: 874.58 MB\n",
      "Epoch 92, Batch 3300/4948, Batch loss: 0.00013723292795475572\n",
      "Memory Usage: 878.75 MB\n",
      "Epoch 92, Batch 3400/4948, Batch loss: 5.415682153397938e-06\n",
      "Memory Usage: 888.94 MB\n",
      "Epoch 92, Batch 3500/4948, Batch loss: 0.0003809256595559418\n",
      "Memory Usage: 894.14 MB\n",
      "Epoch 92, Batch 3600/4948, Batch loss: 4.720589640783146e-05\n",
      "Memory Usage: 894.14 MB\n",
      "Epoch 92, Batch 3700/4948, Batch loss: 0.0005745681701228023\n",
      "Memory Usage: 900.06 MB\n",
      "Epoch 92, Batch 3800/4948, Batch loss: 0.00010513929009903222\n",
      "Memory Usage: 900.06 MB\n",
      "Epoch 92, Batch 3900/4948, Batch loss: 0.00013226980809122324\n",
      "Memory Usage: 900.66 MB\n",
      "Epoch 92, Batch 4000/4948, Batch loss: 0.00014678829757031053\n",
      "Memory Usage: 901.97 MB\n",
      "Epoch 92, Batch 4100/4948, Batch loss: 4.4892087316839024e-05\n",
      "Memory Usage: 901.97 MB\n",
      "Epoch 92, Batch 4200/4948, Batch loss: 0.000878709543030709\n",
      "Memory Usage: 901.97 MB\n",
      "Epoch 92, Batch 4300/4948, Batch loss: 0.00014570620260201395\n",
      "Memory Usage: 901.97 MB\n",
      "Epoch 92, Batch 4400/4948, Batch loss: 4.108836947125383e-06\n",
      "Memory Usage: 901.97 MB\n",
      "Epoch 92, Batch 4500/4948, Batch loss: 0.0007961749797686934\n",
      "Memory Usage: 901.97 MB\n",
      "Epoch 92, Batch 4600/4948, Batch loss: 0.00023166730534285307\n",
      "Memory Usage: 902.17 MB\n",
      "Epoch 92, Batch 4700/4948, Batch loss: 0.00042607050272636116\n",
      "Memory Usage: 902.17 MB\n",
      "Epoch 92, Batch 4800/4948, Batch loss: 0.00011219281441299245\n",
      "Memory Usage: 902.25 MB\n",
      "Epoch 92, Batch 4900/4948, Batch loss: 8.453922782791778e-05\n",
      "Memory Usage: 902.75 MB\n",
      "Epoch 92, Batch 4948/4948, Batch loss: 0.0008526271558366716\n",
      "Memory Usage: 902.80 MB\n",
      "Epoch 92 completed in 64.66 seconds, Total Training Loss: 0.00027372820253983245\n",
      "\n",
      "Epoch 93/100\n",
      "Epoch 93, Batch 100/4948, Batch loss: 0.00016928452532738447\n",
      "Memory Usage: 902.81 MB\n",
      "Epoch 93, Batch 200/4948, Batch loss: 1.0906597708526533e-05\n",
      "Memory Usage: 902.83 MB\n",
      "Epoch 93, Batch 300/4948, Batch loss: 0.00029466647538356483\n",
      "Memory Usage: 902.83 MB\n",
      "Epoch 93, Batch 400/4948, Batch loss: 0.00011321247438900173\n",
      "Memory Usage: 902.83 MB\n",
      "Epoch 93, Batch 500/4948, Batch loss: 5.151204823050648e-05\n",
      "Memory Usage: 903.00 MB\n",
      "Epoch 93, Batch 600/4948, Batch loss: 0.0004695921379607171\n",
      "Memory Usage: 903.27 MB\n",
      "Epoch 93, Batch 700/4948, Batch loss: 5.2062012400710955e-05\n",
      "Memory Usage: 903.30 MB\n",
      "Epoch 93, Batch 800/4948, Batch loss: 0.00012572506966535002\n",
      "Memory Usage: 903.30 MB\n",
      "Epoch 93, Batch 900/4948, Batch loss: 0.0005173541139811277\n",
      "Memory Usage: 903.30 MB\n",
      "Epoch 93, Batch 1000/4948, Batch loss: 8.64676185301505e-05\n",
      "Memory Usage: 903.30 MB\n",
      "Epoch 93, Batch 1100/4948, Batch loss: 0.0007632007473148406\n",
      "Memory Usage: 903.45 MB\n",
      "Epoch 93, Batch 1200/4948, Batch loss: 0.0009923065081238747\n",
      "Memory Usage: 906.00 MB\n",
      "Epoch 93, Batch 1300/4948, Batch loss: 0.00015392049681395292\n",
      "Memory Usage: 906.00 MB\n",
      "Epoch 93, Batch 1400/4948, Batch loss: 0.00012105394853278995\n",
      "Memory Usage: 906.02 MB\n",
      "Epoch 93, Batch 1500/4948, Batch loss: 7.43647979106754e-05\n",
      "Memory Usage: 906.02 MB\n",
      "Epoch 93, Batch 1600/4948, Batch loss: 1.598363451194018e-05\n",
      "Memory Usage: 906.02 MB\n",
      "Epoch 93, Batch 1700/4948, Batch loss: 0.0003289922315161675\n",
      "Memory Usage: 906.02 MB\n",
      "Epoch 93, Batch 1800/4948, Batch loss: 0.0002628718502819538\n",
      "Memory Usage: 856.70 MB\n",
      "Epoch 93, Batch 1900/4948, Batch loss: 0.0027592317201197147\n",
      "Memory Usage: 584.17 MB\n",
      "Epoch 93, Batch 2000/4948, Batch loss: 0.00038778685848228633\n",
      "Memory Usage: 602.80 MB\n",
      "Epoch 93, Batch 2100/4948, Batch loss: 0.0006290922174230218\n",
      "Memory Usage: 624.33 MB\n",
      "Epoch 93, Batch 2200/4948, Batch loss: 0.00010446873784530908\n",
      "Memory Usage: 636.98 MB\n",
      "Epoch 93, Batch 2300/4948, Batch loss: 0.0002214942069258541\n",
      "Memory Usage: 649.61 MB\n",
      "Epoch 93, Batch 2400/4948, Batch loss: 0.00010406956425867975\n",
      "Memory Usage: 663.64 MB\n",
      "Epoch 93, Batch 2500/4948, Batch loss: 4.884055306320079e-05\n",
      "Memory Usage: 676.91 MB\n",
      "Epoch 93, Batch 2600/4948, Batch loss: 0.000370180670870468\n",
      "Memory Usage: 675.97 MB\n",
      "Epoch 93, Batch 2700/4948, Batch loss: 0.00034173193853348494\n",
      "Memory Usage: 548.72 MB\n",
      "Epoch 93, Batch 2800/4948, Batch loss: 0.0011139953276142478\n",
      "Memory Usage: 574.45 MB\n",
      "Epoch 93, Batch 2900/4948, Batch loss: 0.00019731229986064136\n",
      "Memory Usage: 586.25 MB\n",
      "Epoch 93, Batch 3000/4948, Batch loss: 5.748820331064053e-05\n",
      "Memory Usage: 600.94 MB\n",
      "Epoch 93, Batch 3100/4948, Batch loss: 0.000125936305266805\n",
      "Memory Usage: 616.14 MB\n",
      "Epoch 93, Batch 3200/4948, Batch loss: 0.00027646057424135506\n",
      "Memory Usage: 630.44 MB\n",
      "Epoch 93, Batch 3300/4948, Batch loss: 0.00014070399629417807\n",
      "Memory Usage: 644.39 MB\n",
      "Epoch 93, Batch 3400/4948, Batch loss: 6.12277335676481e-06\n",
      "Memory Usage: 656.23 MB\n",
      "Epoch 93, Batch 3500/4948, Batch loss: 4.6130578994052485e-05\n",
      "Memory Usage: 668.22 MB\n",
      "Epoch 93, Batch 3600/4948, Batch loss: 4.00000462832395e-05\n",
      "Memory Usage: 680.52 MB\n",
      "Epoch 93, Batch 3700/4948, Batch loss: 0.0005508578615263104\n",
      "Memory Usage: 682.09 MB\n",
      "Epoch 93, Batch 3800/4948, Batch loss: 0.00010572292376309633\n",
      "Memory Usage: 697.06 MB\n",
      "Epoch 93, Batch 3900/4948, Batch loss: 0.00013277582183945924\n",
      "Memory Usage: 709.39 MB\n",
      "Epoch 93, Batch 4000/4948, Batch loss: 0.00014558366092387587\n",
      "Memory Usage: 721.11 MB\n",
      "Epoch 93, Batch 4100/4948, Batch loss: 4.821699621970765e-05\n",
      "Memory Usage: 734.50 MB\n",
      "Epoch 93, Batch 4200/4948, Batch loss: 0.0008543066214770079\n",
      "Memory Usage: 746.61 MB\n",
      "Epoch 93, Batch 4300/4948, Batch loss: 0.0001466520334361121\n",
      "Memory Usage: 758.14 MB\n",
      "Epoch 93, Batch 4400/4948, Batch loss: 4.316425020078896e-06\n",
      "Memory Usage: 770.48 MB\n",
      "Epoch 93, Batch 4500/4948, Batch loss: 0.0008043114212341607\n",
      "Memory Usage: 782.08 MB\n",
      "Epoch 93, Batch 4600/4948, Batch loss: 0.0002305700327269733\n",
      "Memory Usage: 793.81 MB\n",
      "Epoch 93, Batch 4700/4948, Batch loss: 0.00041513287578709424\n",
      "Memory Usage: 805.52 MB\n",
      "Epoch 93, Batch 4800/4948, Batch loss: 0.00012208228872623295\n",
      "Memory Usage: 817.67 MB\n",
      "Epoch 93, Batch 4900/4948, Batch loss: 7.617761730216444e-05\n",
      "Memory Usage: 827.23 MB\n",
      "Epoch 93, Batch 4948/4948, Batch loss: 0.000870026065967977\n",
      "Memory Usage: 827.34 MB\n",
      "Epoch 93 completed in 68.67 seconds, Total Training Loss: 0.0002761976930123939\n",
      "\n",
      "Epoch 94/100\n",
      "Epoch 94, Batch 100/4948, Batch loss: 0.00016907781537156552\n",
      "Memory Usage: 827.55 MB\n",
      "Epoch 94, Batch 200/4948, Batch loss: 1.0920037311734632e-05\n",
      "Memory Usage: 828.31 MB\n",
      "Epoch 94, Batch 300/4948, Batch loss: 0.000298142374958843\n",
      "Memory Usage: 829.80 MB\n",
      "Epoch 94, Batch 400/4948, Batch loss: 0.00011687930236803368\n",
      "Memory Usage: 831.89 MB\n",
      "Epoch 94, Batch 500/4948, Batch loss: 5.157093983143568e-05\n",
      "Memory Usage: 832.12 MB\n",
      "Epoch 94, Batch 600/4948, Batch loss: 0.00047041522338986397\n",
      "Memory Usage: 832.12 MB\n",
      "Epoch 94, Batch 700/4948, Batch loss: 5.123998926137574e-05\n",
      "Memory Usage: 832.12 MB\n",
      "Epoch 94, Batch 800/4948, Batch loss: 0.00012648565461859107\n",
      "Memory Usage: 832.14 MB\n",
      "Epoch 94, Batch 900/4948, Batch loss: 0.000495856802444905\n",
      "Memory Usage: 832.19 MB\n",
      "Epoch 94, Batch 1000/4948, Batch loss: 8.513293869327754e-05\n",
      "Memory Usage: 832.19 MB\n",
      "Epoch 94, Batch 1100/4948, Batch loss: 0.0007818355807103217\n",
      "Memory Usage: 832.64 MB\n",
      "Epoch 94, Batch 1200/4948, Batch loss: 0.0009933002293109894\n",
      "Memory Usage: 832.19 MB\n",
      "Epoch 94, Batch 1300/4948, Batch loss: 0.00014963066496420652\n",
      "Memory Usage: 837.38 MB\n",
      "Epoch 94, Batch 1400/4948, Batch loss: 0.00012179807526990771\n",
      "Memory Usage: 849.12 MB\n",
      "Epoch 94, Batch 1500/4948, Batch loss: 7.376816211035475e-05\n",
      "Memory Usage: 860.94 MB\n",
      "Epoch 94, Batch 1600/4948, Batch loss: 1.3151283383194823e-05\n",
      "Memory Usage: 872.72 MB\n",
      "Epoch 94, Batch 1700/4948, Batch loss: 0.00030103058088570833\n",
      "Memory Usage: 806.73 MB\n",
      "Epoch 94, Batch 1800/4948, Batch loss: 0.00026061898097395897\n",
      "Memory Usage: 776.50 MB\n",
      "Epoch 94, Batch 1900/4948, Batch loss: 0.0017142758006229997\n",
      "Memory Usage: 778.52 MB\n",
      "Epoch 94, Batch 2000/4948, Batch loss: 0.0004111625603400171\n",
      "Memory Usage: 780.91 MB\n",
      "Epoch 94, Batch 2100/4948, Batch loss: 0.0006396048702299595\n",
      "Memory Usage: 781.11 MB\n",
      "Epoch 94, Batch 2200/4948, Batch loss: 0.000103223261248786\n",
      "Memory Usage: 782.11 MB\n",
      "Epoch 94, Batch 2300/4948, Batch loss: 0.00021843759168405086\n",
      "Memory Usage: 782.12 MB\n",
      "Epoch 94, Batch 2400/4948, Batch loss: 0.00010586890857666731\n",
      "Memory Usage: 782.53 MB\n",
      "Epoch 94, Batch 2500/4948, Batch loss: 4.792830077349208e-05\n",
      "Memory Usage: 782.59 MB\n",
      "Epoch 94, Batch 2600/4948, Batch loss: 0.0003729271120391786\n",
      "Memory Usage: 782.59 MB\n",
      "Epoch 94, Batch 2700/4948, Batch loss: 0.0003382004506420344\n",
      "Memory Usage: 784.09 MB\n",
      "Epoch 94, Batch 2800/4948, Batch loss: 0.0010930163552984595\n",
      "Memory Usage: 784.19 MB\n",
      "Epoch 94, Batch 2900/4948, Batch loss: 0.00019974190217908472\n",
      "Memory Usage: 785.31 MB\n",
      "Epoch 94, Batch 3000/4948, Batch loss: 5.859617158421315e-05\n",
      "Memory Usage: 785.31 MB\n",
      "Epoch 94, Batch 3100/4948, Batch loss: 0.00012732320465147495\n",
      "Memory Usage: 786.47 MB\n",
      "Epoch 94, Batch 3200/4948, Batch loss: 0.0002804242249112576\n",
      "Memory Usage: 786.47 MB\n",
      "Epoch 94, Batch 3300/4948, Batch loss: 0.00013619997480418533\n",
      "Memory Usage: 786.50 MB\n",
      "Epoch 94, Batch 3400/4948, Batch loss: 6.02274030825356e-06\n",
      "Memory Usage: 786.50 MB\n",
      "Epoch 94, Batch 3500/4948, Batch loss: 4.8480535042472184e-05\n",
      "Memory Usage: 786.56 MB\n",
      "Epoch 94, Batch 3600/4948, Batch loss: 3.987898526247591e-05\n",
      "Memory Usage: 787.06 MB\n",
      "Epoch 94, Batch 3700/4948, Batch loss: 0.000558230618480593\n",
      "Memory Usage: 787.56 MB\n",
      "Epoch 94, Batch 3800/4948, Batch loss: 0.00010490300337551162\n",
      "Memory Usage: 787.56 MB\n",
      "Epoch 94, Batch 3900/4948, Batch loss: 0.00013659535034094006\n",
      "Memory Usage: 788.91 MB\n",
      "Epoch 94, Batch 4000/4948, Batch loss: 0.00014786302926950157\n",
      "Memory Usage: 774.09 MB\n",
      "Epoch 94, Batch 4100/4948, Batch loss: 4.752048698719591e-05\n",
      "Memory Usage: 776.34 MB\n",
      "Epoch 94, Batch 4200/4948, Batch loss: 0.0008641792228445411\n",
      "Memory Usage: 776.44 MB\n",
      "Epoch 94, Batch 4300/4948, Batch loss: 0.00014515407383441925\n",
      "Memory Usage: 776.44 MB\n",
      "Epoch 94, Batch 4400/4948, Batch loss: 4.018849267595215e-06\n",
      "Memory Usage: 776.45 MB\n",
      "Epoch 94, Batch 4500/4948, Batch loss: 0.0008087236201390624\n",
      "Memory Usage: 776.62 MB\n",
      "Epoch 94, Batch 4600/4948, Batch loss: 0.00023206262267194688\n",
      "Memory Usage: 776.78 MB\n",
      "Epoch 94, Batch 4700/4948, Batch loss: 0.0004087607958354056\n",
      "Memory Usage: 777.36 MB\n",
      "Epoch 94, Batch 4800/4948, Batch loss: 0.00011732480197679251\n",
      "Memory Usage: 710.17 MB\n",
      "Epoch 94, Batch 4900/4948, Batch loss: 7.52047126297839e-05\n",
      "Memory Usage: 712.22 MB\n",
      "Epoch 94, Batch 4948/4948, Batch loss: 0.0008752093999646604\n",
      "Memory Usage: 717.42 MB\n",
      "Epoch 94 completed in 66.54 seconds, Total Training Loss: 0.00027179771150194726\n",
      "\n",
      "Epoch 95/100\n",
      "Epoch 95, Batch 100/4948, Batch loss: 0.00016907340614125133\n",
      "Memory Usage: 722.08 MB\n",
      "Epoch 95, Batch 200/4948, Batch loss: 1.1280880244157743e-05\n",
      "Memory Usage: 722.11 MB\n",
      "Epoch 95, Batch 300/4948, Batch loss: 0.0002992529480252415\n",
      "Memory Usage: 653.02 MB\n",
      "Epoch 95, Batch 400/4948, Batch loss: 0.00011770615674322471\n",
      "Memory Usage: 701.42 MB\n",
      "Epoch 95, Batch 500/4948, Batch loss: 5.140867142472416e-05\n",
      "Memory Usage: 709.78 MB\n",
      "Epoch 95, Batch 600/4948, Batch loss: 0.00046810315689072013\n",
      "Memory Usage: 714.30 MB\n",
      "Epoch 95, Batch 700/4948, Batch loss: 5.77703322051093e-05\n",
      "Memory Usage: 717.89 MB\n",
      "Epoch 95, Batch 800/4948, Batch loss: 0.00012487407366279513\n",
      "Memory Usage: 718.39 MB\n",
      "Epoch 95, Batch 900/4948, Batch loss: 0.0004996393690817058\n",
      "Memory Usage: 719.39 MB\n",
      "Epoch 95, Batch 1000/4948, Batch loss: 8.53126184665598e-05\n",
      "Memory Usage: 727.55 MB\n",
      "Epoch 95, Batch 1100/4948, Batch loss: 0.0007678064284846187\n",
      "Memory Usage: 734.23 MB\n",
      "Epoch 95, Batch 1200/4948, Batch loss: 0.0009659678908064961\n",
      "Memory Usage: 747.05 MB\n",
      "Epoch 95, Batch 1300/4948, Batch loss: 0.00015357154188677669\n",
      "Memory Usage: 753.91 MB\n",
      "Epoch 95, Batch 1400/4948, Batch loss: 0.00012111742398701608\n",
      "Memory Usage: 754.48 MB\n",
      "Epoch 95, Batch 1500/4948, Batch loss: 7.471967546734959e-05\n",
      "Memory Usage: 755.56 MB\n",
      "Epoch 95, Batch 1600/4948, Batch loss: 1.628130667086225e-05\n",
      "Memory Usage: 755.59 MB\n",
      "Epoch 95, Batch 1700/4948, Batch loss: 0.0005139167769812047\n",
      "Memory Usage: 756.30 MB\n",
      "Epoch 95, Batch 1800/4948, Batch loss: 0.00025944982189685106\n",
      "Memory Usage: 763.86 MB\n",
      "Epoch 95, Batch 1900/4948, Batch loss: 0.0031052627600729465\n",
      "Memory Usage: 782.17 MB\n",
      "Epoch 95, Batch 2000/4948, Batch loss: 0.0003926046774722636\n",
      "Memory Usage: 793.97 MB\n",
      "Epoch 95, Batch 2100/4948, Batch loss: 0.0006686421111226082\n",
      "Memory Usage: 807.75 MB\n",
      "Epoch 95, Batch 2200/4948, Batch loss: 0.00010365715570515022\n",
      "Memory Usage: 819.55 MB\n",
      "Epoch 95, Batch 2300/4948, Batch loss: 0.00021844152070116252\n",
      "Memory Usage: 833.16 MB\n",
      "Epoch 95, Batch 2400/4948, Batch loss: 0.00010506559920031577\n",
      "Memory Usage: 844.91 MB\n",
      "Epoch 95, Batch 2500/4948, Batch loss: 4.947291745338589e-05\n",
      "Memory Usage: 857.98 MB\n",
      "Epoch 95, Batch 2600/4948, Batch loss: 0.00037589389830827713\n",
      "Memory Usage: 869.84 MB\n",
      "Epoch 95, Batch 2700/4948, Batch loss: 0.0003505369240883738\n",
      "Memory Usage: 875.73 MB\n",
      "Epoch 95, Batch 2800/4948, Batch loss: 0.001133164158090949\n",
      "Memory Usage: 875.84 MB\n",
      "Epoch 95, Batch 2900/4948, Batch loss: 0.00019805176998488605\n",
      "Memory Usage: 875.98 MB\n",
      "Epoch 95, Batch 3000/4948, Batch loss: 5.75780650251545e-05\n",
      "Memory Usage: 875.98 MB\n",
      "Epoch 95, Batch 3100/4948, Batch loss: 0.00012216386676300317\n",
      "Memory Usage: 876.02 MB\n",
      "Epoch 95, Batch 3200/4948, Batch loss: 0.0002930130285676569\n",
      "Memory Usage: 876.06 MB\n",
      "Epoch 95, Batch 3300/4948, Batch loss: 0.00013844174100086093\n",
      "Memory Usage: 876.14 MB\n",
      "Epoch 95, Batch 3400/4948, Batch loss: 8.609501492173877e-06\n",
      "Memory Usage: 876.14 MB\n",
      "Epoch 95, Batch 3500/4948, Batch loss: 4.764573532156646e-05\n",
      "Memory Usage: 876.64 MB\n",
      "Epoch 95, Batch 3600/4948, Batch loss: 4.088602145202458e-05\n",
      "Memory Usage: 876.67 MB\n",
      "Epoch 95, Batch 3700/4948, Batch loss: 0.0005436457577161491\n",
      "Memory Usage: 876.69 MB\n",
      "Epoch 95, Batch 3800/4948, Batch loss: 0.00010946790280286223\n",
      "Memory Usage: 877.20 MB\n",
      "Epoch 95, Batch 3900/4948, Batch loss: 0.00014585953613277525\n",
      "Memory Usage: 877.20 MB\n",
      "Epoch 95, Batch 4000/4948, Batch loss: 0.00014796869072597474\n",
      "Memory Usage: 877.20 MB\n",
      "Epoch 95, Batch 4100/4948, Batch loss: 4.7043762606335804e-05\n",
      "Memory Usage: 877.45 MB\n",
      "Epoch 95, Batch 4200/4948, Batch loss: 0.0008530098129995167\n",
      "Memory Usage: 877.47 MB\n",
      "Epoch 95, Batch 4300/4948, Batch loss: 0.00014625300536863506\n",
      "Memory Usage: 877.52 MB\n",
      "Epoch 95, Batch 4400/4948, Batch loss: 3.5423981898929924e-06\n",
      "Memory Usage: 877.52 MB\n",
      "Epoch 95, Batch 4500/4948, Batch loss: 0.0008074715151451528\n",
      "Memory Usage: 877.52 MB\n",
      "Epoch 95, Batch 4600/4948, Batch loss: 0.00023055431665852666\n",
      "Memory Usage: 877.52 MB\n",
      "Epoch 95, Batch 4700/4948, Batch loss: 0.00040298557723872364\n",
      "Memory Usage: 877.52 MB\n",
      "Epoch 95, Batch 4800/4948, Batch loss: 0.00011943175923079252\n",
      "Memory Usage: 878.03 MB\n",
      "Epoch 95, Batch 4900/4948, Batch loss: 7.6924407039769e-05\n",
      "Memory Usage: 878.03 MB\n",
      "Epoch 95, Batch 4948/4948, Batch loss: 0.0008558781119063497\n",
      "Memory Usage: 878.03 MB\n",
      "Epoch 95 completed in 74.09 seconds, Total Training Loss: 0.0002786111823078\n",
      "\n",
      "Epoch 96/100\n",
      "Epoch 96, Batch 100/4948, Batch loss: 0.00016989055438898504\n",
      "Memory Usage: 878.03 MB\n",
      "Epoch 96, Batch 200/4948, Batch loss: 1.1756468666135333e-05\n",
      "Memory Usage: 878.03 MB\n",
      "Epoch 96, Batch 300/4948, Batch loss: 0.00030290658469311893\n",
      "Memory Usage: 878.06 MB\n",
      "Epoch 96, Batch 400/4948, Batch loss: 0.00011666100181173533\n",
      "Memory Usage: 878.06 MB\n",
      "Epoch 96, Batch 500/4948, Batch loss: 5.153751408215612e-05\n",
      "Memory Usage: 878.06 MB\n",
      "Epoch 96, Batch 600/4948, Batch loss: 0.0004710609500762075\n",
      "Memory Usage: 878.08 MB\n",
      "Epoch 96, Batch 700/4948, Batch loss: 5.276497540762648e-05\n",
      "Memory Usage: 878.08 MB\n",
      "Epoch 96, Batch 800/4948, Batch loss: 0.00012506450002547354\n",
      "Memory Usage: 878.08 MB\n",
      "Epoch 96, Batch 900/4948, Batch loss: 0.0005052001215517521\n",
      "Memory Usage: 876.02 MB\n",
      "Epoch 96, Batch 1000/4948, Batch loss: 8.624850306659937e-05\n",
      "Memory Usage: 814.31 MB\n",
      "Epoch 96, Batch 1100/4948, Batch loss: 0.0007615577196702361\n",
      "Memory Usage: 820.20 MB\n",
      "Epoch 96, Batch 1200/4948, Batch loss: 0.000979326548986137\n",
      "Memory Usage: 825.25 MB\n",
      "Epoch 96, Batch 1300/4948, Batch loss: 0.00015399498806800693\n",
      "Memory Usage: 813.64 MB\n",
      "Epoch 96, Batch 1400/4948, Batch loss: 0.00012109597446396947\n",
      "Memory Usage: 687.70 MB\n",
      "Epoch 96, Batch 1500/4948, Batch loss: 7.252862269524485e-05\n",
      "Memory Usage: 544.03 MB\n",
      "Epoch 96, Batch 1600/4948, Batch loss: 1.323431933997199e-05\n",
      "Memory Usage: 544.34 MB\n",
      "Epoch 96, Batch 1700/4948, Batch loss: 0.000328971043927595\n",
      "Memory Usage: 542.41 MB\n",
      "Epoch 96, Batch 1800/4948, Batch loss: 0.0002594817487988621\n",
      "Memory Usage: 554.17 MB\n",
      "Epoch 96, Batch 1900/4948, Batch loss: 0.0030594647396355867\n",
      "Memory Usage: 558.94 MB\n",
      "Epoch 96, Batch 2000/4948, Batch loss: 0.00039319752249866724\n",
      "Memory Usage: 559.20 MB\n",
      "Epoch 96, Batch 2100/4948, Batch loss: 0.0006188435363583267\n",
      "Memory Usage: 561.72 MB\n",
      "Epoch 96, Batch 2200/4948, Batch loss: 0.00010420432954560965\n",
      "Memory Usage: 561.88 MB\n",
      "Epoch 96, Batch 2300/4948, Batch loss: 0.00022652516781818122\n",
      "Memory Usage: 564.09 MB\n",
      "Epoch 96, Batch 2400/4948, Batch loss: 0.0001057811823557131\n",
      "Memory Usage: 567.23 MB\n",
      "Epoch 96, Batch 2500/4948, Batch loss: 4.89609083160758e-05\n",
      "Memory Usage: 570.73 MB\n",
      "Epoch 96, Batch 2600/4948, Batch loss: 0.0003714153717737645\n",
      "Memory Usage: 572.75 MB\n",
      "Epoch 96, Batch 2700/4948, Batch loss: 0.00034152402076870203\n",
      "Memory Usage: 573.78 MB\n",
      "Epoch 96, Batch 2800/4948, Batch loss: 0.0011166566982865334\n",
      "Memory Usage: 573.78 MB\n",
      "Epoch 96, Batch 2900/4948, Batch loss: 0.00019584815890993923\n",
      "Memory Usage: 573.80 MB\n",
      "Epoch 96, Batch 3000/4948, Batch loss: 5.7608533097663894e-05\n",
      "Memory Usage: 574.30 MB\n",
      "Epoch 96, Batch 3100/4948, Batch loss: 0.00012376985978335142\n",
      "Memory Usage: 575.28 MB\n",
      "Epoch 96, Batch 3200/4948, Batch loss: 0.00029209203785285354\n",
      "Memory Usage: 575.28 MB\n",
      "Epoch 96, Batch 3300/4948, Batch loss: 0.00013993862376082689\n",
      "Memory Usage: 575.28 MB\n",
      "Epoch 96, Batch 3400/4948, Batch loss: 6.25006168775144e-06\n",
      "Memory Usage: 577.12 MB\n",
      "Epoch 96, Batch 3500/4948, Batch loss: 4.568639269564301e-05\n",
      "Memory Usage: 577.12 MB\n",
      "Epoch 96, Batch 3600/4948, Batch loss: 4.033001096104272e-05\n",
      "Memory Usage: 578.12 MB\n",
      "Epoch 96, Batch 3700/4948, Batch loss: 0.0005786676192656159\n",
      "Memory Usage: 584.06 MB\n",
      "Epoch 96, Batch 3800/4948, Batch loss: 0.0001061162693076767\n",
      "Memory Usage: 595.88 MB\n",
      "Epoch 96, Batch 3900/4948, Batch loss: 0.0001478056365158409\n",
      "Memory Usage: 607.88 MB\n",
      "Epoch 96, Batch 4000/4948, Batch loss: 0.00014961024862714112\n",
      "Memory Usage: 619.50 MB\n",
      "Epoch 96, Batch 4100/4948, Batch loss: 4.488333433982916e-05\n",
      "Memory Usage: 631.09 MB\n",
      "Epoch 96, Batch 4200/4948, Batch loss: 0.0008390183211304247\n",
      "Memory Usage: 642.64 MB\n",
      "Epoch 96, Batch 4300/4948, Batch loss: 0.00014678516890853643\n",
      "Memory Usage: 654.81 MB\n",
      "Epoch 96, Batch 4400/4948, Batch loss: 3.852031568385428e-06\n",
      "Memory Usage: 666.36 MB\n",
      "Epoch 96, Batch 4500/4948, Batch loss: 0.0008186325430870056\n",
      "Memory Usage: 677.95 MB\n",
      "Epoch 96, Batch 4600/4948, Batch loss: 0.0002323326625628397\n",
      "Memory Usage: 689.66 MB\n",
      "Epoch 96, Batch 4700/4948, Batch loss: 0.00040895608253777027\n",
      "Memory Usage: 701.36 MB\n",
      "Epoch 96, Batch 4800/4948, Batch loss: 0.00013766821939498186\n",
      "Memory Usage: 712.92 MB\n",
      "Epoch 96, Batch 4900/4948, Batch loss: 7.10519525455311e-05\n",
      "Memory Usage: 724.78 MB\n",
      "Epoch 96, Batch 4948/4948, Batch loss: 0.0008359848288819194\n",
      "Memory Usage: 730.45 MB\n",
      "Epoch 96 completed in 83.53 seconds, Total Training Loss: 0.0002765356539917402\n",
      "\n",
      "Epoch 97/100\n",
      "Epoch 97, Batch 100/4948, Batch loss: 0.00016873539425432682\n",
      "Memory Usage: 742.14 MB\n",
      "Epoch 97, Batch 200/4948, Batch loss: 1.1653723959170748e-05\n",
      "Memory Usage: 753.92 MB\n",
      "Epoch 97, Batch 300/4948, Batch loss: 0.0003018394054379314\n",
      "Memory Usage: 766.77 MB\n",
      "Epoch 97, Batch 400/4948, Batch loss: 0.00011770445416914299\n",
      "Memory Usage: 778.77 MB\n",
      "Epoch 97, Batch 500/4948, Batch loss: 5.109851554152556e-05\n",
      "Memory Usage: 790.92 MB\n",
      "Epoch 97, Batch 600/4948, Batch loss: 0.00046656859922222793\n",
      "Memory Usage: 802.45 MB\n",
      "Epoch 97, Batch 700/4948, Batch loss: 0.00010777783609228209\n",
      "Memory Usage: 814.20 MB\n",
      "Epoch 97, Batch 800/4948, Batch loss: 0.00012483744649216533\n",
      "Memory Usage: 826.02 MB\n",
      "Epoch 97, Batch 900/4948, Batch loss: 0.0004855967126786709\n",
      "Memory Usage: 837.83 MB\n",
      "Epoch 97, Batch 1000/4948, Batch loss: 8.534935477655381e-05\n",
      "Memory Usage: 841.00 MB\n",
      "Epoch 97, Batch 1100/4948, Batch loss: 0.0007849393878132105\n",
      "Memory Usage: 841.00 MB\n",
      "Epoch 97, Batch 1200/4948, Batch loss: 0.0009547896916046739\n",
      "Memory Usage: 841.09 MB\n",
      "Epoch 97, Batch 1300/4948, Batch loss: 0.00015429180348291993\n",
      "Memory Usage: 841.12 MB\n",
      "Epoch 97, Batch 1400/4948, Batch loss: 0.00012170882837381214\n",
      "Memory Usage: 841.12 MB\n",
      "Epoch 97, Batch 1500/4948, Batch loss: 7.39072056603618e-05\n",
      "Memory Usage: 841.12 MB\n",
      "Epoch 97, Batch 1600/4948, Batch loss: 1.4013478903507348e-05\n",
      "Memory Usage: 841.12 MB\n",
      "Epoch 97, Batch 1700/4948, Batch loss: 0.0003306126454845071\n",
      "Memory Usage: 841.12 MB\n",
      "Epoch 97, Batch 1800/4948, Batch loss: 0.000260863802395761\n",
      "Memory Usage: 841.12 MB\n",
      "Epoch 97, Batch 1900/4948, Batch loss: 0.0023431116715073586\n",
      "Memory Usage: 841.12 MB\n",
      "Epoch 97, Batch 2000/4948, Batch loss: 0.0003778591053560376\n",
      "Memory Usage: 841.14 MB\n",
      "Epoch 97, Batch 2100/4948, Batch loss: 0.0006001958390697837\n",
      "Memory Usage: 841.14 MB\n",
      "Epoch 97, Batch 2200/4948, Batch loss: 0.00010364289482822642\n",
      "Memory Usage: 841.16 MB\n",
      "Epoch 97, Batch 2300/4948, Batch loss: 0.00021407075109891593\n",
      "Memory Usage: 841.16 MB\n",
      "Epoch 97, Batch 2400/4948, Batch loss: 0.00010646883310982957\n",
      "Memory Usage: 841.16 MB\n",
      "Epoch 97, Batch 2500/4948, Batch loss: 4.8089990741573274e-05\n",
      "Memory Usage: 841.66 MB\n",
      "Epoch 97, Batch 2600/4948, Batch loss: 0.0003698551736306399\n",
      "Memory Usage: 841.66 MB\n",
      "Epoch 97, Batch 2700/4948, Batch loss: 0.0003327101294416934\n",
      "Memory Usage: 841.66 MB\n",
      "Epoch 97, Batch 2800/4948, Batch loss: 0.0011014683404937387\n",
      "Memory Usage: 841.66 MB\n",
      "Epoch 97, Batch 2900/4948, Batch loss: 0.00019741580763366073\n",
      "Memory Usage: 841.66 MB\n",
      "Epoch 97, Batch 3000/4948, Batch loss: 5.8665231335908175e-05\n",
      "Memory Usage: 841.66 MB\n",
      "Epoch 97, Batch 3100/4948, Batch loss: 0.00012359254469629377\n",
      "Memory Usage: 841.66 MB\n",
      "Epoch 97, Batch 3200/4948, Batch loss: 0.0002847655559889972\n",
      "Memory Usage: 841.66 MB\n",
      "Epoch 97, Batch 3300/4948, Batch loss: 0.00013792853860650212\n",
      "Memory Usage: 841.66 MB\n",
      "Epoch 97, Batch 3400/4948, Batch loss: 5.83406244913931e-06\n",
      "Memory Usage: 841.66 MB\n",
      "Epoch 97, Batch 3500/4948, Batch loss: 4.813294799532741e-05\n",
      "Memory Usage: 841.66 MB\n",
      "Epoch 97, Batch 3600/4948, Batch loss: 3.9804082916816697e-05\n",
      "Memory Usage: 841.66 MB\n",
      "Epoch 97, Batch 3700/4948, Batch loss: 0.0005644413759000599\n",
      "Memory Usage: 841.67 MB\n",
      "Epoch 97, Batch 3800/4948, Batch loss: 0.00011174999235663563\n",
      "Memory Usage: 841.73 MB\n",
      "Epoch 97, Batch 3900/4948, Batch loss: 0.0001456605677958578\n",
      "Memory Usage: 841.73 MB\n",
      "Epoch 97, Batch 4000/4948, Batch loss: 0.00014767494576517493\n",
      "Memory Usage: 841.73 MB\n",
      "Epoch 97, Batch 4100/4948, Batch loss: 4.493273081607185e-05\n",
      "Memory Usage: 841.73 MB\n",
      "Epoch 97, Batch 4200/4948, Batch loss: 0.0008507354068569839\n",
      "Memory Usage: 841.73 MB\n",
      "Epoch 97, Batch 4300/4948, Batch loss: 0.00014721443585585803\n",
      "Memory Usage: 841.73 MB\n",
      "Epoch 97, Batch 4400/4948, Batch loss: 3.4031604627671186e-06\n",
      "Memory Usage: 842.05 MB\n",
      "Epoch 97, Batch 4500/4948, Batch loss: 0.0007935031317174435\n",
      "Memory Usage: 842.05 MB\n",
      "Epoch 97, Batch 4600/4948, Batch loss: 0.00023144896840676665\n",
      "Memory Usage: 842.55 MB\n",
      "Epoch 97, Batch 4700/4948, Batch loss: 0.0004078327037859708\n",
      "Memory Usage: 842.55 MB\n",
      "Epoch 97, Batch 4800/4948, Batch loss: 0.0001236969546880573\n",
      "Memory Usage: 843.05 MB\n",
      "Epoch 97, Batch 4900/4948, Batch loss: 7.64343494665809e-05\n",
      "Memory Usage: 843.05 MB\n",
      "Epoch 97, Batch 4948/4948, Batch loss: 0.0008618521969765425\n",
      "Memory Usage: 843.05 MB\n",
      "Epoch 97 completed in 75.47 seconds, Total Training Loss: 0.0002724231498802735\n",
      "\n",
      "Epoch 98/100\n",
      "Epoch 98, Batch 100/4948, Batch loss: 0.00016920360212679952\n",
      "Memory Usage: 843.05 MB\n",
      "Epoch 98, Batch 200/4948, Batch loss: 1.4635973457188811e-05\n",
      "Memory Usage: 843.05 MB\n",
      "Epoch 98, Batch 300/4948, Batch loss: 0.0002976476098410785\n",
      "Memory Usage: 843.36 MB\n",
      "Epoch 98, Batch 400/4948, Batch loss: 0.00011728792014764622\n",
      "Memory Usage: 843.36 MB\n",
      "Epoch 98, Batch 500/4948, Batch loss: 5.112131839268841e-05\n",
      "Memory Usage: 843.36 MB\n",
      "Epoch 98, Batch 600/4948, Batch loss: 0.0004669414774980396\n",
      "Memory Usage: 843.38 MB\n",
      "Epoch 98, Batch 700/4948, Batch loss: 5.259172758087516e-05\n",
      "Memory Usage: 843.38 MB\n",
      "Epoch 98, Batch 800/4948, Batch loss: 0.0001249622437171638\n",
      "Memory Usage: 843.38 MB\n",
      "Epoch 98, Batch 900/4948, Batch loss: 0.000493162136990577\n",
      "Memory Usage: 843.38 MB\n",
      "Epoch 98, Batch 1000/4948, Batch loss: 8.53909004945308e-05\n",
      "Memory Usage: 843.38 MB\n",
      "Epoch 98, Batch 1100/4948, Batch loss: 0.000774088257458061\n",
      "Memory Usage: 843.38 MB\n",
      "Epoch 98, Batch 1200/4948, Batch loss: 0.0009517116122879088\n",
      "Memory Usage: 843.38 MB\n",
      "Epoch 98, Batch 1300/4948, Batch loss: 0.00015459077258128673\n",
      "Memory Usage: 843.38 MB\n",
      "Epoch 98, Batch 1400/4948, Batch loss: 0.0001211473936564289\n",
      "Memory Usage: 843.38 MB\n",
      "Epoch 98, Batch 1500/4948, Batch loss: 7.386841025436297e-05\n",
      "Memory Usage: 843.38 MB\n",
      "Epoch 98, Batch 1600/4948, Batch loss: 1.5302173778763972e-05\n",
      "Memory Usage: 843.39 MB\n",
      "Epoch 98, Batch 1700/4948, Batch loss: 0.0003301382821518928\n",
      "Memory Usage: 843.39 MB\n",
      "Epoch 98, Batch 1800/4948, Batch loss: 0.00025865939096547663\n",
      "Memory Usage: 843.39 MB\n",
      "Epoch 98, Batch 1900/4948, Batch loss: 0.0013438952155411243\n",
      "Memory Usage: 843.39 MB\n",
      "Epoch 98, Batch 2000/4948, Batch loss: 0.00037620196235366166\n",
      "Memory Usage: 843.39 MB\n",
      "Epoch 98, Batch 2100/4948, Batch loss: 0.0006075386772863567\n",
      "Memory Usage: 843.39 MB\n",
      "Epoch 98, Batch 2200/4948, Batch loss: 0.00010196803486905992\n",
      "Memory Usage: 843.39 MB\n",
      "Epoch 98, Batch 2300/4948, Batch loss: 0.0002242986229248345\n",
      "Memory Usage: 843.39 MB\n",
      "Epoch 98, Batch 2400/4948, Batch loss: 0.00010610582830850035\n",
      "Memory Usage: 843.39 MB\n",
      "Epoch 98, Batch 2500/4948, Batch loss: 4.80088492622599e-05\n",
      "Memory Usage: 843.39 MB\n",
      "Epoch 98, Batch 2600/4948, Batch loss: 0.00036890170304104686\n",
      "Memory Usage: 843.39 MB\n",
      "Epoch 98, Batch 2700/4948, Batch loss: 0.00036271833232603967\n",
      "Memory Usage: 843.39 MB\n",
      "Epoch 98, Batch 2800/4948, Batch loss: 0.001081080292351544\n",
      "Memory Usage: 843.41 MB\n",
      "Epoch 98, Batch 2900/4948, Batch loss: 0.00020060216775164008\n",
      "Memory Usage: 843.41 MB\n",
      "Epoch 98, Batch 3000/4948, Batch loss: 5.849102672073059e-05\n",
      "Memory Usage: 843.41 MB\n",
      "Epoch 98, Batch 3100/4948, Batch loss: 0.00012320090900175273\n",
      "Memory Usage: 843.89 MB\n",
      "Epoch 98, Batch 3200/4948, Batch loss: 0.00027712518931366503\n",
      "Memory Usage: 843.89 MB\n",
      "Epoch 98, Batch 3300/4948, Batch loss: 0.00013682592543773353\n",
      "Memory Usage: 843.89 MB\n",
      "Epoch 98, Batch 3400/4948, Batch loss: 5.2646846597781405e-06\n",
      "Memory Usage: 843.89 MB\n",
      "Epoch 98, Batch 3500/4948, Batch loss: 5.4136318794917315e-05\n",
      "Memory Usage: 843.89 MB\n",
      "Epoch 98, Batch 3600/4948, Batch loss: 4.085121690877713e-05\n",
      "Memory Usage: 843.89 MB\n",
      "Epoch 98, Batch 3700/4948, Batch loss: 0.00056663021678105\n",
      "Memory Usage: 843.89 MB\n",
      "Epoch 98, Batch 3800/4948, Batch loss: 0.00010751865193014964\n",
      "Memory Usage: 843.89 MB\n",
      "Epoch 98, Batch 3900/4948, Batch loss: 0.00014220249431673437\n",
      "Memory Usage: 843.89 MB\n",
      "Epoch 98, Batch 4000/4948, Batch loss: 0.0001469153503421694\n",
      "Memory Usage: 843.89 MB\n",
      "Epoch 98, Batch 4100/4948, Batch loss: 4.4739364966517314e-05\n",
      "Memory Usage: 843.91 MB\n",
      "Epoch 98, Batch 4200/4948, Batch loss: 0.0008471371256746352\n",
      "Memory Usage: 843.91 MB\n",
      "Epoch 98, Batch 4300/4948, Batch loss: 0.0001451295829610899\n",
      "Memory Usage: 843.91 MB\n",
      "Epoch 98, Batch 4400/4948, Batch loss: 3.529043851813185e-06\n",
      "Memory Usage: 843.91 MB\n",
      "Epoch 98, Batch 4500/4948, Batch loss: 0.0008020123932510614\n",
      "Memory Usage: 843.91 MB\n",
      "Epoch 98, Batch 4600/4948, Batch loss: 0.00023488010629080236\n",
      "Memory Usage: 843.91 MB\n",
      "Epoch 98, Batch 4700/4948, Batch loss: 0.0004091164446435869\n",
      "Memory Usage: 843.91 MB\n",
      "Epoch 98, Batch 4800/4948, Batch loss: 0.0001570462918607518\n",
      "Memory Usage: 843.91 MB\n",
      "Epoch 98, Batch 4900/4948, Batch loss: 7.674545486224815e-05\n",
      "Memory Usage: 844.41 MB\n",
      "Epoch 98, Batch 4948/4948, Batch loss: 0.0008802333613857627\n",
      "Memory Usage: 844.41 MB\n",
      "Epoch 98 completed in 76.38 seconds, Total Training Loss: 0.00026959801853760853\n",
      "\n",
      "Epoch 99/100\n",
      "Epoch 99, Batch 100/4948, Batch loss: 0.00016855495050549507\n",
      "Memory Usage: 844.41 MB\n",
      "Epoch 99, Batch 200/4948, Batch loss: 1.0481685421837028e-05\n",
      "Memory Usage: 844.41 MB\n",
      "Epoch 99, Batch 300/4948, Batch loss: 0.0002947289030998945\n",
      "Memory Usage: 844.91 MB\n",
      "Epoch 99, Batch 400/4948, Batch loss: 0.00011699989408953115\n",
      "Memory Usage: 844.91 MB\n",
      "Epoch 99, Batch 500/4948, Batch loss: 5.1126389735145494e-05\n",
      "Memory Usage: 844.91 MB\n",
      "Epoch 99, Batch 600/4948, Batch loss: 0.0004684640734922141\n",
      "Memory Usage: 844.91 MB\n",
      "Epoch 99, Batch 700/4948, Batch loss: 6.843764276709408e-05\n",
      "Memory Usage: 844.92 MB\n",
      "Epoch 99, Batch 800/4948, Batch loss: 0.00012387294555082917\n",
      "Memory Usage: 844.92 MB\n",
      "Epoch 99, Batch 900/4948, Batch loss: 0.0004951039445586503\n",
      "Memory Usage: 844.92 MB\n",
      "Epoch 99, Batch 1000/4948, Batch loss: 8.580157009419054e-05\n",
      "Memory Usage: 844.92 MB\n",
      "Epoch 99, Batch 1100/4948, Batch loss: 0.0007636980153620243\n",
      "Memory Usage: 844.92 MB\n",
      "Epoch 99, Batch 1200/4948, Batch loss: 0.0009360780823044479\n",
      "Memory Usage: 844.92 MB\n",
      "Epoch 99, Batch 1300/4948, Batch loss: 0.00015643858932889998\n",
      "Memory Usage: 844.92 MB\n",
      "Epoch 99, Batch 1400/4948, Batch loss: 0.00012099165178369731\n",
      "Memory Usage: 844.92 MB\n",
      "Epoch 99, Batch 1500/4948, Batch loss: 7.371849642368034e-05\n",
      "Memory Usage: 844.92 MB\n",
      "Epoch 99, Batch 1600/4948, Batch loss: 1.537579737487249e-05\n",
      "Memory Usage: 845.42 MB\n",
      "Epoch 99, Batch 1700/4948, Batch loss: 0.0003651272563729435\n",
      "Memory Usage: 844.73 MB\n",
      "Epoch 99, Batch 1800/4948, Batch loss: 0.00025830796221271157\n",
      "Memory Usage: 844.78 MB\n",
      "Epoch 99, Batch 1900/4948, Batch loss: 0.0032229814678430557\n",
      "Memory Usage: 844.11 MB\n",
      "Epoch 99, Batch 2000/4948, Batch loss: 0.0003922191390302032\n",
      "Memory Usage: 844.58 MB\n",
      "Epoch 99, Batch 2100/4948, Batch loss: 0.0006114643183536828\n",
      "Memory Usage: 844.58 MB\n",
      "Epoch 99, Batch 2200/4948, Batch loss: 0.00010221865522908047\n",
      "Memory Usage: 844.58 MB\n",
      "Epoch 99, Batch 2300/4948, Batch loss: 0.00021965250198263675\n",
      "Memory Usage: 844.88 MB\n",
      "Epoch 99, Batch 2400/4948, Batch loss: 0.00010397897131042555\n",
      "Memory Usage: 844.88 MB\n",
      "Epoch 99, Batch 2500/4948, Batch loss: 4.9223755922866985e-05\n",
      "Memory Usage: 844.88 MB\n",
      "Epoch 99, Batch 2600/4948, Batch loss: 0.0003734000201802701\n",
      "Memory Usage: 823.73 MB\n",
      "Epoch 99, Batch 2700/4948, Batch loss: 0.00034176584449596703\n",
      "Memory Usage: 837.02 MB\n",
      "Epoch 99, Batch 2800/4948, Batch loss: 0.00108082196675241\n",
      "Memory Usage: 828.84 MB\n",
      "Epoch 99, Batch 2900/4948, Batch loss: 0.00019703852012753487\n",
      "Memory Usage: 830.14 MB\n",
      "Epoch 99, Batch 3000/4948, Batch loss: 5.7856112107401714e-05\n",
      "Memory Usage: 832.75 MB\n",
      "Epoch 99, Batch 3100/4948, Batch loss: 0.00012115259596612304\n",
      "Memory Usage: 833.25 MB\n",
      "Epoch 99, Batch 3200/4948, Batch loss: 0.0003042549069505185\n",
      "Memory Usage: 834.17 MB\n",
      "Epoch 99, Batch 3300/4948, Batch loss: 0.00013865205983165652\n",
      "Memory Usage: 834.25 MB\n",
      "Epoch 99, Batch 3400/4948, Batch loss: 7.148148597480031e-06\n",
      "Memory Usage: 835.25 MB\n",
      "Epoch 99, Batch 3500/4948, Batch loss: 4.5830649469280615e-05\n",
      "Memory Usage: 835.25 MB\n",
      "Epoch 99, Batch 3600/4948, Batch loss: 4.206457015243359e-05\n",
      "Memory Usage: 835.69 MB\n",
      "Epoch 99, Batch 3700/4948, Batch loss: 0.0005456784274429083\n",
      "Memory Usage: 835.69 MB\n",
      "Epoch 99, Batch 3800/4948, Batch loss: 0.00010558999929344282\n",
      "Memory Usage: 835.69 MB\n",
      "Epoch 99, Batch 3900/4948, Batch loss: 0.00014211729285307229\n",
      "Memory Usage: 835.69 MB\n",
      "Epoch 99, Batch 4000/4948, Batch loss: 0.00014749258116353303\n",
      "Memory Usage: 836.19 MB\n",
      "Epoch 99, Batch 4100/4948, Batch loss: 4.575819184537977e-05\n",
      "Memory Usage: 836.19 MB\n",
      "Epoch 99, Batch 4200/4948, Batch loss: 0.0008380376966670156\n",
      "Memory Usage: 836.19 MB\n",
      "Epoch 99, Batch 4300/4948, Batch loss: 0.00014750946138519794\n",
      "Memory Usage: 836.69 MB\n",
      "Epoch 99, Batch 4400/4948, Batch loss: 3.3846158657979686e-06\n",
      "Memory Usage: 837.48 MB\n",
      "Epoch 99, Batch 4500/4948, Batch loss: 0.0007888911059126258\n",
      "Memory Usage: 837.48 MB\n",
      "Epoch 99, Batch 4600/4948, Batch loss: 0.0002339191996725276\n",
      "Memory Usage: 837.48 MB\n",
      "Epoch 99, Batch 4700/4948, Batch loss: 0.0004082310770172626\n",
      "Memory Usage: 837.95 MB\n",
      "Epoch 99, Batch 4800/4948, Batch loss: 0.00017136793758254498\n",
      "Memory Usage: 837.95 MB\n",
      "Epoch 99, Batch 4900/4948, Batch loss: 7.197350350907072e-05\n",
      "Memory Usage: 838.27 MB\n",
      "Epoch 99, Batch 4948/4948, Batch loss: 0.0008858260698616505\n",
      "Memory Usage: 838.27 MB\n",
      "Epoch 99 completed in 78.87 seconds, Total Training Loss: 0.0002749568724461563\n",
      "\n",
      "Epoch 100/100\n",
      "Epoch 100, Batch 100/4948, Batch loss: 0.00017022452084347606\n",
      "Memory Usage: 838.27 MB\n",
      "Epoch 100, Batch 200/4948, Batch loss: 1.0693040167097934e-05\n",
      "Memory Usage: 838.27 MB\n",
      "Epoch 100, Batch 300/4948, Batch loss: 0.000298446073429659\n",
      "Memory Usage: 838.27 MB\n",
      "Epoch 100, Batch 400/4948, Batch loss: 0.00011812948650913313\n",
      "Memory Usage: 838.27 MB\n",
      "Epoch 100, Batch 500/4948, Batch loss: 5.167175913811661e-05\n",
      "Memory Usage: 839.19 MB\n",
      "Epoch 100, Batch 600/4948, Batch loss: 0.0004680764686781913\n",
      "Memory Usage: 839.19 MB\n",
      "Epoch 100, Batch 700/4948, Batch loss: 5.0946448027389124e-05\n",
      "Memory Usage: 839.19 MB\n",
      "Epoch 100, Batch 800/4948, Batch loss: 0.00012641746434383094\n",
      "Memory Usage: 839.19 MB\n",
      "Epoch 100, Batch 900/4948, Batch loss: 0.0004850457771681249\n",
      "Memory Usage: 839.19 MB\n",
      "Epoch 100, Batch 1000/4948, Batch loss: 8.602932211942971e-05\n",
      "Memory Usage: 839.19 MB\n",
      "Epoch 100, Batch 1100/4948, Batch loss: 0.0007537010824307799\n",
      "Memory Usage: 840.02 MB\n",
      "Epoch 100, Batch 1200/4948, Batch loss: 0.0008998701814562082\n",
      "Memory Usage: 840.02 MB\n",
      "Epoch 100, Batch 1300/4948, Batch loss: 0.00015056945267133415\n",
      "Memory Usage: 840.02 MB\n",
      "Epoch 100, Batch 1400/4948, Batch loss: 0.00012380174302961677\n",
      "Memory Usage: 840.02 MB\n",
      "Epoch 100, Batch 1500/4948, Batch loss: 7.259754784172401e-05\n",
      "Memory Usage: 840.02 MB\n",
      "Epoch 100, Batch 1600/4948, Batch loss: 1.7325486624031328e-05\n",
      "Memory Usage: 840.02 MB\n",
      "Epoch 100, Batch 1700/4948, Batch loss: 0.00038577101076953113\n",
      "Memory Usage: 840.02 MB\n",
      "Epoch 100, Batch 1800/4948, Batch loss: 0.0002579224237706512\n",
      "Memory Usage: 840.02 MB\n",
      "Epoch 100, Batch 1900/4948, Batch loss: 0.0024985752534121275\n",
      "Memory Usage: 840.02 MB\n",
      "Epoch 100, Batch 2000/4948, Batch loss: 0.00037039403105154634\n",
      "Memory Usage: 840.02 MB\n",
      "Epoch 100, Batch 2100/4948, Batch loss: 0.000633120012935251\n",
      "Memory Usage: 840.02 MB\n",
      "Epoch 100, Batch 2200/4948, Batch loss: 0.00010428622044855729\n",
      "Memory Usage: 840.25 MB\n",
      "Epoch 100, Batch 2300/4948, Batch loss: 0.00022026691294740885\n",
      "Memory Usage: 840.84 MB\n",
      "Epoch 100, Batch 2400/4948, Batch loss: 0.00010513733286643401\n",
      "Memory Usage: 788.70 MB\n",
      "Epoch 100, Batch 2500/4948, Batch loss: 4.781517418450676e-05\n",
      "Memory Usage: 789.59 MB\n",
      "Epoch 100, Batch 2600/4948, Batch loss: 0.0003705993585754186\n",
      "Memory Usage: 789.95 MB\n",
      "Epoch 100, Batch 2700/4948, Batch loss: 0.00033882458228617907\n",
      "Memory Usage: 790.47 MB\n",
      "Epoch 100, Batch 2800/4948, Batch loss: 0.0010735407704487443\n",
      "Memory Usage: 791.00 MB\n",
      "Epoch 100, Batch 2900/4948, Batch loss: 0.0001985851995414123\n",
      "Memory Usage: 791.50 MB\n",
      "Epoch 100, Batch 3000/4948, Batch loss: 5.8235735195921734e-05\n",
      "Memory Usage: 791.50 MB\n",
      "Epoch 100, Batch 3100/4948, Batch loss: 0.0001245331804966554\n",
      "Memory Usage: 791.59 MB\n",
      "Epoch 100, Batch 3200/4948, Batch loss: 0.00027777490322478116\n",
      "Memory Usage: 791.59 MB\n",
      "Epoch 100, Batch 3300/4948, Batch loss: 0.0001389258832205087\n",
      "Memory Usage: 791.59 MB\n",
      "Epoch 100, Batch 3400/4948, Batch loss: 6.874486643937416e-06\n",
      "Memory Usage: 791.62 MB\n",
      "Epoch 100, Batch 3500/4948, Batch loss: 4.562485992209986e-05\n",
      "Memory Usage: 791.62 MB\n",
      "Epoch 100, Batch 3600/4948, Batch loss: 4.040582643938251e-05\n",
      "Memory Usage: 791.62 MB\n",
      "Epoch 100, Batch 3700/4948, Batch loss: 0.0005787888658232987\n",
      "Memory Usage: 791.64 MB\n",
      "Epoch 100, Batch 3800/4948, Batch loss: 0.00010494601883692667\n",
      "Memory Usage: 791.64 MB\n",
      "Epoch 100, Batch 3900/4948, Batch loss: 0.00014853515313006938\n",
      "Memory Usage: 791.64 MB\n",
      "Epoch 100, Batch 4000/4948, Batch loss: 0.00014760853082407266\n",
      "Memory Usage: 791.64 MB\n",
      "Epoch 100, Batch 4100/4948, Batch loss: 4.489109414862469e-05\n",
      "Memory Usage: 791.66 MB\n",
      "Epoch 100, Batch 4200/4948, Batch loss: 0.0008426731219515204\n",
      "Memory Usage: 791.72 MB\n",
      "Epoch 100, Batch 4300/4948, Batch loss: 0.00014569456106983125\n",
      "Memory Usage: 791.72 MB\n",
      "Epoch 100, Batch 4400/4948, Batch loss: 3.5278765153634595e-06\n",
      "Memory Usage: 791.72 MB\n",
      "Epoch 100, Batch 4500/4948, Batch loss: 0.0007769432268105447\n",
      "Memory Usage: 791.72 MB\n",
      "Epoch 100, Batch 4600/4948, Batch loss: 0.00022908633400220424\n",
      "Memory Usage: 791.72 MB\n",
      "Epoch 100, Batch 4700/4948, Batch loss: 0.00040818232810124755\n",
      "Memory Usage: 792.00 MB\n",
      "Epoch 100, Batch 4800/4948, Batch loss: 0.00012432580115273595\n",
      "Memory Usage: 792.00 MB\n",
      "Epoch 100, Batch 4900/4948, Batch loss: 7.280492718564346e-05\n",
      "Memory Usage: 792.00 MB\n",
      "Epoch 100, Batch 4948/4948, Batch loss: 0.000853834964800626\n",
      "Memory Usage: 792.00 MB\n",
      "Epoch 100 completed in 74.40 seconds, Total Training Loss: 0.00027190566337168757\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import psutil  # To monitor system resources\n",
    "\n",
    "model = NBeatsNet(input_size=X_train_combined.shape[1], forecast_length=1, layer_size=128)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()\n",
    "patience = 10\n",
    "epochs = 100\n",
    "batch_size = 1000\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "total_batches = (X_train_tensor.size(0) + batch_size - 1) // batch_size  # Total number of batches\n",
    "print(\"Starting the model training\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "        batch_num = i // batch_size + 1\n",
    "        X_batch = X_train_tensor[i:i + batch_size]\n",
    "        y_batch = y_train_tensor[i:i + batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_num % 100 == 0 or batch_num == total_batches:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_num}/{total_batches}, Batch loss: {loss.item()}\")\n",
    "            print(f\"Memory Usage: {psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds, Total Training Loss: {total_loss / total_batches}\")\n",
    "\n",
    "    if epoch % 10 == 0 or epochs_no_improve >= patience:\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_start_time = time.time()\n",
    "\n",
    "        for i in range(0, X_val_tensor.size(0), batch_size):\n",
    "            X_val_batch = X_val_tensor[i:i + batch_size]\n",
    "            y_val_batch = y_val_tensor[i:i + batch_size]\n",
    "\n",
    "            val_output = model(X_val_batch)\n",
    "            val_loss = criterion(val_output, y_val_batch)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        val_time = time.time() - val_start_time\n",
    "        print(f'Validation completed in {val_time:.2f} seconds, Average Validation Loss: {avg_val_loss}')\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the model checkpoint\n",
    "            torch.save(model.state_dict(), 'model_checkpoint.pth')\n",
    "            print(\"Model improved and saved.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(\"No improvement in model.\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Stopping early at epoch {epoch+1} due to no improvement in validation loss.')\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:15:53.475199Z",
     "start_time": "2024-03-26T10:15:30.604257Z"
    }
   },
   "id": "cb21403c80a2ef73",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: RMSE=0.036910660564899445, MAE=0.028544999659061432, MAPE=10.017461329698563%\n",
      "AAL: RMSE=0.019661439582705498, MAE=0.01440964825451374, MAPE=2.034720778465271%\n",
      "AAPL: RMSE=0.04166710749268532, MAE=0.03225204721093178, MAPE=5.439389497041702%\n",
      "ABBV: RMSE=0.03779784217476845, MAE=0.029392343014478683, MAPE=4.487305134534836%\n",
      "ABT: RMSE=0.03339433670043945, MAE=0.025164322927594185, MAPE=5.492720752954483%\n",
      "ACGL: RMSE=0.03295241668820381, MAE=0.022761845961213112, MAPE=8.719033002853394%\n",
      "ACN: RMSE=0.0350993312895298, MAE=0.025914296507835388, MAPE=7.840503752231598%\n",
      "ADBE: RMSE=0.04043014347553253, MAE=0.02895490452647209, MAPE=158.67490768432617%\n",
      "ADI: RMSE=0.04601228982210159, MAE=0.03609092906117439, MAPE=6.7498356103897095%\n",
      "ADM: RMSE=0.041600197553634644, MAE=0.03077520802617073, MAPE=7.313662767410278%\n",
      "ADP: RMSE=0.03851333260536194, MAE=0.029384944587945938, MAPE=4.563738778233528%\n",
      "ADSK: RMSE=0.04117082059383392, MAE=0.02949894778430462, MAPE=98.75063300132751%\n",
      "AEE: RMSE=0.04162093251943588, MAE=0.03264976665377617, MAPE=4.7281064093112946%\n",
      "AEP: RMSE=0.040953319519758224, MAE=0.030666006729006767, MAPE=5.461300164461136%\n",
      "AES: RMSE=0.048764221370220184, MAE=0.03729499503970146, MAPE=17.650610208511353%\n",
      "AFL: RMSE=0.034798379987478256, MAE=0.02593223936855793, MAPE=1328.1683921813965%\n",
      "AIG: RMSE=0.04880387336015701, MAE=0.03800499439239502, MAPE=8.134373277425766%\n",
      "AIZ: RMSE=0.03836044669151306, MAE=0.02749152109026909, MAPE=8.253342658281326%\n",
      "AJG: RMSE=0.03224467113614082, MAE=0.02432161383330822, MAPE=7.474026829004288%\n",
      "AKAM: RMSE=0.04549621045589447, MAE=0.03349824994802475, MAPE=19.1270112991333%\n",
      "ALB: RMSE=0.05816134065389633, MAE=0.044240184128284454, MAPE=35.136279463768005%\n",
      "ALGN: RMSE=0.04047586768865585, MAE=0.027333663776516914, MAPE=427.1543502807617%\n",
      "ALL: RMSE=0.04322657361626625, MAE=0.032827891409397125, MAPE=6.485185772180557%\n",
      "AMAT: RMSE=0.048794444650411606, MAE=0.038000594824552536, MAPE=28.438034653663635%\n",
      "AMCR: RMSE=0.048770926892757416, MAE=0.035401321947574615, MAPE=20.90602219104767%\n",
      "AMD: RMSE=0.051889047026634216, MAE=0.038384053856134415, MAPE=47.656989097595215%\n",
      "AME: RMSE=0.037899866700172424, MAE=0.029623107984662056, MAPE=5.959303677082062%\n",
      "AMGN: RMSE=0.03597104921936989, MAE=0.025841783732175827, MAPE=4.870787635445595%\n",
      "AMP: RMSE=0.03964202105998993, MAE=0.031080586835741997, MAPE=8.453666418790817%\n",
      "AMT: RMSE=0.039226166903972626, MAE=0.030212460085749626, MAPE=13.867929577827454%\n",
      "AMZN: RMSE=0.04264099895954132, MAE=0.031016500666737556, MAPE=34.933093190193176%\n",
      "ANSS: RMSE=0.0449163019657135, MAE=0.03321361169219017, MAPE=65.8997893333435%\n",
      "AON: RMSE=0.044204890727996826, MAE=0.03570528328418732, MAPE=4.974505305290222%\n",
      "AOS: RMSE=0.042771246284246445, MAE=0.03254081681370735, MAPE=19.677406549453735%\n",
      "APA: RMSE=0.029304364696145058, MAE=0.02186225727200508, MAPE=54.130375385284424%\n",
      "APD: RMSE=0.04249677062034607, MAE=0.03254701569676399, MAPE=5.864347890019417%\n",
      "APH: RMSE=0.031014731153845787, MAE=0.023769529536366463, MAPE=5.664714425802231%\n",
      "APTV: RMSE=0.04498446360230446, MAE=0.03350439295172691, MAPE=233.1148624420166%\n",
      "ARE: RMSE=0.039952848106622696, MAE=0.030742980539798737, MAPE=50.59548020362854%\n",
      "ATO: RMSE=0.0444030798971653, MAE=0.03546709567308426, MAPE=5.158103257417679%\n",
      "AVB: RMSE=0.046936046332120895, MAE=0.0353190116584301, MAPE=61.99612617492676%\n",
      "AVGO: RMSE=0.02800561487674713, MAE=0.019478321075439453, MAPE=40.694451332092285%\n",
      "AVY: RMSE=0.03828582912683487, MAE=0.028939928859472275, MAPE=5.174459144473076%\n",
      "AWK: RMSE=0.033861830830574036, MAE=0.026330983266234398, MAPE=5.640151351690292%\n",
      "AXON: RMSE=0.04079066589474678, MAE=0.029674777761101723, MAPE=45.88048756122589%\n",
      "AXP: RMSE=0.05042579025030136, MAE=0.03789817541837692, MAPE=7.692947238683701%\n",
      "AZO: RMSE=0.04337043687701225, MAE=0.033814992755651474, MAPE=5.90268112719059%\n",
      "BA: RMSE=0.024035068228840828, MAE=0.017819544300436974, MAPE=9.487942606210709%\n",
      "BAC: RMSE=0.039752233773469925, MAE=0.029186952859163284, MAPE=66.92533493041992%\n",
      "BALL: RMSE=0.04570912942290306, MAE=0.03228000923991203, MAPE=48.08945059776306%\n",
      "BAX: RMSE=0.03864974156022072, MAE=0.027019208297133446, MAPE=15.031734108924866%\n",
      "BBWI: RMSE=0.04317609220743179, MAE=0.032107286155223846, MAPE=47.06090986728668%\n",
      "BBY: RMSE=0.036693230271339417, MAE=0.025933129712939262, MAPE=96.58988118171692%\n",
      "BDX: RMSE=0.04083329439163208, MAE=0.03152510151267052, MAPE=4.672098532319069%\n",
      "BEN: RMSE=0.04448994621634483, MAE=0.032506588846445084, MAPE=60.80271601676941%\n",
      "BG: RMSE=0.042462509125471115, MAE=0.031367573887109756, MAPE=7.55765363574028%\n",
      "BIIB: RMSE=0.0337926484644413, MAE=0.021423351019620895, MAPE=17.918741703033447%\n",
      "BIO: RMSE=0.03767663240432739, MAE=0.02704116888344288, MAPE=68.89294981956482%\n",
      "BK: RMSE=0.0468464195728302, MAE=0.033883363008499146, MAPE=55.86826205253601%\n",
      "BKNG: RMSE=0.039673637598752975, MAE=0.030048588290810585, MAPE=125.80094337463379%\n",
      "BKR: RMSE=0.04504518583416939, MAE=0.03437234088778496, MAPE=37.68170475959778%\n",
      "BLDR: RMSE=0.03433815762400627, MAE=0.024875862523913383, MAPE=19.251130521297455%\n",
      "BLK: RMSE=0.0387859046459198, MAE=0.0289986003190279, MAPE=12.767007946968079%\n",
      "BMY: RMSE=0.03673335537314415, MAE=0.027742715552449226, MAPE=32.72967338562012%\n",
      "BR: RMSE=0.03428742289543152, MAE=0.025571729987859726, MAPE=5.308559536933899%\n",
      "BRO: RMSE=0.043658267706632614, MAE=0.033725231885910034, MAPE=5.514880269765854%\n",
      "BSX: RMSE=0.03994784131646156, MAE=0.030440635979175568, MAPE=5.366954952478409%\n",
      "BWA: RMSE=0.0503709577023983, MAE=0.037320926785469055, MAPE=61.341798305511475%\n",
      "BX: RMSE=0.051038675010204315, MAE=0.038086794316768646, MAPE=17.575182020664215%\n",
      "BXP: RMSE=0.04658268019556999, MAE=0.03600359708070755, MAPE=24.534118175506592%\n",
      "C: RMSE=0.041212279349565506, MAE=0.031212707981467247, MAPE=38.79241347312927%\n",
      "CAG: RMSE=0.04385693743824959, MAE=0.03253873437643051, MAPE=23.363327980041504%\n",
      "CAH: RMSE=0.030520735308527946, MAE=0.02213217318058014, MAPE=17.683003842830658%\n",
      "CAT: RMSE=0.03989516198635101, MAE=0.029676364734768867, MAPE=17.855867743492126%\n",
      "CB: RMSE=0.04376048222184181, MAE=0.03417329117655754, MAPE=5.387923866510391%\n",
      "CBOE: RMSE=0.030103078112006187, MAE=0.022179771214723587, MAPE=16.73728972673416%\n",
      "CBRE: RMSE=0.04306056722998619, MAE=0.03258426859974861, MAPE=12.36504539847374%\n",
      "CCI: RMSE=0.03839744254946709, MAE=0.02848914824426174, MAPE=24.109603464603424%\n",
      "CCL: RMSE=0.018456825986504555, MAE=0.013717003166675568, MAPE=2.02868040651083%\n",
      "CDNS: RMSE=0.03492608293890953, MAE=0.026633817702531815, MAPE=31.984838843345642%\n",
      "CE: RMSE=0.0489814393222332, MAE=0.03702268376946449, MAPE=57.337486743927%\n",
      "CF: RMSE=0.05140020325779915, MAE=0.03757283091545105, MAPE=63.670969009399414%\n",
      "CHD: RMSE=0.04524817690253258, MAE=0.034520477056503296, MAPE=5.214217305183411%\n",
      "CHRW: RMSE=0.05124017968773842, MAE=0.03729473799467087, MAPE=9.942090511322021%\n",
      "CHTR: RMSE=0.028681505471467972, MAE=0.020169435068964958, MAPE=56.839919090270996%\n",
      "CI: RMSE=0.03945388272404671, MAE=0.029014041647315025, MAPE=12.777738273143768%\n",
      "CINF: RMSE=0.04218125715851784, MAE=0.029726644977927208, MAPE=6.978143751621246%\n",
      "CL: RMSE=0.050630804151296616, MAE=0.039247479289770126, MAPE=5.933227017521858%\n",
      "CLX: RMSE=0.03443954139947891, MAE=0.024008048698306084, MAPE=80.91180920600891%\n",
      "CMA: RMSE=0.05505881458520889, MAE=0.039993513375520706, MAPE=20.914794504642487%\n",
      "CMCSA: RMSE=0.03558401018381119, MAE=0.02559414505958557, MAPE=42.722585797309875%\n",
      "CME: RMSE=0.03648480772972107, MAE=0.027386942878365517, MAPE=4.632199928164482%\n",
      "CMG: RMSE=0.03827172890305519, MAE=0.028036879375576973, MAPE=28.001350164413452%\n",
      "CMI: RMSE=0.042151425033807755, MAE=0.032664597034454346, MAPE=6.754066050052643%\n",
      "CMS: RMSE=0.03795810043811798, MAE=0.029352370649576187, MAPE=4.807181656360626%\n",
      "CNC: RMSE=0.0414220429956913, MAE=0.030603867024183273, MAPE=6.187209859490395%\n",
      "CNP: RMSE=0.04397524148225784, MAE=0.033192213624715805, MAPE=5.337932705879211%\n",
      "COF: RMSE=0.046621426939964294, MAE=0.03548043966293335, MAPE=101.01101398468018%\n",
      "COO: RMSE=0.040184877812862396, MAE=0.030666979029774666, MAPE=34.02709364891052%\n",
      "COP: RMSE=0.04897023364901543, MAE=0.03608497232198715, MAPE=14.731058478355408%\n",
      "COR: RMSE=0.03544687107205391, MAE=0.025815745815634727, MAPE=16.194240748882294%\n",
      "COST: RMSE=0.031147461384534836, MAE=0.022099437192082405, MAPE=5.567233636975288%\n",
      "CPB: RMSE=0.05113142728805542, MAE=0.03757745400071144, MAPE=73.08161854743958%\n",
      "CPRT: RMSE=0.03052295744419098, MAE=0.023326002061367035, MAPE=50.37010312080383%\n",
      "CPT: RMSE=0.040396057069301605, MAE=0.029175860807299614, MAPE=75.54598450660706%\n",
      "CRL: RMSE=0.03354538604617119, MAE=0.024773377925157547, MAPE=99.10749793052673%\n",
      "CRM: RMSE=0.04100727662444115, MAE=0.030416015535593033, MAPE=44.27904486656189%\n",
      "CSCO: RMSE=0.038782645016908646, MAE=0.028393743559718132, MAPE=7.744982838630676%\n",
      "CSGP: RMSE=0.04001224413514137, MAE=0.029597479850053787, MAPE=33.643269538879395%\n",
      "CSX: RMSE=0.041204970329999924, MAE=0.03205396607518196, MAPE=4.752743244171143%\n",
      "CTAS: RMSE=0.028255265206098557, MAE=0.020706994459033012, MAPE=5.610582232475281%\n",
      "CTRA: RMSE=0.06627713888883591, MAE=0.04785505309700966, MAPE=26.507750153541565%\n",
      "CTSH: RMSE=0.04580797627568245, MAE=0.03277880325913429, MAPE=42.8238570690155%\n",
      "CVS: RMSE=0.04692472517490387, MAE=0.03468675538897514, MAPE=36.695924401283264%\n",
      "CVX: RMSE=0.04676138237118721, MAE=0.035062965005636215, MAPE=9.206681698560715%\n",
      "D: RMSE=0.04329356923699379, MAE=0.03200402483344078, MAPE=22.863616049289703%\n",
      "DAL: RMSE=0.03995102271437645, MAE=0.02992294356226921, MAPE=248.9941120147705%\n",
      "DD: RMSE=0.03761729970574379, MAE=0.02766033262014389, MAPE=23.433537781238556%\n",
      "DE: RMSE=0.04600333422422409, MAE=0.03521576523780823, MAPE=5.90473972260952%\n",
      "DFS: RMSE=0.05143667757511139, MAE=0.03833604231476784, MAPE=8.866535127162933%\n",
      "DG: RMSE=0.04773594066500664, MAE=0.034380584955215454, MAPE=8.677279204130173%\n",
      "DGX: RMSE=0.035169702023267746, MAE=0.02680743671953678, MAPE=5.475874990224838%\n",
      "DHI: RMSE=0.03210229426622391, MAE=0.02438202127814293, MAPE=52.61637568473816%\n",
      "DHR: RMSE=0.03933856263756752, MAE=0.029351819306612015, MAPE=5.626855418086052%\n",
      "DIS: RMSE=0.030790045857429504, MAE=0.022414885461330414, MAPE=12.91002631187439%\n",
      "DLR: RMSE=0.0393318310379982, MAE=0.030261889100074768, MAPE=71.52543663978577%\n",
      "DLTR: RMSE=0.05801990628242493, MAE=0.039601318538188934, MAPE=18.946582078933716%\n",
      "DOV: RMSE=0.0378095805644989, MAE=0.028950689360499382, MAPE=6.767161190509796%\n",
      "DPZ: RMSE=0.034615881741046906, MAE=0.025114117190241814, MAPE=39.21746909618378%\n",
      "DRI: RMSE=0.04155333712697029, MAE=0.031798820942640305, MAPE=7.15896412730217%\n",
      "DTE: RMSE=0.03704231232404709, MAE=0.02763928845524788, MAPE=5.00551164150238%\n",
      "DUK: RMSE=0.04249153286218643, MAE=0.033366210758686066, MAPE=5.69036565721035%\n",
      "DVA: RMSE=0.05975503847002983, MAE=0.039989545941352844, MAPE=55.7140052318573%\n",
      "DVN: RMSE=0.054323166608810425, MAE=0.03856172785162926, MAPE=26.33044719696045%\n",
      "DXCM: RMSE=0.043644391000270844, MAE=0.03315064311027527, MAPE=52.091461420059204%\n",
      "EA: RMSE=0.04168308526277542, MAE=0.03239114582538605, MAPE=4.582491144537926%\n",
      "EBAY: RMSE=0.03839246556162834, MAE=0.028035258874297142, MAPE=10537.183380126953%\n",
      "ECL: RMSE=0.04346529021859169, MAE=0.03180908411741257, MAPE=277.2203207015991%\n",
      "ED: RMSE=0.04601946473121643, MAE=0.03559201583266258, MAPE=7.3093704879283905%\n",
      "EFX: RMSE=0.04238976165652275, MAE=0.03191615268588066, MAPE=34.0624213218689%\n",
      "EG: RMSE=0.0391269326210022, MAE=0.029848989099264145, MAPE=40.38574397563934%\n",
      "EIX: RMSE=0.055531807243824005, MAE=0.042114950716495514, MAPE=12.476496398448944%\n",
      "EL: RMSE=0.04426548257470131, MAE=0.03163037449121475, MAPE=20.0869619846344%\n",
      "ELV: RMSE=0.04023495316505432, MAE=0.031048359349370003, MAPE=4.453791677951813%\n",
      "EMN: RMSE=0.046146705746650696, MAE=0.03386112302541733, MAPE=61.33422255516052%\n",
      "EMR: RMSE=0.04907548800110817, MAE=0.0385843925178051, MAPE=6.811443716287613%\n",
      "ENPH: RMSE=0.05947151035070419, MAE=0.04319579899311066, MAPE=130.15481233596802%\n",
      "EOG: RMSE=0.05546353757381439, MAE=0.04108083248138428, MAPE=17.70104169845581%\n",
      "EPAM: RMSE=0.04861321300268173, MAE=0.030252577736973763, MAPE=41.21827185153961%\n",
      "EQIX: RMSE=0.04363008961081505, MAE=0.03433935344219208, MAPE=6.965987384319305%\n",
      "EQR: RMSE=0.044763728976249695, MAE=0.03367560729384422, MAPE=96.03182077407837%\n",
      "EQT: RMSE=0.04663803428411484, MAE=0.03445487841963768, MAPE=35.923025012016296%\n",
      "ES: RMSE=0.04257690906524658, MAE=0.03289339691400528, MAPE=13.66501897573471%\n",
      "ESS: RMSE=0.044606294482946396, MAE=0.033564284443855286, MAPE=64.30208683013916%\n",
      "ETN: RMSE=0.035325199365615845, MAE=0.026181142777204514, MAPE=34.85296368598938%\n",
      "ETR: RMSE=0.04176641255617142, MAE=0.031922414898872375, MAPE=5.499973148107529%\n",
      "EVRG: RMSE=0.04084649309515953, MAE=0.031586360186338425, MAPE=5.533713102340698%\n",
      "EW: RMSE=0.038750238716602325, MAE=0.028132403269410133, MAPE=40.23042321205139%\n",
      "EXC: RMSE=0.04009082168340683, MAE=0.029777955263853073, MAPE=5.72357252240181%\n",
      "EXPD: RMSE=0.04521586000919342, MAE=0.03472563624382019, MAPE=7.2745271027088165%\n",
      "EXPE: RMSE=0.050257276743650436, MAE=0.034172423183918, MAPE=58.90069603919983%\n",
      "EXR: RMSE=0.04093869775533676, MAE=0.028943607583642006, MAPE=38.88358771800995%\n",
      "F: RMSE=0.04716948792338371, MAE=0.03232317045331001, MAPE=33.68955850601196%\n",
      "FANG: RMSE=0.048871785402297974, MAE=0.03798772767186165, MAPE=13.206644356250763%\n",
      "FAST: RMSE=0.037532467395067215, MAE=0.028200417757034302, MAPE=6.351473927497864%\n",
      "FCX: RMSE=0.051493119448423386, MAE=0.04061855748295784, MAPE=23.969192802906036%\n",
      "FDS: RMSE=0.04522360861301422, MAE=0.03638208284974098, MAPE=5.272747576236725%\n",
      "FDX: RMSE=0.04706956073641777, MAE=0.030979618430137634, MAPE=25.103628635406494%\n",
      "FE: RMSE=0.04602082073688507, MAE=0.03446238115429878, MAPE=12.516382336616516%\n",
      "FFIV: RMSE=0.041130539029836655, MAE=0.028938528150320053, MAPE=92.94161796569824%\n",
      "FI: RMSE=0.03734220564365387, MAE=0.028169600293040276, MAPE=5.875929817557335%\n",
      "FICO: RMSE=0.026843469589948654, MAE=0.018303262069821358, MAPE=22.64760136604309%\n",
      "FIS: RMSE=0.03729059174656868, MAE=0.024741146713495255, MAPE=36.37607991695404%\n",
      "FITB: RMSE=0.04716582968831062, MAE=0.034502461552619934, MAPE=94.1534698009491%\n",
      "FLT: RMSE=0.033709753304719925, MAE=0.025182358920574188, MAPE=58.03920030593872%\n",
      "FMC: RMSE=0.04586975648999214, MAE=0.034249454736709595, MAPE=7.913418859243393%\n",
      "FRT: RMSE=0.05006302148103714, MAE=0.03865932673215866, MAPE=157.2096824645996%\n",
      "FSLR: RMSE=0.05194941535592079, MAE=0.03520788252353668, MAPE=33.05952548980713%\n",
      "FTNT: RMSE=0.05613734573125839, MAE=0.039653826504945755, MAPE=9.754034876823425%\n",
      "GD: RMSE=0.0406963974237442, MAE=0.030971677973866463, MAPE=4.521869868040085%\n",
      "GE: RMSE=0.020195597782731056, MAE=0.015367846004664898, MAPE=19.39389705657959%\n",
      "GEN: RMSE=0.04561541602015495, MAE=0.03198760375380516, MAPE=22.38040566444397%\n",
      "GILD: RMSE=0.034058671444654465, MAE=0.0248576607555151, MAPE=79.86623644828796%\n",
      "GIS: RMSE=0.03706590086221695, MAE=0.027103057131171227, MAPE=22.154957056045532%\n",
      "GL: RMSE=0.0448913611471653, MAE=0.03529350459575653, MAPE=6.385142356157303%\n",
      "GLW: RMSE=0.03667156398296356, MAE=0.027237335219979286, MAPE=7.6241135597229%\n",
      "GM: RMSE=0.0490964874625206, MAE=0.036473363637924194, MAPE=58.10313820838928%\n",
      "GNRC: RMSE=0.03823162242770195, MAE=0.025531012564897537, MAPE=162.04243898391724%\n",
      "GOOG: RMSE=0.04592185467481613, MAE=0.035047903656959534, MAPE=10.092132538557053%\n",
      "GOOGL: RMSE=0.045939356088638306, MAE=0.035139188170433044, MAPE=10.365844517946243%\n",
      "GPC: RMSE=0.039377667009830475, MAE=0.029427753761410713, MAPE=13.776446878910065%\n",
      "GPN: RMSE=0.030268175527453423, MAE=0.022264646366238594, MAPE=72.37364649772644%\n",
      "GRMN: RMSE=0.026531945914030075, MAE=0.0188535675406456, MAPE=60.158324241638184%\n",
      "GS: RMSE=0.04534183070063591, MAE=0.03551531583070755, MAPE=8.039334416389465%\n",
      "GWW: RMSE=0.032972946763038635, MAE=0.02397444099187851, MAPE=57.373976707458496%\n",
      "HAL: RMSE=0.03451312705874443, MAE=0.025901542976498604, MAPE=40.43850898742676%\n",
      "HAS: RMSE=0.0367630273103714, MAE=0.027380861341953278, MAPE=20.922915637493134%\n",
      "HBAN: RMSE=0.05200420320034027, MAE=0.03929590806365013, MAPE=43.70880722999573%\n",
      "HCA: RMSE=0.04244684427976608, MAE=0.030785316601395607, MAPE=28.850725293159485%\n",
      "HD: RMSE=0.03395625576376915, MAE=0.0253437589854002, MAPE=5.5815864354372025%\n",
      "HES: RMSE=0.05118420347571373, MAE=0.038670580834150314, MAPE=64.3985390663147%\n",
      "HIG: RMSE=0.04208565875887871, MAE=0.03344094753265381, MAPE=5.503247678279877%\n",
      "HII: RMSE=0.03488403186202049, MAE=0.026299186050891876, MAPE=4.836846515536308%\n",
      "HOLX: RMSE=0.04014422744512558, MAE=0.030719004571437836, MAPE=5.172910913825035%\n",
      "HON: RMSE=0.03819888085126877, MAE=0.0303413774818182, MAPE=4.815739020705223%\n",
      "HPQ: RMSE=0.04706866666674614, MAE=0.03317337483167648, MAPE=6.869516521692276%\n",
      "HRL: RMSE=0.03586408495903015, MAE=0.02634277567267418, MAPE=15.630848705768585%\n",
      "HSIC: RMSE=0.0469241589307785, MAE=0.03554832190275192, MAPE=17.110376060009003%\n",
      "HST: RMSE=0.07473558932542801, MAE=0.055262938141822815, MAPE=17.910195887088776%\n",
      "HSY: RMSE=0.03052235022187233, MAE=0.02261071838438511, MAPE=6.507936120033264%\n",
      "HUBB: RMSE=0.03829631581902504, MAE=0.027998175472021103, MAPE=74.20698404312134%\n",
      "HUM: RMSE=0.04418136924505234, MAE=0.032801948487758636, MAPE=5.302629247307777%\n",
      "IBM: RMSE=0.042289167642593384, MAE=0.030581338331103325, MAPE=62.80651092529297%\n",
      "ICE: RMSE=0.03763655200600624, MAE=0.02874263934791088, MAPE=5.621016770601273%\n",
      "IDXX: RMSE=0.036937467753887177, MAE=0.027821902185678482, MAPE=44.72513198852539%\n",
      "IEX: RMSE=0.042019445449113846, MAE=0.03299117833375931, MAPE=4.784714803099632%\n",
      "IFF: RMSE=0.056401126086711884, MAE=0.03918464481830597, MAPE=24.660445749759674%\n",
      "ILMN: RMSE=0.03489883989095688, MAE=0.02373124472796917, MAPE=9.430576115846634%\n",
      "INCY: RMSE=0.017756637185811996, MAE=0.013261167332530022, MAPE=14.414407312870026%\n",
      "INTC: RMSE=0.03883048892021179, MAE=0.028803348541259766, MAPE=52.191030979156494%\n",
      "INTU: RMSE=0.04299557954072952, MAE=0.03179076313972473, MAPE=36.114731431007385%\n",
      "IP: RMSE=0.043487709015607834, MAE=0.03151118755340576, MAPE=49.06336963176727%\n",
      "IPG: RMSE=0.04575992375612259, MAE=0.03441290184855461, MAPE=10.447677969932556%\n",
      "IRM: RMSE=0.03565504401922226, MAE=0.026672814041376114, MAPE=30.26340901851654%\n",
      "ISRG: RMSE=0.0461641289293766, MAE=0.03457516059279442, MAPE=74.62347745895386%\n",
      "IT: RMSE=0.03325219079852104, MAE=0.024522630497813225, MAPE=34.929874539375305%\n",
      "ITW: RMSE=0.03846421092748642, MAE=0.03032795339822769, MAPE=5.49166239798069%\n",
      "IVZ: RMSE=0.03852835297584534, MAE=0.029006820172071457, MAPE=72.88204431533813%\n",
      "J: RMSE=0.04449988901615143, MAE=0.03375907242298126, MAPE=5.307385697960854%\n",
      "JBHT: RMSE=0.04925660043954849, MAE=0.037140361964702606, MAPE=6.945835053920746%\n",
      "JBL: RMSE=0.03711077570915222, MAE=0.022882703691720963, MAPE=19.761602580547333%\n",
      "JCI: RMSE=0.045639269053936005, MAE=0.03419365733861923, MAPE=42.65471696853638%\n",
      "JKHY: RMSE=0.03722240775823593, MAE=0.027340998873114586, MAPE=5.330175161361694%\n",
      "JNJ: RMSE=0.04190467670559883, MAE=0.03293881192803383, MAPE=4.295824468135834%\n",
      "JNPR: RMSE=0.048669442534446716, MAE=0.036273106932640076, MAPE=10.173384100198746%\n",
      "JPM: RMSE=0.03718719258904457, MAE=0.028666919097304344, MAPE=15.39156585931778%\n",
      "K: RMSE=0.05280889570713043, MAE=0.03928108885884285, MAPE=79.95544672012329%\n",
      "KDP: RMSE=0.04198415204882622, MAE=0.032954003661870956, MAPE=4.371140152215958%\n",
      "KEY: RMSE=0.05031484365463257, MAE=0.0358867309987545, MAPE=38.885924220085144%\n",
      "KIM: RMSE=0.04847975820302963, MAE=0.03696249797940254, MAPE=9.003612399101257%\n",
      "KLAC: RMSE=0.04054202139377594, MAE=0.03131360933184624, MAPE=81.85920119285583%\n",
      "KMB: RMSE=0.04032576084136963, MAE=0.030571063980460167, MAPE=5.637400224804878%\n",
      "KMI: RMSE=0.0240899920463562, MAE=0.01837080530822277, MAPE=18.998341262340546%\n",
      "KMX: RMSE=0.04732714965939522, MAE=0.03414452075958252, MAPE=162694.93408203125%\n",
      "KO: RMSE=0.050917427986860275, MAE=0.040410950779914856, MAPE=5.377211049199104%\n",
      "KR: RMSE=0.03661346063017845, MAE=0.025950836017727852, MAPE=6.333152949810028%\n",
      "L: RMSE=0.0409042127430439, MAE=0.0315299928188324, MAPE=11.435332894325256%\n",
      "LDOS: RMSE=0.03481566160917282, MAE=0.025367246940732002, MAPE=4.696105793118477%\n",
      "LEN: RMSE=0.03625068813562393, MAE=0.02755747362971306, MAPE=26.371371746063232%\n",
      "LH: RMSE=0.0394405871629715, MAE=0.029477842152118683, MAPE=8.731966465711594%\n",
      "LHX: RMSE=0.03509625047445297, MAE=0.02547033503651619, MAPE=4.886728525161743%\n",
      "LIN: RMSE=0.03450910747051239, MAE=0.026259886100888252, MAPE=8.98304358124733%\n",
      "LKQ: RMSE=0.05195849761366844, MAE=0.04121662303805351, MAPE=6.2789589166641235%\n",
      "LLY: RMSE=0.030655285343527794, MAE=0.01983053609728813, MAPE=51.80596709251404%\n",
      "LMT: RMSE=0.04274635761976242, MAE=0.03297429531812668, MAPE=5.210477113723755%\n",
      "LNT: RMSE=0.04089028388261795, MAE=0.031186413019895554, MAPE=4.882507771253586%\n",
      "LOW: RMSE=0.039961837232112885, MAE=0.030789131298661232, MAPE=5.39465919137001%\n",
      "LRCX: RMSE=0.04466909542679787, MAE=0.0341828390955925, MAPE=63.79701495170593%\n",
      "LULU: RMSE=0.043191973119974136, MAE=0.03149966150522232, MAPE=46.66570425033569%\n",
      "LUV: RMSE=0.03254156932234764, MAE=0.024521466344594955, MAPE=54.959142208099365%\n",
      "LVS: RMSE=0.05984720587730408, MAE=0.045443836599588394, MAPE=16.663973033428192%\n",
      "LYB: RMSE=0.048947349190711975, MAE=0.037826601415872574, MAPE=8.195246756076813%\n",
      "LYV: RMSE=0.04501405730843544, MAE=0.030703173950314522, MAPE=41.78881645202637%\n",
      "MA: RMSE=0.0439181849360466, MAE=0.034771040081977844, MAPE=5.561055615544319%\n",
      "MAA: RMSE=0.03678445518016815, MAE=0.02714448608458042, MAPE=23.680351674556732%\n",
      "MAR: RMSE=0.04118287190794945, MAE=0.031340181827545166, MAPE=10.034840553998947%\n",
      "MAS: RMSE=0.04178155958652496, MAE=0.03238455206155777, MAPE=9.337128698825836%\n",
      "MCD: RMSE=0.03984319791197777, MAE=0.03090938739478588, MAPE=4.516324028372765%\n",
      "MCHP: RMSE=0.05123766511678696, MAE=0.04050282761454582, MAPE=10.808765888214111%\n",
      "MCK: RMSE=0.03562798723578453, MAE=0.02610558457672596, MAPE=9.336476027965546%\n",
      "MCO: RMSE=0.03827381134033203, MAE=0.029372885823249817, MAPE=6.26194104552269%\n",
      "MDLZ: RMSE=0.03436204418540001, MAE=0.02593143656849861, MAPE=5.1527149975299835%\n",
      "MDT: RMSE=0.03130509704351425, MAE=0.023165520280599594, MAPE=77.61582136154175%\n",
      "MET: RMSE=0.043214328587055206, MAE=0.033955659717321396, MAPE=8.408768475055695%\n",
      "META: RMSE=0.04393181949853897, MAE=0.029893681406974792, MAPE=45.243507623672485%\n",
      "MGM: RMSE=0.05335570499300957, MAE=0.04103431850671768, MAPE=31.269672513008118%\n",
      "MHK: RMSE=0.03037482500076294, MAE=0.021857133135199547, MAPE=22.362467646598816%\n",
      "MKC: RMSE=0.04053907468914986, MAE=0.029339192435145378, MAPE=20.29496431350708%\n",
      "MKTX: RMSE=0.027035919949412346, MAE=0.01903647929430008, MAPE=18.26401650905609%\n",
      "MLM: RMSE=0.03875144571065903, MAE=0.030147889629006386, MAPE=10.865845531225204%\n",
      "MMC: RMSE=0.038124021142721176, MAE=0.029762206599116325, MAPE=4.794447496533394%\n",
      "MMM: RMSE=0.029876895248889923, MAE=0.021592220291495323, MAPE=98.16574454307556%\n",
      "MNST: RMSE=0.0387580469250679, MAE=0.029895499348640442, MAPE=5.8025021106004715%\n",
      "MO: RMSE=0.04122675210237503, MAE=0.028374681249260902, MAPE=5.81611730158329%\n",
      "MOH: RMSE=0.040310151875019073, MAE=0.03150701895356178, MAPE=5.158941075205803%\n",
      "MOS: RMSE=0.04550919309258461, MAE=0.03200400620698929, MAPE=36.522892117500305%\n",
      "MPC: RMSE=0.03878284618258476, MAE=0.028676822781562805, MAPE=29.37576472759247%\n",
      "MPWR: RMSE=0.05500029772520065, MAE=0.042991477996110916, MAPE=44.70973014831543%\n",
      "MRK: RMSE=0.03878302499651909, MAE=0.029528936371207237, MAPE=50.27834177017212%\n",
      "MRO: RMSE=0.04710264876484871, MAE=0.03497595340013504, MAPE=16.82407408952713%\n",
      "MS: RMSE=0.043056927621364594, MAE=0.03325919806957245, MAPE=5.8875422924757%\n",
      "MSCI: RMSE=0.039878059178590775, MAE=0.029100965708494186, MAPE=7.2012171149253845%\n",
      "MSFT: RMSE=0.03758465126156807, MAE=0.02882474660873413, MAPE=7.224873453378677%\n",
      "MSI: RMSE=0.03258941322565079, MAE=0.02513059787452221, MAPE=7.258071005344391%\n",
      "MTB: RMSE=0.06963853538036346, MAE=0.05212429538369179, MAPE=32.9733669757843%\n",
      "MTCH: RMSE=0.03152386471629143, MAE=0.021062660962343216, MAPE=180.45566082000732%\n",
      "MTD: RMSE=0.042850278317928314, MAE=0.033860404044389725, MAPE=8.331821113824844%\n",
      "MU: RMSE=0.044916845858097076, MAE=0.03330330550670624, MAPE=32.53495693206787%\n",
      "NDAQ: RMSE=0.03872183710336685, MAE=0.02892155945301056, MAPE=4.889608547091484%\n",
      "NDSN: RMSE=0.04433441534638405, MAE=0.0340660884976387, MAPE=5.242025852203369%\n",
      "NEE: RMSE=0.04601322114467621, MAE=0.03489570692181587, MAPE=18.550024926662445%\n",
      "NEM: RMSE=0.035295259207487106, MAE=0.02566925622522831, MAPE=41.28442108631134%\n",
      "NFLX: RMSE=0.04206576943397522, MAE=0.027304744347929955, MAPE=27.7667373418808%\n",
      "NI: RMSE=0.0413566529750824, MAE=0.031678274273872375, MAPE=4.824173450469971%\n",
      "NKE: RMSE=0.038990557193756104, MAE=0.02828161045908928, MAPE=46.15648686885834%\n",
      "NOC: RMSE=0.04194750636816025, MAE=0.03072015754878521, MAPE=5.144486203789711%\n",
      "NOW: RMSE=0.04889918491244316, MAE=0.03676699846982956, MAPE=245.3016996383667%\n",
      "NRG: RMSE=0.04199061542749405, MAE=0.030989868566393852, MAPE=11.397276818752289%\n",
      "NSC: RMSE=0.03859635815024376, MAE=0.029534192755818367, MAPE=5.839822813868523%\n",
      "NTAP: RMSE=0.044095322489738464, MAE=0.03217305243015289, MAPE=8.474010229110718%\n",
      "NTRS: RMSE=0.04789043962955475, MAE=0.034730371087789536, MAPE=42.07562506198883%\n",
      "NUE: RMSE=0.05459388345479965, MAE=0.04230787605047226, MAPE=25.703418254852295%\n",
      "NVDA: RMSE=0.04172363504767418, MAE=0.030706781893968582, MAPE=22.223620116710663%\n",
      "NVR: RMSE=0.03600480034947395, MAE=0.027461204677820206, MAPE=28.86279821395874%\n",
      "NXPI: RMSE=0.0481586828827858, MAE=0.037976257503032684, MAPE=9.180936962366104%\n",
      "O: RMSE=0.03901199623942375, MAE=0.030008098110556602, MAPE=7.243435829877853%\n",
      "ODFL: RMSE=0.0409272126853466, MAE=0.03138509765267372, MAPE=11.461713910102844%\n",
      "OKE: RMSE=0.04875868931412697, MAE=0.039105124771595, MAPE=6.5100520849227905%\n",
      "OMC: RMSE=0.05166009068489075, MAE=0.03776506707072258, MAPE=59.690290689468384%\n",
      "ON: RMSE=0.053079549223184586, MAE=0.041086047887802124, MAPE=39.08798694610596%\n",
      "ORCL: RMSE=0.0417645201086998, MAE=0.02795219048857689, MAPE=53.45935821533203%\n",
      "ORLY: RMSE=0.04031683877110481, MAE=0.03031790256500244, MAPE=6.91918358206749%\n",
      "OXY: RMSE=0.053025394678115845, MAE=0.03755267336964607, MAPE=9.59976390004158%\n",
      "PANW: RMSE=0.03660695254802704, MAE=0.02712121605873108, MAPE=101.95560455322266%\n",
      "PARA: RMSE=0.019902504980564117, MAE=0.01356588862836361, MAPE=2.158733643591404%\n",
      "PAYX: RMSE=0.03905213624238968, MAE=0.03042508102953434, MAPE=4.590770602226257%\n",
      "PCAR: RMSE=0.028287522494792938, MAE=0.021379319950938225, MAPE=74.84879493713379%\n",
      "PCG: RMSE=0.007407154887914658, MAE=0.005679128225892782, MAPE=0.830172561109066%\n",
      "PEAK: RMSE=0.05216379091143608, MAE=0.03991710767149925, MAPE=41.29721820354462%\n",
      "PEG: RMSE=0.036494798958301544, MAE=0.027901986613869667, MAPE=4.913703352212906%\n",
      "PEP: RMSE=0.039202138781547546, MAE=0.030425017699599266, MAPE=4.495273903012276%\n",
      "PFE: RMSE=0.041534412652254105, MAE=0.029741670936346054, MAPE=17.921075224876404%\n",
      "PFG: RMSE=0.04301810637116432, MAE=0.032961536198854446, MAPE=8.286745846271515%\n",
      "PG: RMSE=0.048944056034088135, MAE=0.03958888351917267, MAPE=5.160050466656685%\n",
      "PGR: RMSE=0.03658352792263031, MAE=0.025362340733408928, MAPE=11.796016246080399%\n",
      "PH: RMSE=0.03460211679339409, MAE=0.025838099420070648, MAPE=61.139434576034546%\n",
      "PHM: RMSE=0.029434585943818092, MAE=0.021462149918079376, MAPE=13.703617453575134%\n",
      "PKG: RMSE=0.038678620010614395, MAE=0.02830721251666546, MAPE=5.889704078435898%\n",
      "PLD: RMSE=0.03978642448782921, MAE=0.02906830981373787, MAPE=13.167312741279602%\n",
      "PM: RMSE=0.051445864140987396, MAE=0.03974524885416031, MAPE=7.043928653001785%\n",
      "PNC: RMSE=0.03894401341676712, MAE=0.029159773141145706, MAPE=30.941888689994812%\n",
      "PNR: RMSE=0.04180017113685608, MAE=0.03265559673309326, MAPE=48.81276786327362%\n",
      "PNW: RMSE=0.0406990610063076, MAE=0.03133246302604675, MAPE=47.131529450416565%\n",
      "PODD: RMSE=0.05731481313705444, MAE=0.042064402252435684, MAPE=16.93914830684662%\n",
      "POOL: RMSE=0.03890529274940491, MAE=0.029707469046115875, MAPE=36.84028387069702%\n",
      "PPG: RMSE=0.04405079409480095, MAE=0.034156277775764465, MAPE=48.82557988166809%\n",
      "PPL: RMSE=0.05145177245140076, MAE=0.03890838846564293, MAPE=19.891007244586945%\n",
      "PRU: RMSE=0.04629582539200783, MAE=0.03625559061765671, MAPE=8.108878880739212%\n",
      "PSA: RMSE=0.03782009333372116, MAE=0.027622690424323082, MAPE=11.622985452413559%\n",
      "PSX: RMSE=0.043918486684560776, MAE=0.03356324881315231, MAPE=33.75058174133301%\n",
      "PTC: RMSE=0.034244418144226074, MAE=0.025536786764860153, MAPE=24.473872780799866%\n",
      "PWR: RMSE=0.036265779286623, MAE=0.027549482882022858, MAPE=48.25272262096405%\n",
      "PXD: RMSE=0.052805427461862564, MAE=0.03967976197600365, MAPE=15.107674896717072%\n",
      "QCOM: RMSE=0.05424649640917778, MAE=0.03959893807768822, MAPE=67.83488988876343%\n",
      "RCL: RMSE=0.04296502098441124, MAE=0.03255891799926758, MAPE=43.60928237438202%\n",
      "REG: RMSE=0.051078084856271744, MAE=0.03902331367135048, MAPE=10.18153503537178%\n",
      "REGN: RMSE=0.04209379851818085, MAE=0.031356796622276306, MAPE=7.048127055168152%\n",
      "RF: RMSE=0.05681363493204117, MAE=0.044179767370224, MAPE=25.41455626487732%\n",
      "RHI: RMSE=0.04043373838067055, MAE=0.03032161109149456, MAPE=122.20689058303833%\n",
      "RJF: RMSE=0.04536519572138786, MAE=0.035578563809394836, MAPE=6.766325235366821%\n",
      "RL: RMSE=0.06107071787118912, MAE=0.0462697334587574, MAPE=57.982295751571655%\n",
      "RMD: RMSE=0.03867246210575104, MAE=0.027359770610928535, MAPE=278.04126739501953%\n",
      "ROK: RMSE=0.045156802982091904, MAE=0.034611064940690994, MAPE=18.14260333776474%\n",
      "ROL: RMSE=0.040233250707387924, MAE=0.029342196881771088, MAPE=5.346508696675301%\n",
      "ROP: RMSE=0.03396068885922432, MAE=0.026352616026997566, MAPE=5.456588044762611%\n",
      "ROST: RMSE=0.04485464468598366, MAE=0.03092532604932785, MAPE=32.86428153514862%\n",
      "RSG: RMSE=0.0310747642070055, MAE=0.023323075845837593, MAPE=4.147741198539734%\n",
      "RTX: RMSE=0.05385955795645714, MAE=0.041794292628765106, MAPE=35.478368401527405%\n",
      "RVTY: RMSE=0.04205969721078873, MAE=0.030786918476223946, MAPE=80.08966445922852%\n",
      "SBAC: RMSE=0.04068927839398384, MAE=0.03098854422569275, MAPE=42.846545577049255%\n",
      "SBUX: RMSE=0.038666874170303345, MAE=0.02918381802737713, MAPE=26.30961239337921%\n",
      "SCHW: RMSE=0.047044724225997925, MAE=0.034038376063108444, MAPE=84.45419669151306%\n",
      "SHW: RMSE=0.038095247000455856, MAE=0.028076795861124992, MAPE=10.15140488743782%\n",
      "SJM: RMSE=0.0459284707903862, MAE=0.03506721556186676, MAPE=29.42817509174347%\n",
      "SLB: RMSE=0.030225390568375587, MAE=0.022663716226816177, MAPE=48.36026728153229%\n",
      "SNA: RMSE=0.0371452271938324, MAE=0.028109587728977203, MAPE=7.982799410820007%\n",
      "SNPS: RMSE=0.032281167805194855, MAE=0.024609757587313652, MAPE=51.27583146095276%\n",
      "SO: RMSE=0.04191868007183075, MAE=0.03135577589273453, MAPE=5.3105659782886505%\n",
      "SPG: RMSE=0.04008319601416588, MAE=0.029970942065119743, MAPE=86.82838082313538%\n",
      "SPGI: RMSE=0.0370209701359272, MAE=0.027654696255922318, MAPE=5.236056819558144%\n",
      "SRE: RMSE=0.04167637228965759, MAE=0.032395441085100174, MAPE=5.477850139141083%\n",
      "STE: RMSE=0.04452221095561981, MAE=0.03510547801852226, MAPE=6.0991160571575165%\n",
      "STLD: RMSE=0.04163495451211929, MAE=0.030681947246193886, MAPE=38.04973661899567%\n",
      "STT: RMSE=0.06244468316435814, MAE=0.04662773385643959, MAPE=122.44716882705688%\n",
      "STX: RMSE=0.0426667146384716, MAE=0.03156473860144615, MAPE=39.764195680618286%\n",
      "STZ: RMSE=0.03975887969136238, MAE=0.03162368759512901, MAPE=4.411594569683075%\n",
      "SWK: RMSE=0.03862987831234932, MAE=0.02848312258720398, MAPE=18.02845299243927%\n",
      "SWKS: RMSE=0.03168025612831116, MAE=0.0243060439825058, MAPE=110.62449216842651%\n",
      "SYK: RMSE=0.044878631830215454, MAE=0.03533041477203369, MAPE=6.947441399097443%\n",
      "SYY: RMSE=0.04759824275970459, MAE=0.035083770751953125, MAPE=5.353043228387833%\n",
      "T: RMSE=0.05978807434439659, MAE=0.04279530048370361, MAPE=324.86283779144287%\n",
      "TAP: RMSE=0.028171727433800697, MAE=0.020972905680537224, MAPE=36.17114722728729%\n",
      "TDG: RMSE=0.030945491045713425, MAE=0.02281302958726883, MAPE=32.59532153606415%\n",
      "TDY: RMSE=0.03880392387509346, MAE=0.030753469094634056, MAPE=5.023972317576408%\n",
      "TECH: RMSE=0.03850097581744194, MAE=0.02919752709567547, MAPE=388.3690118789673%\n",
      "TEL: RMSE=0.03938201442360878, MAE=0.029765848070383072, MAPE=6.519965827465057%\n",
      "TER: RMSE=0.04560070112347603, MAE=0.031875040382146835, MAPE=65.96300005912781%\n",
      "TFC: RMSE=0.049362268298864365, MAE=0.03732599318027496, MAPE=34.70200300216675%\n",
      "TFX: RMSE=0.03007124364376068, MAE=0.022091053426265717, MAPE=80.46534657478333%\n",
      "TGT: RMSE=0.04434056207537651, MAE=0.029072877019643784, MAPE=71.1689293384552%\n",
      "TJX: RMSE=0.04066957160830498, MAE=0.031274594366550446, MAPE=58.488017320632935%\n",
      "TMO: RMSE=0.040492650121450424, MAE=0.03112245909869671, MAPE=5.059925466775894%\n",
      "TMUS: RMSE=0.03971870243549347, MAE=0.031057139858603477, MAPE=4.908609017729759%\n",
      "TPR: RMSE=0.05790863186120987, MAE=0.0421067513525486, MAPE=42.13023483753204%\n",
      "TRGP: RMSE=0.04140437766909599, MAE=0.03198036551475525, MAPE=11.86974048614502%\n",
      "TRMB: RMSE=0.04165719822049141, MAE=0.030877308920025826, MAPE=58.84994864463806%\n",
      "TROW: RMSE=0.03807989880442619, MAE=0.028069889172911644, MAPE=29.474344849586487%\n",
      "TRV: RMSE=0.04533984884619713, MAE=0.03514678403735161, MAPE=5.385488644242287%\n",
      "TSCO: RMSE=0.045499153435230255, MAE=0.03572792187333107, MAPE=5.835627391934395%\n",
      "TSLA: RMSE=0.05738319456577301, MAE=0.04094313085079193, MAPE=40.60513973236084%\n",
      "TSN: RMSE=0.03645727038383484, MAE=0.024566277861595154, MAPE=14.713592827320099%\n",
      "TT: RMSE=0.0321413055062294, MAE=0.023451676592230797, MAPE=28.400027751922607%\n",
      "TTWO: RMSE=0.033109091222286224, MAE=0.023132694885134697, MAPE=32.343608140945435%\n",
      "TXN: RMSE=0.04650534689426422, MAE=0.037305209785699844, MAPE=5.144442245364189%\n",
      "TXT: RMSE=0.05372794717550278, MAE=0.04168282821774483, MAPE=6.846333295106888%\n",
      "TYL: RMSE=0.040092501789331436, MAE=0.030225833877921104, MAPE=17.50529706478119%\n",
      "UAL: RMSE=0.0345238521695137, MAE=0.026238270103931427, MAPE=9.237681329250336%\n",
      "UDR: RMSE=0.04563325643539429, MAE=0.03318982198834419, MAPE=171.74891233444214%\n",
      "UHS: RMSE=0.053467489778995514, MAE=0.03915273770689964, MAPE=30.053529143333435%\n",
      "ULTA: RMSE=0.0453256256878376, MAE=0.033623237162828445, MAPE=8.141131699085236%\n",
      "UNH: RMSE=0.04669051617383957, MAE=0.03760921582579613, MAPE=4.735761508345604%\n",
      "UNP: RMSE=0.03514416888356209, MAE=0.02553151547908783, MAPE=5.1456209272146225%\n",
      "UPS: RMSE=0.04328067973256111, MAE=0.030994322150945663, MAPE=9.367367625236511%\n",
      "URI: RMSE=0.036963846534490585, MAE=0.02824731543660164, MAPE=94.77599859237671%\n",
      "USB: RMSE=0.058161232620477676, MAE=0.043837547302246094, MAPE=36.62891387939453%\n",
      "V: RMSE=0.03957579284906387, MAE=0.030699994415044785, MAPE=5.224602669477463%\n",
      "VFC: RMSE=0.026374351233243942, MAE=0.019006064161658287, MAPE=12.044507265090942%\n",
      "VLO: RMSE=0.05286634713411331, MAE=0.03985454887151718, MAPE=15.55003970861435%\n",
      "VMC: RMSE=0.042870067059993744, MAE=0.03377906605601311, MAPE=7.7387794852256775%\n",
      "VRSK: RMSE=0.04262417554855347, MAE=0.03323547914624214, MAPE=7.706167548894882%\n",
      "VRSN: RMSE=0.03818288818001747, MAE=0.027884630486369133, MAPE=6.18666373193264%\n",
      "VRTX: RMSE=0.03269485756754875, MAE=0.02287272736430168, MAPE=18.85431706905365%\n",
      "VTR: RMSE=0.03825986385345459, MAE=0.029402969405055046, MAPE=24.603894352912903%\n",
      "VTRS: RMSE=0.008034591563045979, MAE=0.005144953727722168, MAPE=0.5599720403552055%\n",
      "VZ: RMSE=0.04599754139780998, MAE=0.032673366367816925, MAPE=41.88992381095886%\n",
      "WAB: RMSE=0.0420292392373085, MAE=0.031518254429101944, MAPE=37.94276714324951%\n",
      "WAT: RMSE=0.03973367437720299, MAE=0.029893668368458748, MAPE=42.22993552684784%\n",
      "WBA: RMSE=0.025826046243309975, MAE=0.01874059997498989, MAPE=10.632693767547607%\n",
      "WBD: RMSE=0.018309252336621284, MAE=0.013197343796491623, MAPE=1.9773488864302635%\n",
      "WDC: RMSE=0.0378219410777092, MAE=0.02790018729865551, MAPE=11.713842302560806%\n",
      "WEC: RMSE=0.03984010964632034, MAE=0.03088616207242012, MAPE=4.766068235039711%\n",
      "WELL: RMSE=0.04703793302178383, MAE=0.036732155829668045, MAPE=13.766132295131683%\n",
      "WFC: RMSE=0.05382518842816353, MAE=0.0403231680393219, MAPE=48.21205735206604%\n",
      "WHR: RMSE=0.04400689899921417, MAE=0.03344887122511864, MAPE=59.44494009017944%\n",
      "WM: RMSE=0.03989902138710022, MAE=0.03144265338778496, MAPE=4.281774163246155%\n",
      "WMB: RMSE=0.041625529527664185, MAE=0.032655686140060425, MAPE=6.5168604254722595%\n",
      "WMT: RMSE=0.04001828283071518, MAE=0.02966395579278469, MAPE=6.209675967693329%\n",
      "WRB: RMSE=0.041282668709754944, MAE=0.030981115996837616, MAPE=5.408763885498047%\n",
      "WST: RMSE=0.04096667096018791, MAE=0.0299516674131155, MAPE=33.416908979415894%\n",
      "WTW: RMSE=0.04281020537018776, MAE=0.031995989382267, MAPE=5.819873884320259%\n",
      "WY: RMSE=0.046038635075092316, MAE=0.035199880599975586, MAPE=9.105706959962845%\n",
      "WYNN: RMSE=0.027383936569094658, MAE=0.02067124843597412, MAPE=4.869170859456062%\n",
      "XEL: RMSE=0.04174294322729111, MAE=0.03232916444540024, MAPE=4.993224889039993%\n",
      "XOM: RMSE=0.04548605531454086, MAE=0.035179946571588516, MAPE=19.367393851280212%\n",
      "XRAY: RMSE=0.04160647839307785, MAE=0.029388438910245895, MAPE=11.261525005102158%\n",
      "XYL: RMSE=0.03606327623128891, MAE=0.02763371728360653, MAPE=18.437854945659637%\n",
      "YUM: RMSE=0.0393097959458828, MAE=0.031128281727433205, MAPE=5.034162104129791%\n",
      "ZBH: RMSE=0.03893958032131195, MAE=0.028243187814950943, MAPE=69.00315880775452%\n",
      "ZBRA: RMSE=0.03635653480887413, MAE=0.026448940858244896, MAPE=41.9687956571579%\n",
      "ZION: RMSE=0.05570356920361519, MAE=0.04152701050043106, MAPE=39.74820077419281%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "model.eval()  # Switch to evaluation mode\n",
    "\n",
    "for idx, company in enumerate(df.columns):\n",
    "    # Find indices for the current company in the test dataset\n",
    "    company_test_indices = labels_test_combined == idx\n",
    "    if not np.any(company_test_indices):\n",
    "        print(f\"Skipping {company} due to no test data.\")\n",
    "        continue\n",
    "\n",
    "    X_test_company = X_test_combined[company_test_indices]\n",
    "    y_test_company = y_test_combined[company_test_indices]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_test_tensor = torch.tensor(X_test_company, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test_company, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test_tensor)\n",
    "        test_loss = criterion(test_output, y_test_tensor.view(-1, 1))\n",
    "        rmse = torch.sqrt(test_loss)\n",
    "\n",
    "        # Convert tensors to numpy arrays for MAE and MAPE calculations\n",
    "        test_output_np = test_output.numpy()\n",
    "        y_test_np = y_test_tensor.numpy()\n",
    "\n",
    "        mae = np.mean(np.abs(test_output_np - y_test_np))\n",
    "        mape = np.mean(np.abs((test_output_np - y_test_np) / y_test_np)) * 100 if np.any(y_test_np) else float('nan')\n",
    "\n",
    "    print(f\"{company}: RMSE={rmse.item()}, MAE={mae}, MAPE={mape}%\")\n",
    "    results.append({'Company': company, 'RMSE': rmse.item(), 'MAE': mae, 'MAPE': mape})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:17:46.529482Z",
     "start_time": "2024-03-26T12:17:44.179607Z"
    }
   },
   "id": "9febf34ba36919a9",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, columns=['Company Name', 'RMSE', 'MAE', 'MAPE'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:17:48.790593Z",
     "start_time": "2024-03-26T12:17:48.787876Z"
    }
   },
   "id": "78f0bdcf56c64c1d",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract MAPE values from results list\n",
    "mapes = [result['MAPE'] for result in results]\n",
    "\n",
    "# Define ranges\n",
    "ranges = [(0, 10), (10, 25), (25, 60), (60, 100), (100,1000), (1000, float('inf'))]\n",
    "\n",
    "# Initialize count for each range\n",
    "counts = [0] * len(ranges)\n",
    "\n",
    "# Count the number of data points in each range\n",
    "for mape in mapes:\n",
    "    for i, (start, end) in enumerate(ranges):\n",
    "        if start <= mape < end:\n",
    "            counts[i] += 1\n",
    "            break\n",
    "    else:\n",
    "        counts[-1] += 1  # For values above 100\n",
    "\n",
    "# Define range labels\n",
    "range_labels = ['0-10', '10-25', '25-60', '60-100', '100-1000', '1000+']\n",
    "\n",
    "# Plotting bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(ranges)), counts, color='skyblue', edgecolor='black')\n",
    "plt.xticks(range(len(ranges)), range_labels)\n",
    "plt.title('Distribution of MAPE Values')\n",
    "plt.xlabel('MAPE Range')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:18:11.172176Z",
     "start_time": "2024-03-26T12:18:10.985219Z"
    }
   },
   "id": "996a5197c82889e3",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_df.to_excel('/Users/keremsmacbook/DataspellProjects/MultivariateForecasting/OutputData/S&P500_NBeats_V0.2.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:20:51.194209Z",
     "start_time": "2024-03-26T12:20:51.173110Z"
    }
   },
   "id": "8144fda1fd083729",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "99de23e6f0d0ec72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
