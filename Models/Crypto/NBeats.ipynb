{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Prerequisite </h2>\n",
    "- pip install torch    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b96be4f5deceda"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:04:28.818154Z",
     "start_time": "2024-06-06T11:04:28.814903Z"
    }
   },
   "id": "45f8cd0c6bac992a",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/keremsmacbook/DataspellProjects/MultivariateForecasting/Data/Crypto.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:10:37.245263Z",
     "start_time": "2024-06-06T11:10:37.238720Z"
    }
   },
   "id": "7fc4c59fc0fb04e1",
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0                       Date  bitcoin_Price  ethereum_Price  \\\n0           0  2023-12-25 00:00:00 UTC+0   43621.237697     2272.949384   \n1           1  2023-12-24 00:00:00 UTC+0   43058.521704     2268.370864   \n\n   avax_Price  binance_Price  doge_Price  cardano_Price  polkadot_Price  \\\n0   48.298842     267.152891    0.094472       0.625149        9.210169   \n1   47.982728     264.894649    0.091928       0.593517        8.649198   \n\n   ripple_Price  solana_Price  tron_Price  \n0      0.646002    120.937582    0.105882  \n1      0.613854    112.642132    0.106460  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Date</th>\n      <th>bitcoin_Price</th>\n      <th>ethereum_Price</th>\n      <th>avax_Price</th>\n      <th>binance_Price</th>\n      <th>doge_Price</th>\n      <th>cardano_Price</th>\n      <th>polkadot_Price</th>\n      <th>ripple_Price</th>\n      <th>solana_Price</th>\n      <th>tron_Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2023-12-25 00:00:00 UTC+0</td>\n      <td>43621.237697</td>\n      <td>2272.949384</td>\n      <td>48.298842</td>\n      <td>267.152891</td>\n      <td>0.094472</td>\n      <td>0.625149</td>\n      <td>9.210169</td>\n      <td>0.646002</td>\n      <td>120.937582</td>\n      <td>0.105882</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2023-12-24 00:00:00 UTC+0</td>\n      <td>43058.521704</td>\n      <td>2268.370864</td>\n      <td>47.982728</td>\n      <td>264.894649</td>\n      <td>0.091928</td>\n      <td>0.593517</td>\n      <td>8.649198</td>\n      <td>0.613854</td>\n      <td>112.642132</td>\n      <td>0.106460</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:10:37.697038Z",
     "start_time": "2024-06-06T11:10:37.691434Z"
    }
   },
   "id": "dbc70b7852c60d2c",
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['Date'] = df['Date'].str.replace(' UTC', '')\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d %H:%M:%S%z')\n",
    "df.set_index('Date', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:10:37.932520Z",
     "start_time": "2024-06-06T11:10:37.925351Z"
    }
   },
   "id": "f53b8fd8a4597732",
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(1089, 11)"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:10:38.145424Z",
     "start_time": "2024-06-06T11:10:38.142662Z"
    }
   },
   "id": "340643e748d40bdd",
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                           Unnamed: 0  bitcoin_Price  ethereum_Price  \\\nDate                                                                   \n2023-12-25 00:00:00+00:00           0   43621.237697     2272.949384   \n2023-12-24 00:00:00+00:00           1   43058.521704     2268.370864   \n2023-12-23 00:00:00+00:00           2   43783.239603     2312.957281   \n2023-12-22 00:00:00+00:00           3   44021.347627     2328.084867   \n2023-12-21 00:00:00+00:00           4   43855.294074     2238.018423   \n2023-12-20 00:00:00+00:00           5   43637.571325     2199.705986   \n2023-12-19 00:00:00+00:00           6   42275.503840     2177.727320   \n2023-12-18 00:00:00+00:00           7   42632.732664     2216.921088   \n2023-12-17 00:00:00+00:00           8   41411.574489     2197.782227   \n2023-12-16 00:00:00+00:00           9   42235.985646     2227.348631   \n\n                           avax_Price  binance_Price  doge_Price  \\\nDate                                                               \n2023-12-25 00:00:00+00:00   48.298842     267.152891    0.094472   \n2023-12-24 00:00:00+00:00   47.982728     264.894649    0.091928   \n2023-12-23 00:00:00+00:00   48.155064     271.591529    0.093645   \n2023-12-22 00:00:00+00:00   45.468036     271.487518    0.095322   \n2023-12-21 00:00:00+00:00   46.050435     271.445952    0.094995   \n2023-12-20 00:00:00+00:00   43.433527     260.168629    0.091417   \n2023-12-19 00:00:00+00:00   39.791542     252.051420    0.090191   \n2023-12-18 00:00:00+00:00   41.082502     241.281920    0.092002   \n2023-12-17 00:00:00+00:00   40.678957     239.032809    0.092960   \n2023-12-16 00:00:00+00:00   42.057678     243.973830    0.096907   \n\n                           cardano_Price  polkadot_Price  ripple_Price  \\\nDate                                                                     \n2023-12-25 00:00:00+00:00       0.625149        9.210169      0.646002   \n2023-12-24 00:00:00+00:00       0.593517        8.649198      0.613854   \n2023-12-23 00:00:00+00:00       0.614550        8.332853      0.620245   \n2023-12-22 00:00:00+00:00       0.623957        7.978560      0.625628   \n2023-12-21 00:00:00+00:00       0.636154        8.343146      0.623467   \n2023-12-20 00:00:00+00:00       0.589006        6.941014      0.616860   \n2023-12-19 00:00:00+00:00       0.575898        6.710684      0.604835   \n2023-12-18 00:00:00+00:00       0.600966        6.876500      0.612367   \n2023-12-17 00:00:00+00:00       0.580505        6.789950      0.610150   \n2023-12-16 00:00:00+00:00       0.606587        7.089570      0.619568   \n\n                           solana_Price  tron_Price  \nDate                                                 \n2023-12-25 00:00:00+00:00    120.937582    0.105882  \n2023-12-24 00:00:00+00:00    112.642132    0.106460  \n2023-12-23 00:00:00+00:00    108.092274    0.106865  \n2023-12-22 00:00:00+00:00     98.076259    0.105204  \n2023-12-21 00:00:00+00:00     93.901695    0.104655  \n2023-12-20 00:00:00+00:00     82.169887    0.102694  \n2023-12-19 00:00:00+00:00     72.751983    0.100546  \n2023-12-18 00:00:00+00:00     74.357978    0.100896  \n2023-12-17 00:00:00+00:00     71.107822    0.102039  \n2023-12-16 00:00:00+00:00     73.436505    0.102765  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>bitcoin_Price</th>\n      <th>ethereum_Price</th>\n      <th>avax_Price</th>\n      <th>binance_Price</th>\n      <th>doge_Price</th>\n      <th>cardano_Price</th>\n      <th>polkadot_Price</th>\n      <th>ripple_Price</th>\n      <th>solana_Price</th>\n      <th>tron_Price</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2023-12-25 00:00:00+00:00</th>\n      <td>0</td>\n      <td>43621.237697</td>\n      <td>2272.949384</td>\n      <td>48.298842</td>\n      <td>267.152891</td>\n      <td>0.094472</td>\n      <td>0.625149</td>\n      <td>9.210169</td>\n      <td>0.646002</td>\n      <td>120.937582</td>\n      <td>0.105882</td>\n    </tr>\n    <tr>\n      <th>2023-12-24 00:00:00+00:00</th>\n      <td>1</td>\n      <td>43058.521704</td>\n      <td>2268.370864</td>\n      <td>47.982728</td>\n      <td>264.894649</td>\n      <td>0.091928</td>\n      <td>0.593517</td>\n      <td>8.649198</td>\n      <td>0.613854</td>\n      <td>112.642132</td>\n      <td>0.106460</td>\n    </tr>\n    <tr>\n      <th>2023-12-23 00:00:00+00:00</th>\n      <td>2</td>\n      <td>43783.239603</td>\n      <td>2312.957281</td>\n      <td>48.155064</td>\n      <td>271.591529</td>\n      <td>0.093645</td>\n      <td>0.614550</td>\n      <td>8.332853</td>\n      <td>0.620245</td>\n      <td>108.092274</td>\n      <td>0.106865</td>\n    </tr>\n    <tr>\n      <th>2023-12-22 00:00:00+00:00</th>\n      <td>3</td>\n      <td>44021.347627</td>\n      <td>2328.084867</td>\n      <td>45.468036</td>\n      <td>271.487518</td>\n      <td>0.095322</td>\n      <td>0.623957</td>\n      <td>7.978560</td>\n      <td>0.625628</td>\n      <td>98.076259</td>\n      <td>0.105204</td>\n    </tr>\n    <tr>\n      <th>2023-12-21 00:00:00+00:00</th>\n      <td>4</td>\n      <td>43855.294074</td>\n      <td>2238.018423</td>\n      <td>46.050435</td>\n      <td>271.445952</td>\n      <td>0.094995</td>\n      <td>0.636154</td>\n      <td>8.343146</td>\n      <td>0.623467</td>\n      <td>93.901695</td>\n      <td>0.104655</td>\n    </tr>\n    <tr>\n      <th>2023-12-20 00:00:00+00:00</th>\n      <td>5</td>\n      <td>43637.571325</td>\n      <td>2199.705986</td>\n      <td>43.433527</td>\n      <td>260.168629</td>\n      <td>0.091417</td>\n      <td>0.589006</td>\n      <td>6.941014</td>\n      <td>0.616860</td>\n      <td>82.169887</td>\n      <td>0.102694</td>\n    </tr>\n    <tr>\n      <th>2023-12-19 00:00:00+00:00</th>\n      <td>6</td>\n      <td>42275.503840</td>\n      <td>2177.727320</td>\n      <td>39.791542</td>\n      <td>252.051420</td>\n      <td>0.090191</td>\n      <td>0.575898</td>\n      <td>6.710684</td>\n      <td>0.604835</td>\n      <td>72.751983</td>\n      <td>0.100546</td>\n    </tr>\n    <tr>\n      <th>2023-12-18 00:00:00+00:00</th>\n      <td>7</td>\n      <td>42632.732664</td>\n      <td>2216.921088</td>\n      <td>41.082502</td>\n      <td>241.281920</td>\n      <td>0.092002</td>\n      <td>0.600966</td>\n      <td>6.876500</td>\n      <td>0.612367</td>\n      <td>74.357978</td>\n      <td>0.100896</td>\n    </tr>\n    <tr>\n      <th>2023-12-17 00:00:00+00:00</th>\n      <td>8</td>\n      <td>41411.574489</td>\n      <td>2197.782227</td>\n      <td>40.678957</td>\n      <td>239.032809</td>\n      <td>0.092960</td>\n      <td>0.580505</td>\n      <td>6.789950</td>\n      <td>0.610150</td>\n      <td>71.107822</td>\n      <td>0.102039</td>\n    </tr>\n    <tr>\n      <th>2023-12-16 00:00:00+00:00</th>\n      <td>9</td>\n      <td>42235.985646</td>\n      <td>2227.348631</td>\n      <td>42.057678</td>\n      <td>243.973830</td>\n      <td>0.096907</td>\n      <td>0.606587</td>\n      <td>7.089570</td>\n      <td>0.619568</td>\n      <td>73.436505</td>\n      <td>0.102765</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:10:38.310311Z",
     "start_time": "2024-06-06T11:10:38.304556Z"
    }
   },
   "id": "3b7b7417b0bdba39",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.drop(\"Unnamed: 0\", inplace = True, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:10:38.473257Z",
     "start_time": "2024-06-06T11:10:38.471051Z"
    }
   },
   "id": "bb590d8e42618f28",
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                           bitcoin_Price  ethereum_Price  avax_Price  \\\nDate                                                                   \n2023-12-25 00:00:00+00:00   43621.237697     2272.949384   48.298842   \n2023-12-24 00:00:00+00:00   43058.521704     2268.370864   47.982728   \n2023-12-23 00:00:00+00:00   43783.239603     2312.957281   48.155064   \n2023-12-22 00:00:00+00:00   44021.347627     2328.084867   45.468036   \n2023-12-21 00:00:00+00:00   43855.294074     2238.018423   46.050435   \n2023-12-20 00:00:00+00:00   43637.571325     2199.705986   43.433527   \n2023-12-19 00:00:00+00:00   42275.503840     2177.727320   39.791542   \n2023-12-18 00:00:00+00:00   42632.732664     2216.921088   41.082502   \n2023-12-17 00:00:00+00:00   41411.574489     2197.782227   40.678957   \n2023-12-16 00:00:00+00:00   42235.985646     2227.348631   42.057678   \n\n                           binance_Price  doge_Price  cardano_Price  \\\nDate                                                                  \n2023-12-25 00:00:00+00:00     267.152891    0.094472       0.625149   \n2023-12-24 00:00:00+00:00     264.894649    0.091928       0.593517   \n2023-12-23 00:00:00+00:00     271.591529    0.093645       0.614550   \n2023-12-22 00:00:00+00:00     271.487518    0.095322       0.623957   \n2023-12-21 00:00:00+00:00     271.445952    0.094995       0.636154   \n2023-12-20 00:00:00+00:00     260.168629    0.091417       0.589006   \n2023-12-19 00:00:00+00:00     252.051420    0.090191       0.575898   \n2023-12-18 00:00:00+00:00     241.281920    0.092002       0.600966   \n2023-12-17 00:00:00+00:00     239.032809    0.092960       0.580505   \n2023-12-16 00:00:00+00:00     243.973830    0.096907       0.606587   \n\n                           polkadot_Price  ripple_Price  solana_Price  \\\nDate                                                                    \n2023-12-25 00:00:00+00:00        9.210169      0.646002    120.937582   \n2023-12-24 00:00:00+00:00        8.649198      0.613854    112.642132   \n2023-12-23 00:00:00+00:00        8.332853      0.620245    108.092274   \n2023-12-22 00:00:00+00:00        7.978560      0.625628     98.076259   \n2023-12-21 00:00:00+00:00        8.343146      0.623467     93.901695   \n2023-12-20 00:00:00+00:00        6.941014      0.616860     82.169887   \n2023-12-19 00:00:00+00:00        6.710684      0.604835     72.751983   \n2023-12-18 00:00:00+00:00        6.876500      0.612367     74.357978   \n2023-12-17 00:00:00+00:00        6.789950      0.610150     71.107822   \n2023-12-16 00:00:00+00:00        7.089570      0.619568     73.436505   \n\n                           tron_Price  \nDate                                   \n2023-12-25 00:00:00+00:00    0.105882  \n2023-12-24 00:00:00+00:00    0.106460  \n2023-12-23 00:00:00+00:00    0.106865  \n2023-12-22 00:00:00+00:00    0.105204  \n2023-12-21 00:00:00+00:00    0.104655  \n2023-12-20 00:00:00+00:00    0.102694  \n2023-12-19 00:00:00+00:00    0.100546  \n2023-12-18 00:00:00+00:00    0.100896  \n2023-12-17 00:00:00+00:00    0.102039  \n2023-12-16 00:00:00+00:00    0.102765  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bitcoin_Price</th>\n      <th>ethereum_Price</th>\n      <th>avax_Price</th>\n      <th>binance_Price</th>\n      <th>doge_Price</th>\n      <th>cardano_Price</th>\n      <th>polkadot_Price</th>\n      <th>ripple_Price</th>\n      <th>solana_Price</th>\n      <th>tron_Price</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2023-12-25 00:00:00+00:00</th>\n      <td>43621.237697</td>\n      <td>2272.949384</td>\n      <td>48.298842</td>\n      <td>267.152891</td>\n      <td>0.094472</td>\n      <td>0.625149</td>\n      <td>9.210169</td>\n      <td>0.646002</td>\n      <td>120.937582</td>\n      <td>0.105882</td>\n    </tr>\n    <tr>\n      <th>2023-12-24 00:00:00+00:00</th>\n      <td>43058.521704</td>\n      <td>2268.370864</td>\n      <td>47.982728</td>\n      <td>264.894649</td>\n      <td>0.091928</td>\n      <td>0.593517</td>\n      <td>8.649198</td>\n      <td>0.613854</td>\n      <td>112.642132</td>\n      <td>0.106460</td>\n    </tr>\n    <tr>\n      <th>2023-12-23 00:00:00+00:00</th>\n      <td>43783.239603</td>\n      <td>2312.957281</td>\n      <td>48.155064</td>\n      <td>271.591529</td>\n      <td>0.093645</td>\n      <td>0.614550</td>\n      <td>8.332853</td>\n      <td>0.620245</td>\n      <td>108.092274</td>\n      <td>0.106865</td>\n    </tr>\n    <tr>\n      <th>2023-12-22 00:00:00+00:00</th>\n      <td>44021.347627</td>\n      <td>2328.084867</td>\n      <td>45.468036</td>\n      <td>271.487518</td>\n      <td>0.095322</td>\n      <td>0.623957</td>\n      <td>7.978560</td>\n      <td>0.625628</td>\n      <td>98.076259</td>\n      <td>0.105204</td>\n    </tr>\n    <tr>\n      <th>2023-12-21 00:00:00+00:00</th>\n      <td>43855.294074</td>\n      <td>2238.018423</td>\n      <td>46.050435</td>\n      <td>271.445952</td>\n      <td>0.094995</td>\n      <td>0.636154</td>\n      <td>8.343146</td>\n      <td>0.623467</td>\n      <td>93.901695</td>\n      <td>0.104655</td>\n    </tr>\n    <tr>\n      <th>2023-12-20 00:00:00+00:00</th>\n      <td>43637.571325</td>\n      <td>2199.705986</td>\n      <td>43.433527</td>\n      <td>260.168629</td>\n      <td>0.091417</td>\n      <td>0.589006</td>\n      <td>6.941014</td>\n      <td>0.616860</td>\n      <td>82.169887</td>\n      <td>0.102694</td>\n    </tr>\n    <tr>\n      <th>2023-12-19 00:00:00+00:00</th>\n      <td>42275.503840</td>\n      <td>2177.727320</td>\n      <td>39.791542</td>\n      <td>252.051420</td>\n      <td>0.090191</td>\n      <td>0.575898</td>\n      <td>6.710684</td>\n      <td>0.604835</td>\n      <td>72.751983</td>\n      <td>0.100546</td>\n    </tr>\n    <tr>\n      <th>2023-12-18 00:00:00+00:00</th>\n      <td>42632.732664</td>\n      <td>2216.921088</td>\n      <td>41.082502</td>\n      <td>241.281920</td>\n      <td>0.092002</td>\n      <td>0.600966</td>\n      <td>6.876500</td>\n      <td>0.612367</td>\n      <td>74.357978</td>\n      <td>0.100896</td>\n    </tr>\n    <tr>\n      <th>2023-12-17 00:00:00+00:00</th>\n      <td>41411.574489</td>\n      <td>2197.782227</td>\n      <td>40.678957</td>\n      <td>239.032809</td>\n      <td>0.092960</td>\n      <td>0.580505</td>\n      <td>6.789950</td>\n      <td>0.610150</td>\n      <td>71.107822</td>\n      <td>0.102039</td>\n    </tr>\n    <tr>\n      <th>2023-12-16 00:00:00+00:00</th>\n      <td>42235.985646</td>\n      <td>2227.348631</td>\n      <td>42.057678</td>\n      <td>243.973830</td>\n      <td>0.096907</td>\n      <td>0.606587</td>\n      <td>7.089570</td>\n      <td>0.619568</td>\n      <td>73.436505</td>\n      <td>0.102765</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:10:38.803463Z",
     "start_time": "2024-06-06T11:10:38.796327Z"
    }
   },
   "id": "aca485f175fd7867",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "all_data_normalized = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:10:58.211910Z",
     "start_time": "2024-06-06T11:10:58.209245Z"
    }
   },
   "id": "f33d8a0cdc42689e",
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, input_size, layer_size=64, output_size=None):\n",
    "        super(NBeatsBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, layer_size)\n",
    "        self.fc2 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc3 = nn.Linear(layer_size, layer_size)\n",
    "        self.fc4 = nn.Linear(layer_size, output_size if output_size is not None else input_size)\n",
    "        self.output_size = output_size if output_size is not None else input_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        forecast = self.fc4(x)\n",
    "        backcast = x[:, :self.output_size]\n",
    "        return backcast, forecast\n",
    "\n",
    "class NBeatsNet(nn.Module):\n",
    "    def __init__(self, input_size, forecast_length, stack_types=[1, 1], nb_blocks_per_stack=4, layer_size=64, thetas_dim=[4, 8], share_weights_in_stack=False):\n",
    "        super(NBeatsNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.forecast_length = forecast_length\n",
    "        self.stack_types = stack_types\n",
    "        self.nb_blocks_per_stack = nb_blocks_per_stack\n",
    "\n",
    "        blocks = []\n",
    "        for stack_id in range(len(stack_types)):\n",
    "            for block_id in range(nb_blocks_per_stack):\n",
    "                block_init = NBeatsBlock(input_size=input_size, layer_size=layer_size, output_size=forecast_length) # Adjusted to use 'input_size'\n",
    "                blocks.append(block_init)\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        forecast = torch.zeros(x.size(0), self.forecast_length, device=x.device)  # Adjusted to properly initialize the forecast tensor\n",
    "        for block in self.blocks:\n",
    "            backcast, block_forecast = block(x.view(x.size(0), -1))\n",
    "            forecast += block_forecast.view(forecast.size())\n",
    "        return forecast"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:10:58.835423Z",
     "start_time": "2024-06-06T11:10:58.828181Z"
    }
   },
   "id": "3a92397e0b44f4bf",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length, label):\n",
    "    X, y, labels = [], [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "        labels.append(label)\n",
    "    return np.array(X), np.array(y), np.array(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:10:59.439431Z",
     "start_time": "2024-06-06T11:10:59.436753Z"
    }
   },
   "id": "bd369e0cbe661dd2",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_combined shape: (40650, 30, 1)\n",
      "y_train_combined shape: (40650, 1)\n",
      "labels_train_combined shape: (40650,)\n",
      "X_val_combined shape: (4900, 30, 1)\n",
      "y_val_combined shape: (4900, 1)\n",
      "labels_val_combined shape: (4900,)\n",
      "X_test_combined shape: (2180, 30, 1)\n",
      "y_test_combined shape: (2180, 1)\n",
      "labels_test_combined shape: (2180,)\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 30\n",
    "test_size = 0.2\n",
    "validation_fraction = 0.1  \n",
    "\n",
    "X_train_combined, y_train_combined, labels_train_combined = [], [], []\n",
    "X_val_combined, y_val_combined, labels_val_combined = [], [], []\n",
    "X_test_combined, y_test_combined, labels_test_combined = [], [], []\n",
    "\n",
    "def create_sequences(data, sequence_length, label):\n",
    "    X, y, labels = [], [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "        labels.append(label)\n",
    "    return np.array(X), np.array(y), np.array(labels)\n",
    "\n",
    "for idx, company in enumerate(df.columns):\n",
    "    company_data = df[company].dropna().values.reshape(-1, 1)  \n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    normalized_data = scaler.fit_transform(company_data)\n",
    "\n",
    "    if len(normalized_data) <= sequence_length:\n",
    "        print(f\"Skipping {company}: Not enough data for sequence creation.\")\n",
    "        continue\n",
    "\n",
    "    train_size = int(len(normalized_data) * (1 - test_size))\n",
    "    train_data = normalized_data[:train_size]\n",
    "    test_data = normalized_data[train_size - sequence_length:]  \n",
    "\n",
    "    X_test_tmp, y_test_tmp, labels_test_tmp = create_sequences(test_data, sequence_length, idx)\n",
    "    X_test_combined.extend(X_test_tmp)\n",
    "    y_test_combined.extend(y_test_tmp)\n",
    "    labels_test_combined.extend(labels_test_tmp)\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=int(1 / validation_fraction))\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(train_data):\n",
    "        X_train_tmp, y_train_tmp, labels_train_tmp = create_sequences(train_data[train_idx], sequence_length, idx)\n",
    "        X_val_tmp, y_val_tmp, labels_val_tmp = create_sequences(train_data[val_idx], sequence_length, idx)\n",
    "\n",
    "        X_train_combined.extend(X_train_tmp)\n",
    "        y_train_combined.extend(y_train_tmp)\n",
    "        labels_train_combined.extend(labels_train_tmp)\n",
    "\n",
    "        X_val_combined.extend(X_val_tmp)\n",
    "        y_val_combined.extend(y_val_tmp)\n",
    "        labels_val_combined.extend(labels_val_tmp)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train_combined = np.array(X_train_combined)\n",
    "y_train_combined = np.array(y_train_combined).reshape(-1, 1)\n",
    "labels_train_combined = np.array(labels_train_combined)\n",
    "\n",
    "X_val_combined = np.array(X_val_combined)\n",
    "y_val_combined = np.array(y_val_combined).reshape(-1, 1)\n",
    "labels_val_combined = np.array(labels_val_combined)\n",
    "\n",
    "X_test_combined = np.array(X_test_combined)\n",
    "y_test_combined = np.array(y_test_combined).reshape(-1, 1)\n",
    "labels_test_combined = np.array(labels_test_combined)\n",
    "\n",
    "# Print the shapes\n",
    "print(f\"X_train_combined shape: {X_train_combined.shape}\")\n",
    "print(f\"y_train_combined shape: {y_train_combined.shape}\")\n",
    "print(f\"labels_train_combined shape: {labels_train_combined.shape}\")\n",
    "print(f\"X_val_combined shape: {X_val_combined.shape}\")\n",
    "print(f\"y_val_combined shape: {y_val_combined.shape}\")\n",
    "print(f\"labels_val_combined shape: {labels_val_combined.shape}\")\n",
    "print(f\"X_test_combined shape: {X_test_combined.shape}\")\n",
    "print(f\"y_test_combined shape: {y_test_combined.shape}\")\n",
    "print(f\"labels_test_combined shape: {labels_test_combined.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:10:59.923364Z",
     "start_time": "2024-06-06T11:10:59.859836Z"
    }
   },
   "id": "ac9d5d1b69441008",
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 21004.82it/s]\n",
      "100%|██████████| 41/41 [00:00<00:00, 47570.25it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 7741.42it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 31441.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_convert_to_tensor(data, batch_size=1000, dtype=torch.float32):\n",
    "    tensor_chunks = []\n",
    "    for i in tqdm(range(0, len(data), batch_size)):\n",
    "        batch = data[i:i+batch_size]\n",
    "        tensor = torch.tensor(batch, dtype=dtype)\n",
    "        tensor_chunks.append(tensor)\n",
    "    return torch.cat(tensor_chunks, dim=0)\n",
    "\n",
    "# Convert training and validation data to tensors\n",
    "X_train_tensor = batch_convert_to_tensor(X_train_combined, batch_size=1000)\n",
    "y_train_tensor = batch_convert_to_tensor(y_train_combined, batch_size=1000)\n",
    "X_val_tensor = batch_convert_to_tensor(X_val_combined, batch_size=1000)\n",
    "y_val_tensor = batch_convert_to_tensor(y_val_combined, batch_size=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:11:16.285221Z",
     "start_time": "2024-06-06T11:11:16.268351Z"
    }
   },
   "id": "9e5a043431cfc56e",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the model training\n",
      "\n",
      "Epoch 1/500\n",
      "Epoch 1, Batch 41/41, Batch loss: 0.011357094161212444\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 1 completed in 0.48 seconds, Total Training Loss: 0.03967309570956475\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.028388979285955428\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 2/500\n",
      "Epoch 2, Batch 41/41, Batch loss: 0.0048033506609499454\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 2 completed in 0.69 seconds, Total Training Loss: 0.005893807119985179\n",
      "\n",
      "Epoch 3/500\n",
      "Epoch 3, Batch 41/41, Batch loss: 0.003644001204520464\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 3 completed in 0.43 seconds, Total Training Loss: 0.0031515696751828327\n",
      "\n",
      "Epoch 4/500\n",
      "Epoch 4, Batch 41/41, Batch loss: 0.002864713780581951\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 4 completed in 0.46 seconds, Total Training Loss: 0.0021468558233009834\n",
      "\n",
      "Epoch 5/500\n",
      "Epoch 5, Batch 41/41, Batch loss: 0.0024964935146272182\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 5 completed in 0.43 seconds, Total Training Loss: 0.0016969476000224127\n",
      "\n",
      "Epoch 6/500\n",
      "Epoch 6, Batch 41/41, Batch loss: 0.0022574064787477255\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 6 completed in 0.44 seconds, Total Training Loss: 0.0014338027064762328\n",
      "\n",
      "Epoch 7/500\n",
      "Epoch 7, Batch 41/41, Batch loss: 0.002077359240502119\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 7 completed in 0.44 seconds, Total Training Loss: 0.0012799392072072185\n",
      "\n",
      "Epoch 8/500\n",
      "Epoch 8, Batch 41/41, Batch loss: 0.001943595940247178\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 8 completed in 0.43 seconds, Total Training Loss: 0.0011812393786385655\n",
      "\n",
      "Epoch 9/500\n",
      "Epoch 9, Batch 41/41, Batch loss: 0.0018439656123518944\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 9 completed in 0.46 seconds, Total Training Loss: 0.0011132402929744298\n",
      "\n",
      "Epoch 10/500\n",
      "Epoch 10, Batch 41/41, Batch loss: 0.0017452881438657641\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 10 completed in 0.45 seconds, Total Training Loss: 0.0010358028003105485\n",
      "\n",
      "Epoch 11/500\n",
      "Epoch 11, Batch 41/41, Batch loss: 0.0016390476375818253\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 11 completed in 0.46 seconds, Total Training Loss: 0.0009545216762046217\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0031762179685756566\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 12/500\n",
      "Epoch 12, Batch 41/41, Batch loss: 0.001573484856635332\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 12 completed in 0.43 seconds, Total Training Loss: 0.0008880319462438886\n",
      "\n",
      "Epoch 13/500\n",
      "Epoch 13, Batch 41/41, Batch loss: 0.0015266431728377938\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 13 completed in 0.44 seconds, Total Training Loss: 0.0009104787251733175\n",
      "\n",
      "Epoch 14/500\n",
      "Epoch 14, Batch 41/41, Batch loss: 0.0014980740379542112\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 14 completed in 0.42 seconds, Total Training Loss: 0.000858202847799786\n",
      "\n",
      "Epoch 15/500\n",
      "Epoch 15, Batch 41/41, Batch loss: 0.0014646691270172596\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 15 completed in 0.43 seconds, Total Training Loss: 0.0009486560533021963\n",
      "\n",
      "Epoch 16/500\n",
      "Epoch 16, Batch 41/41, Batch loss: 0.001433068886399269\n",
      "Memory Usage: 556.42 MB\n",
      "Epoch 16 completed in 0.44 seconds, Total Training Loss: 0.0007834997325894287\n",
      "\n",
      "Epoch 17/500\n",
      "Epoch 17, Batch 41/41, Batch loss: 0.0013978423085063696\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 17 completed in 0.44 seconds, Total Training Loss: 0.0008394300819462642\n",
      "\n",
      "Epoch 18/500\n",
      "Epoch 18, Batch 41/41, Batch loss: 0.0014087900053709745\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 18 completed in 0.46 seconds, Total Training Loss: 0.0007781373323069136\n",
      "\n",
      "Epoch 19/500\n",
      "Epoch 19, Batch 41/41, Batch loss: 0.001369141391478479\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 19 completed in 0.44 seconds, Total Training Loss: 0.0009561421295696097\n",
      "\n",
      "Epoch 20/500\n",
      "Epoch 20, Batch 41/41, Batch loss: 0.0013776489067822695\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 20 completed in 0.42 seconds, Total Training Loss: 0.00073442048543678\n",
      "\n",
      "Epoch 21/500\n",
      "Epoch 21, Batch 41/41, Batch loss: 0.001337842782959342\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 21 completed in 0.42 seconds, Total Training Loss: 0.0008980339267376311\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.002398346085101366\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 22/500\n",
      "Epoch 22, Batch 41/41, Batch loss: 0.001352895749732852\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 22 completed in 0.43 seconds, Total Training Loss: 0.0007061698799101045\n",
      "\n",
      "Epoch 23/500\n",
      "Epoch 23, Batch 41/41, Batch loss: 0.0013171298196539283\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 23 completed in 0.45 seconds, Total Training Loss: 0.0008253236143301786\n",
      "\n",
      "Epoch 24/500\n",
      "Epoch 24, Batch 41/41, Batch loss: 0.0013508254196494818\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 24 completed in 0.45 seconds, Total Training Loss: 0.000714890996566349\n",
      "\n",
      "Epoch 25/500\n",
      "Epoch 25, Batch 41/41, Batch loss: 0.001306187710724771\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 25 completed in 0.45 seconds, Total Training Loss: 0.0009063207090082692\n",
      "\n",
      "Epoch 26/500\n",
      "Epoch 26, Batch 41/41, Batch loss: 0.0013586214045062661\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 26 completed in 0.43 seconds, Total Training Loss: 0.000716768810490044\n",
      "\n",
      "Epoch 27/500\n",
      "Epoch 27, Batch 41/41, Batch loss: 0.0012934008846059442\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 27 completed in 0.45 seconds, Total Training Loss: 0.0009796048845334842\n",
      "\n",
      "Epoch 28/500\n",
      "Epoch 28, Batch 41/41, Batch loss: 0.0012806900776922703\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 28 completed in 0.44 seconds, Total Training Loss: 0.0006631477091748199\n",
      "\n",
      "Epoch 29/500\n",
      "Epoch 29, Batch 41/41, Batch loss: 0.001255570212379098\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 29 completed in 0.45 seconds, Total Training Loss: 0.0007318303451473007\n",
      "\n",
      "Epoch 30/500\n",
      "Epoch 30, Batch 41/41, Batch loss: 0.0012819248950108886\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 30 completed in 0.46 seconds, Total Training Loss: 0.0006508728173041785\n",
      "\n",
      "Epoch 31/500\n",
      "Epoch 31, Batch 41/41, Batch loss: 0.0012587561504915357\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 31 completed in 0.45 seconds, Total Training Loss: 0.0007643188740674793\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0021823942428454758\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 32/500\n",
      "Epoch 32, Batch 41/41, Batch loss: 0.0013322792947292328\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 32 completed in 0.48 seconds, Total Training Loss: 0.0006779119948765672\n",
      "\n",
      "Epoch 33/500\n",
      "Epoch 33, Batch 41/41, Batch loss: 0.0012946838978677988\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 33 completed in 0.45 seconds, Total Training Loss: 0.0008937863380330183\n",
      "\n",
      "Epoch 34/500\n",
      "Epoch 34, Batch 41/41, Batch loss: 0.001408638316206634\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 34 completed in 0.45 seconds, Total Training Loss: 0.0007263138423895282\n",
      "\n",
      "Epoch 35/500\n",
      "Epoch 35, Batch 41/41, Batch loss: 0.0012945556081831455\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 35 completed in 0.45 seconds, Total Training Loss: 0.001049943865282552\n",
      "\n",
      "Epoch 36/500\n",
      "Epoch 36, Batch 41/41, Batch loss: 0.00129322602879256\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 36 completed in 0.46 seconds, Total Training Loss: 0.0006697408078740942\n",
      "\n",
      "Epoch 37/500\n",
      "Epoch 37, Batch 41/41, Batch loss: 0.0012576439185068011\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 37 completed in 0.45 seconds, Total Training Loss: 0.000754762720955513\n",
      "\n",
      "Epoch 38/500\n",
      "Epoch 38, Batch 41/41, Batch loss: 0.0013042261125519872\n",
      "Memory Usage: 556.44 MB\n",
      "Epoch 38 completed in 0.44 seconds, Total Training Loss: 0.0006532132337735853\n",
      "\n",
      "Epoch 39/500\n",
      "Epoch 39, Batch 41/41, Batch loss: 0.001264614169485867\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 39 completed in 0.44 seconds, Total Training Loss: 0.0008010931647376997\n",
      "\n",
      "Epoch 40/500\n",
      "Epoch 40, Batch 41/41, Batch loss: 0.0013427557423710823\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 40 completed in 0.43 seconds, Total Training Loss: 0.0006807393090240089\n",
      "\n",
      "Epoch 41/500\n",
      "Epoch 41, Batch 41/41, Batch loss: 0.0012565971119329333\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 41 completed in 0.44 seconds, Total Training Loss: 0.0009157951980142105\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0022111881989985704\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 42/500\n",
      "Epoch 42, Batch 41/41, Batch loss: 0.0013188528828322887\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 42 completed in 0.46 seconds, Total Training Loss: 0.0006839791309264511\n",
      "\n",
      "Epoch 43/500\n",
      "Epoch 43, Batch 41/41, Batch loss: 0.0012307234574109316\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 43 completed in 0.41 seconds, Total Training Loss: 0.0008808731984506111\n",
      "\n",
      "Epoch 44/500\n",
      "Epoch 44, Batch 41/41, Batch loss: 0.001250491593964398\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 44 completed in 0.43 seconds, Total Training Loss: 0.0006285285983211929\n",
      "\n",
      "Epoch 45/500\n",
      "Epoch 45, Batch 41/41, Batch loss: 0.0012098397128283978\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 45 completed in 0.45 seconds, Total Training Loss: 0.0007324612996876012\n",
      "\n",
      "Epoch 46/500\n",
      "Epoch 46, Batch 41/41, Batch loss: 0.0012605899246409535\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 46 completed in 0.46 seconds, Total Training Loss: 0.0006171329234082183\n",
      "\n",
      "Epoch 47/500\n",
      "Epoch 47, Batch 41/41, Batch loss: 0.0012204886879771948\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 47 completed in 0.49 seconds, Total Training Loss: 0.000740622901237036\n",
      "\n",
      "Epoch 48/500\n",
      "Epoch 48, Batch 41/41, Batch loss: 0.0013094524620100856\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 48 completed in 0.45 seconds, Total Training Loss: 0.0006436605660227786\n",
      "\n",
      "Epoch 49/500\n",
      "Epoch 49, Batch 41/41, Batch loss: 0.001247673761099577\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 49 completed in 0.44 seconds, Total Training Loss: 0.0008200926907792142\n",
      "\n",
      "Epoch 50/500\n",
      "Epoch 50, Batch 41/41, Batch loss: 0.0013662399724125862\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 50 completed in 0.45 seconds, Total Training Loss: 0.0006732357220110915\n",
      "\n",
      "Epoch 51/500\n",
      "Epoch 51, Batch 41/41, Batch loss: 0.0012599072651937604\n",
      "Memory Usage: 556.94 MB\n",
      "Epoch 51 completed in 0.44 seconds, Total Training Loss: 0.000914101214531423\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.002188093913719058\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 52/500\n",
      "Epoch 52, Batch 41/41, Batch loss: 0.0013413438573479652\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 52 completed in 0.43 seconds, Total Training Loss: 0.0006768769068535553\n",
      "\n",
      "Epoch 53/500\n",
      "Epoch 53, Batch 41/41, Batch loss: 0.0012353325728327036\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 53 completed in 0.44 seconds, Total Training Loss: 0.0008863846660682522\n",
      "\n",
      "Epoch 54/500\n",
      "Epoch 54, Batch 41/41, Batch loss: 0.0012723447289317846\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 54 completed in 0.43 seconds, Total Training Loss: 0.0006277419542595631\n",
      "\n",
      "Epoch 55/500\n",
      "Epoch 55, Batch 41/41, Batch loss: 0.0012184141669422388\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 55 completed in 0.43 seconds, Total Training Loss: 0.0007556956996090664\n",
      "\n",
      "Epoch 56/500\n",
      "Epoch 56, Batch 41/41, Batch loss: 0.0012766116997227073\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 56 completed in 0.45 seconds, Total Training Loss: 0.0006185790260177582\n",
      "\n",
      "Epoch 57/500\n",
      "Epoch 57, Batch 41/41, Batch loss: 0.0012207765830680728\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 57 completed in 0.44 seconds, Total Training Loss: 0.0007429041760610189\n",
      "\n",
      "Epoch 58/500\n",
      "Epoch 58, Batch 41/41, Batch loss: 0.0013108296552672982\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 58 completed in 0.45 seconds, Total Training Loss: 0.0006360263509034156\n",
      "\n",
      "Epoch 59/500\n",
      "Epoch 59, Batch 41/41, Batch loss: 0.0012345404829829931\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 59 completed in 0.46 seconds, Total Training Loss: 0.0008071975696621826\n",
      "\n",
      "Epoch 60/500\n",
      "Epoch 60, Batch 41/41, Batch loss: 0.0013457335298880935\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 60 completed in 0.43 seconds, Total Training Loss: 0.0006564428066911471\n",
      "\n",
      "Epoch 61/500\n",
      "Epoch 61, Batch 41/41, Batch loss: 0.0012425619643181562\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 61 completed in 0.44 seconds, Total Training Loss: 0.0008799433353266157\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.002113958285190165\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 62/500\n",
      "Epoch 62, Batch 41/41, Batch loss: 0.0013233317295089364\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 62 completed in 0.43 seconds, Total Training Loss: 0.0006523145261226313\n",
      "\n",
      "Epoch 63/500\n",
      "Epoch 63, Batch 41/41, Batch loss: 0.0012189316330477595\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 63 completed in 0.46 seconds, Total Training Loss: 0.0008537864845626556\n",
      "\n",
      "Epoch 64/500\n",
      "Epoch 64, Batch 41/41, Batch loss: 0.001291755703277886\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 64 completed in 0.45 seconds, Total Training Loss: 0.0006299816494702543\n",
      "\n",
      "Epoch 65/500\n",
      "Epoch 65, Batch 41/41, Batch loss: 0.0012066321214661002\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 65 completed in 0.46 seconds, Total Training Loss: 0.0007943996741498889\n",
      "\n",
      "Epoch 66/500\n",
      "Epoch 66, Batch 41/41, Batch loss: 0.0012572089908644557\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 66 completed in 0.44 seconds, Total Training Loss: 0.0006047285883474481\n",
      "\n",
      "Epoch 67/500\n",
      "Epoch 67, Batch 41/41, Batch loss: 0.0011926143197342753\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 67 completed in 0.44 seconds, Total Training Loss: 0.0007289659910517313\n",
      "\n",
      "Epoch 68/500\n",
      "Epoch 68, Batch 41/41, Batch loss: 0.0012664651731029153\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 68 completed in 0.44 seconds, Total Training Loss: 0.0006038788908371338\n",
      "\n",
      "Epoch 69/500\n",
      "Epoch 69, Batch 41/41, Batch loss: 0.0012088108342140913\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 69 completed in 0.45 seconds, Total Training Loss: 0.0007189980824477971\n",
      "\n",
      "Epoch 70/500\n",
      "Epoch 70, Batch 41/41, Batch loss: 0.0012946621282026172\n",
      "Memory Usage: 557.94 MB\n",
      "Epoch 70 completed in 0.45 seconds, Total Training Loss: 0.0006200248982811278\n",
      "\n",
      "Epoch 71/500\n",
      "Epoch 71, Batch 41/41, Batch loss: 0.0012195343151688576\n",
      "Memory Usage: 558.44 MB\n",
      "Epoch 71 completed in 0.44 seconds, Total Training Loss: 0.0007562560927997366\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.002053733915090561\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 72/500\n",
      "Epoch 72, Batch 41/41, Batch loss: 0.0013391615357249975\n",
      "Memory Usage: 558.44 MB\n",
      "Epoch 72 completed in 0.42 seconds, Total Training Loss: 0.0006408456706927476\n",
      "\n",
      "Epoch 73/500\n",
      "Epoch 73, Batch 41/41, Batch loss: 0.0012341326801106334\n",
      "Memory Usage: 558.44 MB\n",
      "Epoch 73 completed in 0.45 seconds, Total Training Loss: 0.0008317407762927072\n",
      "\n",
      "Epoch 74/500\n",
      "Epoch 74, Batch 41/41, Batch loss: 0.0013368736254051328\n",
      "Memory Usage: 558.44 MB\n",
      "Epoch 74 completed in 0.44 seconds, Total Training Loss: 0.0006464579497199937\n",
      "\n",
      "Epoch 75/500\n",
      "Epoch 75, Batch 41/41, Batch loss: 0.0012155388249084353\n",
      "Memory Usage: 558.44 MB\n",
      "Epoch 75 completed in 0.45 seconds, Total Training Loss: 0.000849683236305193\n",
      "\n",
      "Epoch 76/500\n",
      "Epoch 76, Batch 41/41, Batch loss: 0.0013054368318989873\n",
      "Memory Usage: 558.44 MB\n",
      "Epoch 76 completed in 0.46 seconds, Total Training Loss: 0.0006262356443430024\n",
      "\n",
      "Epoch 77/500\n",
      "Epoch 77, Batch 41/41, Batch loss: 0.0011999671114608645\n",
      "Memory Usage: 558.44 MB\n",
      "Epoch 77 completed in 0.44 seconds, Total Training Loss: 0.0007951462566114326\n",
      "\n",
      "Epoch 78/500\n",
      "Epoch 78, Batch 41/41, Batch loss: 0.001273376401513815\n",
      "Memory Usage: 558.44 MB\n",
      "Epoch 78 completed in 0.45 seconds, Total Training Loss: 0.0006007448773300162\n",
      "\n",
      "Epoch 79/500\n",
      "Epoch 79, Batch 41/41, Batch loss: 0.0012065722839906812\n",
      "Memory Usage: 558.44 MB\n",
      "Epoch 79 completed in 0.47 seconds, Total Training Loss: 0.0007388095918719135\n",
      "\n",
      "Epoch 80/500\n",
      "Epoch 80, Batch 41/41, Batch loss: 0.0013094805181026459\n",
      "Memory Usage: 558.44 MB\n",
      "Epoch 80 completed in 0.45 seconds, Total Training Loss: 0.0006193612768206881\n",
      "\n",
      "Epoch 81/500\n",
      "Epoch 81, Batch 41/41, Batch loss: 0.0012075458653271198\n",
      "Memory Usage: 558.44 MB\n",
      "Epoch 81 completed in 0.46 seconds, Total Training Loss: 0.0007752000798596205\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0020038305083289742\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 82/500\n",
      "Epoch 82, Batch 41/41, Batch loss: 0.0013016725424677134\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 82 completed in 0.47 seconds, Total Training Loss: 0.0006127358899059956\n",
      "\n",
      "Epoch 83/500\n",
      "Epoch 83, Batch 41/41, Batch loss: 0.0012046393239870667\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 83 completed in 0.46 seconds, Total Training Loss: 0.0007749370092713479\n",
      "\n",
      "Epoch 84/500\n",
      "Epoch 84, Batch 41/41, Batch loss: 0.001297548646107316\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 84 completed in 0.45 seconds, Total Training Loss: 0.0006123991649762568\n",
      "\n",
      "Epoch 85/500\n",
      "Epoch 85, Batch 41/41, Batch loss: 0.001198698882944882\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 85 completed in 0.42 seconds, Total Training Loss: 0.0007759556479513554\n",
      "\n",
      "Epoch 86/500\n",
      "Epoch 86, Batch 41/41, Batch loss: 0.0012847009347751737\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 86 completed in 0.46 seconds, Total Training Loss: 0.0006025030840935027\n",
      "\n",
      "Epoch 87/500\n",
      "Epoch 87, Batch 41/41, Batch loss: 0.0011919363168999553\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 87 completed in 0.45 seconds, Total Training Loss: 0.0007502111551641464\n",
      "\n",
      "Epoch 88/500\n",
      "Epoch 88, Batch 41/41, Batch loss: 0.0012834396911785007\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 88 completed in 0.43 seconds, Total Training Loss: 0.0006031430728886867\n",
      "\n",
      "Epoch 89/500\n",
      "Epoch 89, Batch 41/41, Batch loss: 0.0011858082143589854\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 89 completed in 0.47 seconds, Total Training Loss: 0.0007507008792855209\n",
      "\n",
      "Epoch 90/500\n",
      "Epoch 90, Batch 41/41, Batch loss: 0.0012781389523297548\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 90 completed in 0.45 seconds, Total Training Loss: 0.0005976027275276621\n",
      "\n",
      "Epoch 91/500\n",
      "Epoch 91, Batch 41/41, Batch loss: 0.0011866734130308032\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 91 completed in 0.43 seconds, Total Training Loss: 0.0007348745140650241\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0019572551595047117\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 92/500\n",
      "Epoch 92, Batch 41/41, Batch loss: 0.0012938837753608823\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 92 completed in 0.43 seconds, Total Training Loss: 0.0006037971142199594\n",
      "\n",
      "Epoch 93/500\n",
      "Epoch 93, Batch 41/41, Batch loss: 0.0011844170512631536\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 93 completed in 0.45 seconds, Total Training Loss: 0.0007476078051074249\n",
      "\n",
      "Epoch 94/500\n",
      "Epoch 94, Batch 41/41, Batch loss: 0.0012901314767077565\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 94 completed in 0.49 seconds, Total Training Loss: 0.0006011161855961446\n",
      "\n",
      "Epoch 95/500\n",
      "Epoch 95, Batch 41/41, Batch loss: 0.0011825606925413013\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 95 completed in 0.43 seconds, Total Training Loss: 0.0007317975776059916\n",
      "\n",
      "Epoch 96/500\n",
      "Epoch 96, Batch 41/41, Batch loss: 0.0012869051424786448\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 96 completed in 0.43 seconds, Total Training Loss: 0.0005994625281323906\n",
      "\n",
      "Epoch 97/500\n",
      "Epoch 97, Batch 41/41, Batch loss: 0.0011849486036226153\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 97 completed in 0.43 seconds, Total Training Loss: 0.0007390160803546811\n",
      "\n",
      "Epoch 98/500\n",
      "Epoch 98, Batch 41/41, Batch loss: 0.0013037417083978653\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 98 completed in 0.44 seconds, Total Training Loss: 0.0006042610150804493\n",
      "\n",
      "Epoch 99/500\n",
      "Epoch 99, Batch 41/41, Batch loss: 0.0011928407475352287\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 99 completed in 0.44 seconds, Total Training Loss: 0.0007512988874024325\n",
      "\n",
      "Epoch 100/500\n",
      "Epoch 100, Batch 41/41, Batch loss: 0.0013008988462388515\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 100 completed in 0.42 seconds, Total Training Loss: 0.0006083677513610072\n",
      "\n",
      "Epoch 101/500\n",
      "Epoch 101, Batch 41/41, Batch loss: 0.0011804018868133426\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 101 completed in 0.45 seconds, Total Training Loss: 0.0007678689289605245\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0019493062514811753\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 102/500\n",
      "Epoch 102, Batch 41/41, Batch loss: 0.0013022538041695952\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 102 completed in 0.46 seconds, Total Training Loss: 0.0005987770617627198\n",
      "\n",
      "Epoch 103/500\n",
      "Epoch 103, Batch 41/41, Batch loss: 0.001180639723315835\n",
      "Memory Usage: 559.44 MB\n",
      "Epoch 103 completed in 0.45 seconds, Total Training Loss: 0.0007419820510066773\n",
      "\n",
      "Epoch 104/500\n",
      "Epoch 104, Batch 41/41, Batch loss: 0.0012655960163101554\n",
      "Memory Usage: 560.44 MB\n",
      "Epoch 104 completed in 0.47 seconds, Total Training Loss: 0.0005847468465373557\n",
      "\n",
      "Epoch 105/500\n",
      "Epoch 105, Batch 41/41, Batch loss: 0.001170111820101738\n",
      "Memory Usage: 560.44 MB\n",
      "Epoch 105 completed in 0.44 seconds, Total Training Loss: 0.0007029758392202827\n",
      "\n",
      "Epoch 106/500\n",
      "Epoch 106, Batch 41/41, Batch loss: 0.0012704848777502775\n",
      "Memory Usage: 560.44 MB\n",
      "Epoch 106 completed in 0.45 seconds, Total Training Loss: 0.0005856983602492752\n",
      "\n",
      "Epoch 107/500\n",
      "Epoch 107, Batch 41/41, Batch loss: 0.0011777490144595504\n",
      "Memory Usage: 560.44 MB\n",
      "Epoch 107 completed in 0.46 seconds, Total Training Loss: 0.0006933062012227843\n",
      "\n",
      "Epoch 108/500\n",
      "Epoch 108, Batch 41/41, Batch loss: 0.0012626213720068336\n",
      "Memory Usage: 560.44 MB\n",
      "Epoch 108 completed in 0.47 seconds, Total Training Loss: 0.0005874668128033191\n",
      "\n",
      "Epoch 109/500\n",
      "Epoch 109, Batch 41/41, Batch loss: 0.001170352566987276\n",
      "Memory Usage: 560.44 MB\n",
      "Epoch 109 completed in 0.44 seconds, Total Training Loss: 0.0006972464162106739\n",
      "\n",
      "Epoch 110/500\n",
      "Epoch 110, Batch 41/41, Batch loss: 0.0012864606687799096\n",
      "Memory Usage: 560.44 MB\n",
      "Epoch 110 completed in 0.42 seconds, Total Training Loss: 0.0005948567951435973\n",
      "\n",
      "Epoch 111/500\n",
      "Epoch 111, Batch 41/41, Batch loss: 0.001179172657430172\n",
      "Memory Usage: 560.44 MB\n",
      "Epoch 111 completed in 0.47 seconds, Total Training Loss: 0.0007256304649295421\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0019513306673616172\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 112/500\n",
      "Epoch 112, Batch 41/41, Batch loss: 0.0012937692226842046\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 112 completed in 0.42 seconds, Total Training Loss: 0.000599992009505868\n",
      "\n",
      "Epoch 113/500\n",
      "Epoch 113, Batch 41/41, Batch loss: 0.0011774813756346703\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 113 completed in 0.45 seconds, Total Training Loss: 0.0007660640183574997\n",
      "\n",
      "Epoch 114/500\n",
      "Epoch 114, Batch 41/41, Batch loss: 0.0013080730568617582\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 114 completed in 0.45 seconds, Total Training Loss: 0.0006042406143600735\n",
      "\n",
      "Epoch 115/500\n",
      "Epoch 115, Batch 41/41, Batch loss: 0.0011758344480767846\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 115 completed in 0.44 seconds, Total Training Loss: 0.0007712649885655903\n",
      "\n",
      "Epoch 116/500\n",
      "Epoch 116, Batch 41/41, Batch loss: 0.0012636252213269472\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 116 completed in 0.41 seconds, Total Training Loss: 0.0005820560889821159\n",
      "\n",
      "Epoch 117/500\n",
      "Epoch 117, Batch 41/41, Batch loss: 0.0011614247923716903\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 117 completed in 0.44 seconds, Total Training Loss: 0.0007189002896732901\n",
      "\n",
      "Epoch 118/500\n",
      "Epoch 118, Batch 41/41, Batch loss: 0.001246039755642414\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 118 completed in 0.44 seconds, Total Training Loss: 0.0005705721971647041\n",
      "\n",
      "Epoch 119/500\n",
      "Epoch 119, Batch 41/41, Batch loss: 0.0011585563188418746\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 119 completed in 0.46 seconds, Total Training Loss: 0.0006887246698436396\n",
      "\n",
      "Epoch 120/500\n",
      "Epoch 120, Batch 41/41, Batch loss: 0.0012415731325745583\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 120 completed in 0.46 seconds, Total Training Loss: 0.0005709350804043025\n",
      "\n",
      "Epoch 121/500\n",
      "Epoch 121, Batch 41/41, Batch loss: 0.001159414416179061\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 121 completed in 0.43 seconds, Total Training Loss: 0.0006732324425062937\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0018903622403740883\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 122/500\n",
      "Epoch 122, Batch 41/41, Batch loss: 0.001251384848728776\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 122 completed in 0.43 seconds, Total Training Loss: 0.0005750634298282221\n",
      "\n",
      "Epoch 123/500\n",
      "Epoch 123, Batch 41/41, Batch loss: 0.001164600602351129\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 123 completed in 0.47 seconds, Total Training Loss: 0.0006934938932450988\n",
      "\n",
      "Epoch 124/500\n",
      "Epoch 124, Batch 41/41, Batch loss: 0.0012604816583916545\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 124 completed in 0.47 seconds, Total Training Loss: 0.0005763295204549028\n",
      "\n",
      "Epoch 125/500\n",
      "Epoch 125, Batch 41/41, Batch loss: 0.0011670440435409546\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 125 completed in 0.44 seconds, Total Training Loss: 0.0007083234292023429\n",
      "\n",
      "Epoch 126/500\n",
      "Epoch 126, Batch 41/41, Batch loss: 0.0012586582452058792\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 126 completed in 0.45 seconds, Total Training Loss: 0.0005783677316219659\n",
      "\n",
      "Epoch 127/500\n",
      "Epoch 127, Batch 41/41, Batch loss: 0.001171013806015253\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 127 completed in 0.45 seconds, Total Training Loss: 0.0007254556875545304\n",
      "\n",
      "Epoch 128/500\n",
      "Epoch 128, Batch 41/41, Batch loss: 0.0012741164537146688\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 128 completed in 0.43 seconds, Total Training Loss: 0.0005842779712288686\n",
      "\n",
      "Epoch 129/500\n",
      "Epoch 129, Batch 41/41, Batch loss: 0.0011631367960944772\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 129 completed in 0.41 seconds, Total Training Loss: 0.0007344382629082425\n",
      "\n",
      "Epoch 130/500\n",
      "Epoch 130, Batch 41/41, Batch loss: 0.0012641728390008211\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 130 completed in 0.41 seconds, Total Training Loss: 0.0005780084194578571\n",
      "\n",
      "Epoch 131/500\n",
      "Epoch 131, Batch 41/41, Batch loss: 0.0011635198025032878\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 131 completed in 0.45 seconds, Total Training Loss: 0.0007309352190421167\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0019110650289803743\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 132/500\n",
      "Epoch 132, Batch 41/41, Batch loss: 0.0012403911678120494\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 132 completed in 0.47 seconds, Total Training Loss: 0.0005679020202766945\n",
      "\n",
      "Epoch 133/500\n",
      "Epoch 133, Batch 41/41, Batch loss: 0.0011422043899074197\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 133 completed in 0.41 seconds, Total Training Loss: 0.0006972519910139055\n",
      "\n",
      "Epoch 134/500\n",
      "Epoch 134, Batch 41/41, Batch loss: 0.0012067781062796712\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 134 completed in 0.41 seconds, Total Training Loss: 0.0005519104767988277\n",
      "\n",
      "Epoch 135/500\n",
      "Epoch 135, Batch 41/41, Batch loss: 0.0011367176193743944\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 135 completed in 0.40 seconds, Total Training Loss: 0.0006525674164039651\n",
      "\n",
      "Epoch 136/500\n",
      "Epoch 136, Batch 41/41, Batch loss: 0.0012009465135633945\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 136 completed in 0.41 seconds, Total Training Loss: 0.0005486690050286309\n",
      "\n",
      "Epoch 137/500\n",
      "Epoch 137, Batch 41/41, Batch loss: 0.0011288018431514502\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 137 completed in 0.46 seconds, Total Training Loss: 0.0006392097690500455\n",
      "\n",
      "Epoch 138/500\n",
      "Epoch 138, Batch 41/41, Batch loss: 0.0012029616627842188\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 138 completed in 0.44 seconds, Total Training Loss: 0.0005522534297708217\n",
      "\n",
      "Epoch 139/500\n",
      "Epoch 139, Batch 41/41, Batch loss: 0.0011332305148243904\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 139 completed in 0.41 seconds, Total Training Loss: 0.0006385066151823394\n",
      "\n",
      "Epoch 140/500\n",
      "Epoch 140, Batch 41/41, Batch loss: 0.0012075999984517694\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 140 completed in 0.45 seconds, Total Training Loss: 0.0005542540820897557\n",
      "\n",
      "Epoch 141/500\n",
      "Epoch 141, Batch 41/41, Batch loss: 0.001133098267018795\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 141 completed in 0.44 seconds, Total Training Loss: 0.0006517449300878159\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0018465294037014247\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 142/500\n",
      "Epoch 142, Batch 41/41, Batch loss: 0.0012234575115144253\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 142 completed in 0.43 seconds, Total Training Loss: 0.0005629890787886547\n",
      "\n",
      "Epoch 143/500\n",
      "Epoch 143, Batch 41/41, Batch loss: 0.0011305664665997028\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 143 completed in 0.45 seconds, Total Training Loss: 0.0006843940574937024\n",
      "\n",
      "Epoch 144/500\n",
      "Epoch 144, Batch 41/41, Batch loss: 0.0012403642758727074\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 144 completed in 0.44 seconds, Total Training Loss: 0.0005695036274917628\n",
      "\n",
      "Epoch 145/500\n",
      "Epoch 145, Batch 41/41, Batch loss: 0.0011317833559587598\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 145 completed in 0.45 seconds, Total Training Loss: 0.0007164651185853361\n",
      "\n",
      "Epoch 146/500\n",
      "Epoch 146, Batch 41/41, Batch loss: 0.0012316809734329581\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 146 completed in 0.45 seconds, Total Training Loss: 0.000566337135352092\n",
      "\n",
      "Epoch 147/500\n",
      "Epoch 147, Batch 41/41, Batch loss: 0.0011190411169081926\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 147 completed in 0.46 seconds, Total Training Loss: 0.0007094884164938022\n",
      "\n",
      "Epoch 148/500\n",
      "Epoch 148, Batch 41/41, Batch loss: 0.001203270978294313\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 148 completed in 0.44 seconds, Total Training Loss: 0.0005466547694728743\n",
      "\n",
      "Epoch 149/500\n",
      "Epoch 149, Batch 41/41, Batch loss: 0.0011073318310081959\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 149 completed in 0.44 seconds, Total Training Loss: 0.0006776729913814584\n",
      "\n",
      "Epoch 150/500\n",
      "Epoch 150, Batch 41/41, Batch loss: 0.0011739361798390746\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 150 completed in 0.45 seconds, Total Training Loss: 0.0005304679668880431\n",
      "\n",
      "Epoch 151/500\n",
      "Epoch 151, Batch 41/41, Batch loss: 0.0010904243681579828\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 151 completed in 0.45 seconds, Total Training Loss: 0.0006142453366475998\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0017425303114578128\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 152/500\n",
      "Epoch 152, Batch 41/41, Batch loss: 0.0011512466007843614\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 152 completed in 0.47 seconds, Total Training Loss: 0.0005249361248144585\n",
      "\n",
      "Epoch 153/500\n",
      "Epoch 153, Batch 41/41, Batch loss: 0.0010742554441094398\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 153 completed in 0.45 seconds, Total Training Loss: 0.0005898846536250104\n",
      "\n",
      "Epoch 154/500\n",
      "Epoch 154, Batch 41/41, Batch loss: 0.0011222768807783723\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 154 completed in 0.45 seconds, Total Training Loss: 0.000521493066249915\n",
      "\n",
      "Epoch 155/500\n",
      "Epoch 155, Batch 41/41, Batch loss: 0.001051690662279725\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 155 completed in 0.46 seconds, Total Training Loss: 0.0005707598877673196\n",
      "\n",
      "Epoch 156/500\n",
      "Epoch 156, Batch 41/41, Batch loss: 0.0011043010745197535\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 156 completed in 0.47 seconds, Total Training Loss: 0.0005162597659452841\n",
      "\n",
      "Epoch 157/500\n",
      "Epoch 157, Batch 41/41, Batch loss: 0.0010326573392376304\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 157 completed in 0.48 seconds, Total Training Loss: 0.0005669032277471227\n",
      "\n",
      "Epoch 158/500\n",
      "Epoch 158, Batch 41/41, Batch loss: 0.0010892195859923959\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 158 completed in 0.46 seconds, Total Training Loss: 0.0005137517719271184\n",
      "\n",
      "Epoch 159/500\n",
      "Epoch 159, Batch 41/41, Batch loss: 0.0010254887165501714\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 159 completed in 0.47 seconds, Total Training Loss: 0.0005952872695449581\n",
      "\n",
      "Epoch 160/500\n",
      "Epoch 160, Batch 41/41, Batch loss: 0.0011363201774656773\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 160 completed in 0.46 seconds, Total Training Loss: 0.0005487487922925162\n",
      "\n",
      "Epoch 161/500\n",
      "Epoch 161, Batch 41/41, Batch loss: 0.0010341780725866556\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 161 completed in 0.46 seconds, Total Training Loss: 0.0006820291430524709\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0017916421638801694\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 162/500\n",
      "Epoch 162, Batch 41/41, Batch loss: 0.0012084945337846875\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 162 completed in 0.45 seconds, Total Training Loss: 0.000586668325612242\n",
      "\n",
      "Epoch 163/500\n",
      "Epoch 163, Batch 41/41, Batch loss: 0.0010471980785951018\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 163 completed in 0.48 seconds, Total Training Loss: 0.0007735113783671362\n",
      "\n",
      "Epoch 164/500\n",
      "Epoch 164, Batch 41/41, Batch loss: 0.0011545231100171804\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 164 completed in 0.44 seconds, Total Training Loss: 0.0005521277079668191\n",
      "\n",
      "Epoch 165/500\n",
      "Epoch 165, Batch 41/41, Batch loss: 0.0010207431623712182\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 165 completed in 0.43 seconds, Total Training Loss: 0.0006886292209922586\n",
      "\n",
      "Epoch 166/500\n",
      "Epoch 166, Batch 41/41, Batch loss: 0.0010502940276637673\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 166 completed in 0.46 seconds, Total Training Loss: 0.0004886477589735012\n",
      "\n",
      "Epoch 167/500\n",
      "Epoch 167, Batch 41/41, Batch loss: 0.0009939331794157624\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 167 completed in 0.47 seconds, Total Training Loss: 0.0005641783652940719\n",
      "\n",
      "Epoch 168/500\n",
      "Epoch 168, Batch 41/41, Batch loss: 0.001038250862620771\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 168 completed in 0.46 seconds, Total Training Loss: 0.0004763752692336466\n",
      "\n",
      "Epoch 169/500\n",
      "Epoch 169, Batch 41/41, Batch loss: 0.0009972094558179379\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 169 completed in 0.46 seconds, Total Training Loss: 0.0005451046988416304\n",
      "\n",
      "Epoch 170/500\n",
      "Epoch 170, Batch 41/41, Batch loss: 0.0010412991978228092\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 170 completed in 0.44 seconds, Total Training Loss: 0.000480822245263858\n",
      "\n",
      "Epoch 171/500\n",
      "Epoch 171, Batch 41/41, Batch loss: 0.0009814907098188996\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 171 completed in 0.46 seconds, Total Training Loss: 0.0005250078433962743\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.001543120644055307\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 172/500\n",
      "Epoch 172, Batch 41/41, Batch loss: 0.0010331801604479551\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 172 completed in 0.43 seconds, Total Training Loss: 0.00047591568816243105\n",
      "\n",
      "Epoch 173/500\n",
      "Epoch 173, Batch 41/41, Batch loss: 0.0009926734492182732\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 173 completed in 0.46 seconds, Total Training Loss: 0.0005486084396025257\n",
      "\n",
      "Epoch 174/500\n",
      "Epoch 174, Batch 41/41, Batch loss: 0.0010643040295690298\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 174 completed in 0.44 seconds, Total Training Loss: 0.0004933380068240051\n",
      "\n",
      "Epoch 175/500\n",
      "Epoch 175, Batch 41/41, Batch loss: 0.0009741390822455287\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 175 completed in 0.46 seconds, Total Training Loss: 0.0005441288511331262\n",
      "\n",
      "Epoch 176/500\n",
      "Epoch 176, Batch 41/41, Batch loss: 0.001031927764415741\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 176 completed in 0.43 seconds, Total Training Loss: 0.0004971964922222513\n",
      "\n",
      "Epoch 177/500\n",
      "Epoch 177, Batch 41/41, Batch loss: 0.0009906620252877474\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 177 completed in 0.42 seconds, Total Training Loss: 0.0005782166939220255\n",
      "\n",
      "Epoch 178/500\n",
      "Epoch 178, Batch 41/41, Batch loss: 0.0010759473079815507\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 178 completed in 0.44 seconds, Total Training Loss: 0.0005178717241861018\n",
      "\n",
      "Epoch 179/500\n",
      "Epoch 179, Batch 41/41, Batch loss: 0.0009694669861346483\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 179 completed in 0.42 seconds, Total Training Loss: 0.0006141171825442054\n",
      "\n",
      "Epoch 180/500\n",
      "Epoch 180, Batch 41/41, Batch loss: 0.0010767689673230052\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 180 completed in 0.44 seconds, Total Training Loss: 0.0005510998657328168\n",
      "\n",
      "Epoch 181/500\n",
      "Epoch 181, Batch 41/41, Batch loss: 0.0009929978987202048\n",
      "Memory Usage: 561.44 MB\n",
      "Epoch 181 completed in 0.42 seconds, Total Training Loss: 0.0006550977456599201\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0016591123538091778\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 182/500\n",
      "Epoch 182, Batch 41/41, Batch loss: 0.0010903255315497518\n",
      "Memory Usage: 561.94 MB\n",
      "Epoch 182 completed in 0.48 seconds, Total Training Loss: 0.0005342399208308141\n",
      "\n",
      "Epoch 183/500\n",
      "Epoch 183, Batch 41/41, Batch loss: 0.0009655500180087984\n",
      "Memory Usage: 561.94 MB\n",
      "Epoch 183 completed in 0.45 seconds, Total Training Loss: 0.0007191934465237598\n",
      "\n",
      "Epoch 184/500\n",
      "Epoch 184, Batch 41/41, Batch loss: 0.0010804166086018085\n",
      "Memory Usage: 561.94 MB\n",
      "Epoch 184 completed in 0.45 seconds, Total Training Loss: 0.0005163705630743274\n",
      "\n",
      "Epoch 185/500\n",
      "Epoch 185, Batch 41/41, Batch loss: 0.000977837247774005\n",
      "Memory Usage: 561.94 MB\n",
      "Epoch 185 completed in 0.45 seconds, Total Training Loss: 0.0005649347177695301\n",
      "\n",
      "Epoch 186/500\n",
      "Epoch 186, Batch 41/41, Batch loss: 0.0010481425561010838\n",
      "Memory Usage: 561.94 MB\n",
      "Epoch 186 completed in 0.46 seconds, Total Training Loss: 0.0004903349479541137\n",
      "\n",
      "Epoch 187/500\n",
      "Epoch 187, Batch 41/41, Batch loss: 0.0009547639638185501\n",
      "Memory Usage: 561.94 MB\n",
      "Epoch 187 completed in 0.48 seconds, Total Training Loss: 0.0006440559921363118\n",
      "\n",
      "Epoch 188/500\n",
      "Epoch 188, Batch 41/41, Batch loss: 0.0010819276794791222\n",
      "Memory Usage: 561.94 MB\n",
      "Epoch 188 completed in 0.45 seconds, Total Training Loss: 0.0005079675146389934\n",
      "\n",
      "Epoch 189/500\n",
      "Epoch 189, Batch 41/41, Batch loss: 0.0010075847385451198\n",
      "Memory Usage: 561.94 MB\n",
      "Epoch 189 completed in 0.46 seconds, Total Training Loss: 0.0005415252907834834\n",
      "\n",
      "Epoch 190/500\n",
      "Epoch 190, Batch 41/41, Batch loss: 0.0010292092338204384\n",
      "Memory Usage: 561.94 MB\n",
      "Epoch 190 completed in 0.44 seconds, Total Training Loss: 0.000520862550929007\n",
      "\n",
      "Epoch 191/500\n",
      "Epoch 191, Batch 41/41, Batch loss: 0.000981617602519691\n",
      "Memory Usage: 561.94 MB\n",
      "Epoch 191 completed in 0.48 seconds, Total Training Loss: 0.0007135062332963571\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.001691739703528583\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 192/500\n",
      "Epoch 192, Batch 41/41, Batch loss: 0.0010230392217636108\n",
      "Memory Usage: 562.97 MB\n",
      "Epoch 192 completed in 0.45 seconds, Total Training Loss: 0.0005015182554105683\n",
      "\n",
      "Epoch 193/500\n",
      "Epoch 193, Batch 41/41, Batch loss: 0.0010463560465723276\n",
      "Memory Usage: 562.97 MB\n",
      "Epoch 193 completed in 0.44 seconds, Total Training Loss: 0.000604539612739733\n",
      "\n",
      "Epoch 194/500\n",
      "Epoch 194, Batch 41/41, Batch loss: 0.0011026697466149926\n",
      "Memory Usage: 562.97 MB\n",
      "Epoch 194 completed in 0.45 seconds, Total Training Loss: 0.000544954347566179\n",
      "\n",
      "Epoch 195/500\n",
      "Epoch 195, Batch 41/41, Batch loss: 0.000951732974499464\n",
      "Memory Usage: 562.97 MB\n",
      "Epoch 195 completed in 0.44 seconds, Total Training Loss: 0.0005033936217731069\n",
      "\n",
      "Epoch 196/500\n",
      "Epoch 196, Batch 41/41, Batch loss: 0.0009730580495670438\n",
      "Memory Usage: 562.97 MB\n",
      "Epoch 196 completed in 0.40 seconds, Total Training Loss: 0.00048159302436610364\n",
      "\n",
      "Epoch 197/500\n",
      "Epoch 197, Batch 41/41, Batch loss: 0.0009500245214439929\n",
      "Memory Usage: 562.97 MB\n",
      "Epoch 197 completed in 0.45 seconds, Total Training Loss: 0.0006005435822838236\n",
      "\n",
      "Epoch 198/500\n",
      "Epoch 198, Batch 41/41, Batch loss: 0.0009832021314650774\n",
      "Memory Usage: 562.97 MB\n",
      "Epoch 198 completed in 0.44 seconds, Total Training Loss: 0.0004669486024351677\n",
      "\n",
      "Epoch 199/500\n",
      "Epoch 199, Batch 41/41, Batch loss: 0.0009762357803992927\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 199 completed in 0.40 seconds, Total Training Loss: 0.0005306573849094737\n",
      "\n",
      "Epoch 200/500\n",
      "Epoch 200, Batch 41/41, Batch loss: 0.0009852906223386526\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 200 completed in 0.42 seconds, Total Training Loss: 0.0005082714867568007\n",
      "\n",
      "Epoch 201/500\n",
      "Epoch 201, Batch 41/41, Batch loss: 0.0009391607600264251\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 201 completed in 0.44 seconds, Total Training Loss: 0.0004864288976688574\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0015065066749230028\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 202/500\n",
      "Epoch 202, Batch 41/41, Batch loss: 0.0009484574547968805\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 202 completed in 0.48 seconds, Total Training Loss: 0.00045671403708316883\n",
      "\n",
      "Epoch 203/500\n",
      "Epoch 203, Batch 41/41, Batch loss: 0.0009483526227995753\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 203 completed in 0.48 seconds, Total Training Loss: 0.0005630210690447357\n",
      "\n",
      "Epoch 204/500\n",
      "Epoch 204, Batch 41/41, Batch loss: 0.0010366290807724\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 204 completed in 0.48 seconds, Total Training Loss: 0.0004939423306495883\n",
      "\n",
      "Epoch 205/500\n",
      "Epoch 205, Batch 41/41, Batch loss: 0.0009301321115344763\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 205 completed in 0.45 seconds, Total Training Loss: 0.0004932734760010001\n",
      "\n",
      "Epoch 206/500\n",
      "Epoch 206, Batch 41/41, Batch loss: 0.0009558089659549296\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 206 completed in 0.48 seconds, Total Training Loss: 0.0004703599644438704\n",
      "\n",
      "Epoch 207/500\n",
      "Epoch 207, Batch 41/41, Batch loss: 0.0009283111430704594\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 207 completed in 0.49 seconds, Total Training Loss: 0.0005895166736658345\n",
      "\n",
      "Epoch 208/500\n",
      "Epoch 208, Batch 41/41, Batch loss: 0.0010609113378450274\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 208 completed in 0.47 seconds, Total Training Loss: 0.0005031554008870976\n",
      "\n",
      "Epoch 209/500\n",
      "Epoch 209, Batch 41/41, Batch loss: 0.0009667844860814512\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 209 completed in 0.48 seconds, Total Training Loss: 0.0005575474002043625\n",
      "\n",
      "Epoch 210/500\n",
      "Epoch 210, Batch 41/41, Batch loss: 0.0010100635699927807\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 210 completed in 0.47 seconds, Total Training Loss: 0.0004903569694384137\n",
      "\n",
      "Epoch 211/500\n",
      "Epoch 211, Batch 41/41, Batch loss: 0.0009213738958351314\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 211 completed in 0.48 seconds, Total Training Loss: 0.0006331428842868929\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0016553518595173954\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 212/500\n",
      "Epoch 212, Batch 41/41, Batch loss: 0.001067730481736362\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 212 completed in 0.48 seconds, Total Training Loss: 0.0005332602803075755\n",
      "\n",
      "Epoch 213/500\n",
      "Epoch 213, Batch 41/41, Batch loss: 0.001028164871968329\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 213 completed in 0.42 seconds, Total Training Loss: 0.00063055584302297\n",
      "\n",
      "Epoch 214/500\n",
      "Epoch 214, Batch 41/41, Batch loss: 0.001043612719513476\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 214 completed in 0.44 seconds, Total Training Loss: 0.0005006377644504694\n",
      "\n",
      "Epoch 215/500\n",
      "Epoch 215, Batch 41/41, Batch loss: 0.0009077801951207221\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 215 completed in 0.46 seconds, Total Training Loss: 0.0005880792173784135\n",
      "\n",
      "Epoch 216/500\n",
      "Epoch 216, Batch 41/41, Batch loss: 0.000972852052655071\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 216 completed in 0.46 seconds, Total Training Loss: 0.0004876166722835212\n",
      "\n",
      "Epoch 217/500\n",
      "Epoch 217, Batch 41/41, Batch loss: 0.0009850553469732404\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 217 completed in 0.43 seconds, Total Training Loss: 0.0006715854020823338\n",
      "\n",
      "Epoch 218/500\n",
      "Epoch 218, Batch 41/41, Batch loss: 0.0011071284534409642\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 218 completed in 0.42 seconds, Total Training Loss: 0.0005093508328328182\n",
      "\n",
      "Epoch 219/500\n",
      "Epoch 219, Batch 41/41, Batch loss: 0.0009186909883283079\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 219 completed in 0.42 seconds, Total Training Loss: 0.00048534673100575896\n",
      "\n",
      "Epoch 220/500\n",
      "Epoch 220, Batch 41/41, Batch loss: 0.0009477261337451637\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 220 completed in 0.45 seconds, Total Training Loss: 0.0004788330981859983\n",
      "\n",
      "Epoch 221/500\n",
      "Epoch 221, Batch 41/41, Batch loss: 0.0009383580181747675\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 221 completed in 0.44 seconds, Total Training Loss: 0.0006284613973992059\n",
      "Validation completed in 0.01 seconds, Average Validation Loss: 0.0015720053110271693\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 222/500\n",
      "Epoch 222, Batch 41/41, Batch loss: 0.0009338955860584974\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 222 completed in 0.45 seconds, Total Training Loss: 0.0004475386035997739\n",
      "\n",
      "Epoch 223/500\n",
      "Epoch 223, Batch 41/41, Batch loss: 0.0009621633216738701\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 223 completed in 0.41 seconds, Total Training Loss: 0.0005261040549645791\n",
      "\n",
      "Epoch 224/500\n",
      "Epoch 224, Batch 41/41, Batch loss: 0.000977097894065082\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 224 completed in 0.42 seconds, Total Training Loss: 0.0005078761023469269\n",
      "\n",
      "Epoch 225/500\n",
      "Epoch 225, Batch 41/41, Batch loss: 0.0009023635066114366\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 225 completed in 0.44 seconds, Total Training Loss: 0.00045732576038018326\n",
      "\n",
      "Epoch 226/500\n",
      "Epoch 226, Batch 41/41, Batch loss: 0.0009149050456471741\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 226 completed in 0.43 seconds, Total Training Loss: 0.00045565391887865235\n",
      "\n",
      "Epoch 227/500\n",
      "Epoch 227, Batch 41/41, Batch loss: 0.0008929757750593126\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 227 completed in 0.44 seconds, Total Training Loss: 0.0005396354056911816\n",
      "\n",
      "Epoch 228/500\n",
      "Epoch 228, Batch 41/41, Batch loss: 0.0009262131643481553\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 228 completed in 0.44 seconds, Total Training Loss: 0.0004400686210407535\n",
      "\n",
      "Epoch 229/500\n",
      "Epoch 229, Batch 41/41, Batch loss: 0.0009057666757144034\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 229 completed in 0.43 seconds, Total Training Loss: 0.0004583739570168801\n",
      "\n",
      "Epoch 230/500\n",
      "Epoch 230, Batch 41/41, Batch loss: 0.0008981985156424344\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 230 completed in 0.45 seconds, Total Training Loss: 0.00047647560273706005\n",
      "\n",
      "Epoch 231/500\n",
      "Epoch 231, Batch 41/41, Batch loss: 0.0008872079779393971\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 231 completed in 0.44 seconds, Total Training Loss: 0.0004929034193770438\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0014642819296568632\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 232/500\n",
      "Epoch 232, Batch 41/41, Batch loss: 0.0008884354610927403\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 232 completed in 0.44 seconds, Total Training Loss: 0.00043429817316525547\n",
      "\n",
      "Epoch 233/500\n",
      "Epoch 233, Batch 41/41, Batch loss: 0.0009108784724958241\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 233 completed in 0.42 seconds, Total Training Loss: 0.0004888628967952483\n",
      "\n",
      "Epoch 234/500\n",
      "Epoch 234, Batch 41/41, Batch loss: 0.0008902232511900365\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 234 completed in 0.44 seconds, Total Training Loss: 0.0004941523786727907\n",
      "\n",
      "Epoch 235/500\n",
      "Epoch 235, Batch 41/41, Batch loss: 0.000880519044585526\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 235 completed in 0.44 seconds, Total Training Loss: 0.0004512914775859383\n",
      "\n",
      "Epoch 236/500\n",
      "Epoch 236, Batch 41/41, Batch loss: 0.0008814586326479912\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 236 completed in 0.44 seconds, Total Training Loss: 0.0004328639101359153\n",
      "\n",
      "Epoch 237/500\n",
      "Epoch 237, Batch 41/41, Batch loss: 0.0009137876331806183\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 237 completed in 0.44 seconds, Total Training Loss: 0.0005165639721359717\n",
      "\n",
      "Epoch 238/500\n",
      "Epoch 238, Batch 41/41, Batch loss: 0.0009071401436813176\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 238 completed in 0.46 seconds, Total Training Loss: 0.0005013266283374211\n",
      "\n",
      "Epoch 239/500\n",
      "Epoch 239, Batch 41/41, Batch loss: 0.0008705712389200926\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 239 completed in 0.45 seconds, Total Training Loss: 0.0004428186849265064\n",
      "\n",
      "Epoch 240/500\n",
      "Epoch 240, Batch 41/41, Batch loss: 0.0008849541191011667\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 240 completed in 0.47 seconds, Total Training Loss: 0.0004375900605781481\n",
      "\n",
      "Epoch 241/500\n",
      "Epoch 241, Batch 41/41, Batch loss: 0.0009261794039048254\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 241 completed in 0.46 seconds, Total Training Loss: 0.0005641976547450191\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0016104083508253098\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 242/500\n",
      "Epoch 242, Batch 41/41, Batch loss: 0.0009708205470815301\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 242 completed in 0.46 seconds, Total Training Loss: 0.0005066150915808976\n",
      "\n",
      "Epoch 243/500\n",
      "Epoch 243, Batch 41/41, Batch loss: 0.0008625588961876929\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 243 completed in 0.46 seconds, Total Training Loss: 0.00045057622947951596\n",
      "\n",
      "Epoch 244/500\n",
      "Epoch 244, Batch 41/41, Batch loss: 0.0008957334794104099\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 244 completed in 0.45 seconds, Total Training Loss: 0.00044151606739544104\n",
      "\n",
      "Epoch 245/500\n",
      "Epoch 245, Batch 41/41, Batch loss: 0.0009400973794981837\n",
      "Memory Usage: 563.47 MB\n",
      "Epoch 245 completed in 0.46 seconds, Total Training Loss: 0.0006560783188367012\n",
      "\n",
      "Epoch 246/500\n",
      "Epoch 246, Batch 41/41, Batch loss: 0.0011586243053898215\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 246 completed in 0.44 seconds, Total Training Loss: 0.0005546135972708282\n",
      "\n",
      "Epoch 247/500\n",
      "Epoch 247, Batch 41/41, Batch loss: 0.0009320002282038331\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 247 completed in 0.45 seconds, Total Training Loss: 0.0005835663852361371\n",
      "\n",
      "Epoch 248/500\n",
      "Epoch 248, Batch 41/41, Batch loss: 0.0009506269707344472\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 248 completed in 0.48 seconds, Total Training Loss: 0.0005033982956885621\n",
      "\n",
      "Epoch 249/500\n",
      "Epoch 249, Batch 41/41, Batch loss: 0.0009134527062997222\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 249 completed in 0.48 seconds, Total Training Loss: 0.0008092652573307022\n",
      "\n",
      "Epoch 250/500\n",
      "Epoch 250, Batch 41/41, Batch loss: 0.0009046871564351022\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 250 completed in 0.47 seconds, Total Training Loss: 0.00045251346960956254\n",
      "\n",
      "Epoch 251/500\n",
      "Epoch 251, Batch 41/41, Batch loss: 0.000961380370426923\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 251 completed in 0.49 seconds, Total Training Loss: 0.00046660452133481865\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0016308686463162303\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 252/500\n",
      "Epoch 252, Batch 41/41, Batch loss: 0.0009049197542481124\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 252 completed in 0.46 seconds, Total Training Loss: 0.0005331425654287337\n",
      "\n",
      "Epoch 253/500\n",
      "Epoch 253, Batch 41/41, Batch loss: 0.0008706224616616964\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 253 completed in 0.43 seconds, Total Training Loss: 0.00045648231309492187\n",
      "\n",
      "Epoch 254/500\n",
      "Epoch 254, Batch 41/41, Batch loss: 0.0008724872604943812\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 254 completed in 0.44 seconds, Total Training Loss: 0.0004255998861569934\n",
      "\n",
      "Epoch 255/500\n",
      "Epoch 255, Batch 41/41, Batch loss: 0.0008798010530881584\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 255 completed in 0.44 seconds, Total Training Loss: 0.000580511765381344\n",
      "\n",
      "Epoch 256/500\n",
      "Epoch 256, Batch 41/41, Batch loss: 0.0009627771214582026\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 256 completed in 0.46 seconds, Total Training Loss: 0.00044804515062246426\n",
      "\n",
      "Epoch 257/500\n",
      "Epoch 257, Batch 41/41, Batch loss: 0.0008636745042167604\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 257 completed in 0.43 seconds, Total Training Loss: 0.0004298249829245345\n",
      "\n",
      "Epoch 258/500\n",
      "Epoch 258, Batch 41/41, Batch loss: 0.0008889613673090935\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 258 completed in 0.45 seconds, Total Training Loss: 0.00047862207039299124\n",
      "\n",
      "Epoch 259/500\n",
      "Epoch 259, Batch 41/41, Batch loss: 0.0009172993595711887\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 259 completed in 0.44 seconds, Total Training Loss: 0.0005798757079102826\n",
      "\n",
      "Epoch 260/500\n",
      "Epoch 260, Batch 41/41, Batch loss: 0.0008458218071609735\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 260 completed in 0.46 seconds, Total Training Loss: 0.0004180618547607304\n",
      "\n",
      "Epoch 261/500\n",
      "Epoch 261, Batch 41/41, Batch loss: 0.0008909251191653311\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 261 completed in 0.46 seconds, Total Training Loss: 0.0004451270408647294\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0015378679614514112\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 262/500\n",
      "Epoch 262, Batch 41/41, Batch loss: 0.0008580195135436952\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 262 completed in 0.46 seconds, Total Training Loss: 0.0005676504639800774\n",
      "\n",
      "Epoch 263/500\n",
      "Epoch 263, Batch 41/41, Batch loss: 0.0008747079991735518\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 263 completed in 0.44 seconds, Total Training Loss: 0.0004111852226409743\n",
      "\n",
      "Epoch 264/500\n",
      "Epoch 264, Batch 41/41, Batch loss: 0.0008787226397544146\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 264 completed in 0.45 seconds, Total Training Loss: 0.0004424132719400293\n",
      "\n",
      "Epoch 265/500\n",
      "Epoch 265, Batch 41/41, Batch loss: 0.0008427368011325598\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 265 completed in 0.42 seconds, Total Training Loss: 0.0004743781679369532\n",
      "\n",
      "Epoch 266/500\n",
      "Epoch 266, Batch 41/41, Batch loss: 0.0008395914919674397\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 266 completed in 0.43 seconds, Total Training Loss: 0.0004486095475419084\n",
      "\n",
      "Epoch 267/500\n",
      "Epoch 267, Batch 41/41, Batch loss: 0.0008322335197590292\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 267 completed in 0.44 seconds, Total Training Loss: 0.00040724202096121523\n",
      "\n",
      "Epoch 268/500\n",
      "Epoch 268, Batch 41/41, Batch loss: 0.0008663106127642095\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 268 completed in 0.42 seconds, Total Training Loss: 0.0004726925988329529\n",
      "\n",
      "Epoch 269/500\n",
      "Epoch 269, Batch 41/41, Batch loss: 0.0008472788613289595\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 269 completed in 0.41 seconds, Total Training Loss: 0.00047267818675808066\n",
      "\n",
      "Epoch 270/500\n",
      "Epoch 270, Batch 41/41, Batch loss: 0.0008247873629443347\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 270 completed in 0.43 seconds, Total Training Loss: 0.0004126390982802571\n",
      "\n",
      "Epoch 271/500\n",
      "Epoch 271, Batch 41/41, Batch loss: 0.0008392995223402977\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 271 completed in 0.41 seconds, Total Training Loss: 0.00042334925961856753\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0014637963380664588\n",
      "Model improved and saved.\n",
      "\n",
      "Epoch 272/500\n",
      "Epoch 272, Batch 41/41, Batch loss: 0.0008479600655846298\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 272 completed in 0.44 seconds, Total Training Loss: 0.0004999024570612918\n",
      "\n",
      "Epoch 273/500\n",
      "Epoch 273, Batch 41/41, Batch loss: 0.0008542503928765655\n",
      "Memory Usage: 563.97 MB\n",
      "Epoch 273 completed in 0.46 seconds, Total Training Loss: 0.0004478204065845802\n",
      "\n",
      "Epoch 274/500\n",
      "Epoch 274, Batch 41/41, Batch loss: 0.0008182830060832202\n",
      "Memory Usage: 565.48 MB\n",
      "Epoch 274 completed in 0.46 seconds, Total Training Loss: 0.000411409618408936\n",
      "\n",
      "Epoch 275/500\n",
      "Epoch 275, Batch 41/41, Batch loss: 0.0008298639440909028\n",
      "Memory Usage: 565.98 MB\n",
      "Epoch 275 completed in 0.46 seconds, Total Training Loss: 0.00042727068346987547\n",
      "\n",
      "Epoch 276/500\n",
      "Epoch 276, Batch 41/41, Batch loss: 0.0008631617529317737\n",
      "Memory Usage: 565.98 MB\n",
      "Epoch 276 completed in 0.44 seconds, Total Training Loss: 0.000543348712487453\n",
      "\n",
      "Epoch 277/500\n",
      "Epoch 277, Batch 41/41, Batch loss: 0.0008775431197136641\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 277 completed in 0.44 seconds, Total Training Loss: 0.0004488514889267319\n",
      "\n",
      "Epoch 278/500\n",
      "Epoch 278, Batch 41/41, Batch loss: 0.0008265817887149751\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 278 completed in 0.43 seconds, Total Training Loss: 0.0004345131973223477\n",
      "\n",
      "Epoch 279/500\n",
      "Epoch 279, Batch 41/41, Batch loss: 0.0008331490680575371\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 279 completed in 0.46 seconds, Total Training Loss: 0.00042067919754871824\n",
      "\n",
      "Epoch 280/500\n",
      "Epoch 280, Batch 41/41, Batch loss: 0.0008532589417882264\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 280 completed in 0.47 seconds, Total Training Loss: 0.0005512194374005492\n",
      "\n",
      "Epoch 281/500\n",
      "Epoch 281, Batch 41/41, Batch loss: 0.0009230421273969114\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 281 completed in 0.42 seconds, Total Training Loss: 0.00046188199431947775\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0017701396951451897\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 282/500\n",
      "Epoch 282, Batch 41/41, Batch loss: 0.0008324906229972839\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 282 completed in 0.45 seconds, Total Training Loss: 0.00048660837548995025\n",
      "\n",
      "Epoch 283/500\n",
      "Epoch 283, Batch 41/41, Batch loss: 0.0008489207248203456\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 283 completed in 0.41 seconds, Total Training Loss: 0.00042784141774840156\n",
      "\n",
      "Epoch 284/500\n",
      "Epoch 284, Batch 41/41, Batch loss: 0.0008294928120449185\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 284 completed in 0.42 seconds, Total Training Loss: 0.0005736990798784351\n",
      "\n",
      "Epoch 285/500\n",
      "Epoch 285, Batch 41/41, Batch loss: 0.0009127375669777393\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 285 completed in 0.43 seconds, Total Training Loss: 0.0004467436415214296\n",
      "\n",
      "Epoch 286/500\n",
      "Epoch 286, Batch 41/41, Batch loss: 0.0008205010090023279\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 286 completed in 0.43 seconds, Total Training Loss: 0.0004708353412641417\n",
      "\n",
      "Epoch 287/500\n",
      "Epoch 287, Batch 41/41, Batch loss: 0.0008389967260882258\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 287 completed in 0.42 seconds, Total Training Loss: 0.00042374369283008\n",
      "\n",
      "Epoch 288/500\n",
      "Epoch 288, Batch 41/41, Batch loss: 0.0008248956291936338\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 288 completed in 0.43 seconds, Total Training Loss: 0.0006109079296016362\n",
      "\n",
      "Epoch 289/500\n",
      "Epoch 289, Batch 41/41, Batch loss: 0.0009177598403766751\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 289 completed in 0.44 seconds, Total Training Loss: 0.00043400669741591936\n",
      "\n",
      "Epoch 290/500\n",
      "Epoch 290, Batch 41/41, Batch loss: 0.0008138340199366212\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 290 completed in 0.46 seconds, Total Training Loss: 0.0004334283079426713\n",
      "\n",
      "Epoch 291/500\n",
      "Epoch 291, Batch 41/41, Batch loss: 0.0008511581690981984\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 291 completed in 0.42 seconds, Total Training Loss: 0.0004413993787815476\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0016230933368206024\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 292/500\n",
      "Epoch 292, Batch 41/41, Batch loss: 0.0008230061503127217\n",
      "Memory Usage: 567.52 MB\n",
      "Epoch 292 completed in 0.45 seconds, Total Training Loss: 0.0006885426713468893\n",
      "\n",
      "Epoch 293/500\n",
      "Epoch 293, Batch 41/41, Batch loss: 0.0009103170596063137\n",
      "Memory Usage: 515.78 MB\n",
      "Epoch 293 completed in 0.62 seconds, Total Training Loss: 0.00041347048025317033\n",
      "\n",
      "Epoch 294/500\n",
      "Epoch 294, Batch 41/41, Batch loss: 0.00084955949569121\n",
      "Memory Usage: 501.61 MB\n",
      "Epoch 294 completed in 0.41 seconds, Total Training Loss: 0.00043636449153404454\n",
      "\n",
      "Epoch 295/500\n",
      "Epoch 295, Batch 41/41, Batch loss: 0.000832615012768656\n",
      "Memory Usage: 508.27 MB\n",
      "Epoch 295 completed in 0.49 seconds, Total Training Loss: 0.00045373756012584014\n",
      "\n",
      "Epoch 296/500\n",
      "Epoch 296, Batch 41/41, Batch loss: 0.000860789732541889\n",
      "Memory Usage: 499.38 MB\n",
      "Epoch 296 completed in 0.65 seconds, Total Training Loss: 0.0006286774507493758\n",
      "\n",
      "Epoch 297/500\n",
      "Epoch 297, Batch 41/41, Batch loss: 0.0008147922926582396\n",
      "Memory Usage: 476.95 MB\n",
      "Epoch 297 completed in 0.60 seconds, Total Training Loss: 0.0004160287999406048\n",
      "\n",
      "Epoch 298/500\n",
      "Epoch 298, Batch 41/41, Batch loss: 0.000904099375475198\n",
      "Memory Usage: 471.38 MB\n",
      "Epoch 298 completed in 0.48 seconds, Total Training Loss: 0.000447838671190278\n",
      "\n",
      "Epoch 299/500\n",
      "Epoch 299, Batch 41/41, Batch loss: 0.0008323053480125964\n",
      "Memory Usage: 479.41 MB\n",
      "Epoch 299 completed in 0.50 seconds, Total Training Loss: 0.0005466894430488847\n",
      "\n",
      "Epoch 300/500\n",
      "Epoch 300, Batch 41/41, Batch loss: 0.0008020575041882694\n",
      "Memory Usage: 480.64 MB\n",
      "Epoch 300 completed in 0.49 seconds, Total Training Loss: 0.00040159143934100186\n",
      "\n",
      "Epoch 301/500\n",
      "Epoch 301, Batch 41/41, Batch loss: 0.0008174197282642126\n",
      "Memory Usage: 309.05 MB\n",
      "Epoch 301 completed in 2.58 seconds, Total Training Loss: 0.0004082055756731368\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0016000350005924702\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 302/500\n",
      "Epoch 302, Batch 41/41, Batch loss: 0.0008185799233615398\n",
      "Memory Usage: 323.28 MB\n",
      "Epoch 302 completed in 0.53 seconds, Total Training Loss: 0.0005384868524809618\n",
      "\n",
      "Epoch 303/500\n",
      "Epoch 303, Batch 41/41, Batch loss: 0.0008996905526146293\n",
      "Memory Usage: 325.30 MB\n",
      "Epoch 303 completed in 0.51 seconds, Total Training Loss: 0.00041487565932784047\n",
      "\n",
      "Epoch 304/500\n",
      "Epoch 304, Batch 41/41, Batch loss: 0.0008257491863332689\n",
      "Memory Usage: 326.39 MB\n",
      "Epoch 304 completed in 0.43 seconds, Total Training Loss: 0.00041801618552687235\n",
      "\n",
      "Epoch 305/500\n",
      "Epoch 305, Batch 41/41, Batch loss: 0.0007984304684214294\n",
      "Memory Usage: 328.95 MB\n",
      "Epoch 305 completed in 0.45 seconds, Total Training Loss: 0.00042456790283833425\n",
      "\n",
      "Epoch 306/500\n",
      "Epoch 306, Batch 41/41, Batch loss: 0.0008003223920240998\n",
      "Memory Usage: 329.89 MB\n",
      "Epoch 306 completed in 0.43 seconds, Total Training Loss: 0.0005562288717511565\n",
      "\n",
      "Epoch 307/500\n",
      "Epoch 307, Batch 41/41, Batch loss: 0.000826930336188525\n",
      "Memory Usage: 331.22 MB\n",
      "Epoch 307 completed in 0.44 seconds, Total Training Loss: 0.00041206750960909853\n",
      "\n",
      "Epoch 308/500\n",
      "Epoch 308, Batch 41/41, Batch loss: 0.0008674003765918314\n",
      "Memory Usage: 331.52 MB\n",
      "Epoch 308 completed in 0.43 seconds, Total Training Loss: 0.00045094801245751316\n",
      "\n",
      "Epoch 309/500\n",
      "Epoch 309, Batch 41/41, Batch loss: 0.0007993513718247414\n",
      "Memory Usage: 333.05 MB\n",
      "Epoch 309 completed in 0.43 seconds, Total Training Loss: 0.00044342711682874344\n",
      "\n",
      "Epoch 310/500\n",
      "Epoch 310, Batch 41/41, Batch loss: 0.000779583293478936\n",
      "Memory Usage: 333.09 MB\n",
      "Epoch 310 completed in 0.43 seconds, Total Training Loss: 0.00041483302789501754\n",
      "\n",
      "Epoch 311/500\n",
      "Epoch 311, Batch 41/41, Batch loss: 0.0007822209154255688\n",
      "Memory Usage: 333.09 MB\n",
      "Epoch 311 completed in 0.43 seconds, Total Training Loss: 0.00039154223681756907\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0016327369026839732\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 312/500\n",
      "Epoch 312, Batch 41/41, Batch loss: 0.0008391277515329421\n",
      "Memory Usage: 338.27 MB\n",
      "Epoch 312 completed in 0.41 seconds, Total Training Loss: 0.0004899459919415233\n",
      "\n",
      "Epoch 313/500\n",
      "Epoch 313, Batch 41/41, Batch loss: 0.0008434563642367721\n",
      "Memory Usage: 338.62 MB\n",
      "Epoch 313 completed in 0.41 seconds, Total Training Loss: 0.00041517827847245624\n",
      "\n",
      "Epoch 314/500\n",
      "Epoch 314, Batch 41/41, Batch loss: 0.0007805437198840082\n",
      "Memory Usage: 338.66 MB\n",
      "Epoch 314 completed in 0.44 seconds, Total Training Loss: 0.0004051593735689161\n",
      "\n",
      "Epoch 315/500\n",
      "Epoch 315, Batch 41/41, Batch loss: 0.0007845304789952934\n",
      "Memory Usage: 340.67 MB\n",
      "Epoch 315 completed in 0.53 seconds, Total Training Loss: 0.0004072031671700931\n",
      "\n",
      "Epoch 316/500\n",
      "Epoch 316, Batch 41/41, Batch loss: 0.000806845142506063\n",
      "Memory Usage: 341.17 MB\n",
      "Epoch 316 completed in 0.48 seconds, Total Training Loss: 0.000520607992323831\n",
      "\n",
      "Epoch 317/500\n",
      "Epoch 317, Batch 41/41, Batch loss: 0.0008397818310186267\n",
      "Memory Usage: 341.17 MB\n",
      "Epoch 317 completed in 0.49 seconds, Total Training Loss: 0.0004115920608278318\n",
      "\n",
      "Epoch 318/500\n",
      "Epoch 318, Batch 41/41, Batch loss: 0.0007866783416830003\n",
      "Memory Usage: 341.17 MB\n",
      "Epoch 318 completed in 0.47 seconds, Total Training Loss: 0.00041580924295065016\n",
      "\n",
      "Epoch 319/500\n",
      "Epoch 319, Batch 41/41, Batch loss: 0.0007748809293843806\n",
      "Memory Usage: 344.14 MB\n",
      "Epoch 319 completed in 0.53 seconds, Total Training Loss: 0.0004035839147258141\n",
      "\n",
      "Epoch 320/500\n",
      "Epoch 320, Batch 41/41, Batch loss: 0.0008056231890805066\n",
      "Memory Usage: 347.44 MB\n",
      "Epoch 320 completed in 1.20 seconds, Total Training Loss: 0.0005111322406730501\n",
      "\n",
      "Epoch 321/500\n",
      "Epoch 321, Batch 41/41, Batch loss: 0.0008321020286530256\n",
      "Memory Usage: 347.94 MB\n",
      "Epoch 321 completed in 0.48 seconds, Total Training Loss: 0.0004101924954206546\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0017862865701317788\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 322/500\n",
      "Epoch 322, Batch 41/41, Batch loss: 0.000793009006883949\n",
      "Memory Usage: 348.67 MB\n",
      "Epoch 322 completed in 0.52 seconds, Total Training Loss: 0.00042891541546318543\n",
      "\n",
      "Epoch 323/500\n",
      "Epoch 323, Batch 41/41, Batch loss: 0.0007711768848821521\n",
      "Memory Usage: 349.50 MB\n",
      "Epoch 323 completed in 0.53 seconds, Total Training Loss: 0.00040231001167274194\n",
      "\n",
      "Epoch 324/500\n",
      "Epoch 324, Batch 41/41, Batch loss: 0.0007992418832145631\n",
      "Memory Usage: 351.45 MB\n",
      "Epoch 324 completed in 0.80 seconds, Total Training Loss: 0.0005130697103506863\n",
      "\n",
      "Epoch 325/500\n",
      "Epoch 325, Batch 41/41, Batch loss: 0.0008311174460686743\n",
      "Memory Usage: 302.27 MB\n",
      "Epoch 325 completed in 0.99 seconds, Total Training Loss: 0.00041635085405531997\n",
      "\n",
      "Epoch 326/500\n",
      "Epoch 326, Batch 41/41, Batch loss: 0.0007834525313228369\n",
      "Memory Usage: 262.27 MB\n",
      "Epoch 326 completed in 0.64 seconds, Total Training Loss: 0.0004381296012292207\n",
      "\n",
      "Epoch 327/500\n",
      "Epoch 327, Batch 41/41, Batch loss: 0.000763260992243886\n",
      "Memory Usage: 255.84 MB\n",
      "Epoch 327 completed in 0.50 seconds, Total Training Loss: 0.0003960517704665729\n",
      "\n",
      "Epoch 328/500\n",
      "Epoch 328, Batch 41/41, Batch loss: 0.0007956150220707059\n",
      "Memory Usage: 255.84 MB\n",
      "Epoch 328 completed in 0.47 seconds, Total Training Loss: 0.0004898182995952438\n",
      "\n",
      "Epoch 329/500\n",
      "Epoch 329, Batch 41/41, Batch loss: 0.0008016383508220315\n",
      "Memory Usage: 256.47 MB\n",
      "Epoch 329 completed in 0.42 seconds, Total Training Loss: 0.0004054228356392009\n",
      "\n",
      "Epoch 330/500\n",
      "Epoch 330, Batch 41/41, Batch loss: 0.0007738936692476273\n",
      "Memory Usage: 256.47 MB\n",
      "Epoch 330 completed in 0.40 seconds, Total Training Loss: 0.00042835049361001883\n",
      "\n",
      "Epoch 331/500\n",
      "Epoch 331, Batch 41/41, Batch loss: 0.0007563924300484359\n",
      "Memory Usage: 256.47 MB\n",
      "Epoch 331 completed in 0.43 seconds, Total Training Loss: 0.00039238120832412344\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.001622664136812091\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 332/500\n",
      "Epoch 332, Batch 41/41, Batch loss: 0.0007797413854859769\n",
      "Memory Usage: 256.59 MB\n",
      "Epoch 332 completed in 0.43 seconds, Total Training Loss: 0.00047844348290066303\n",
      "\n",
      "Epoch 333/500\n",
      "Epoch 333, Batch 41/41, Batch loss: 0.0007766165072098374\n",
      "Memory Usage: 257.58 MB\n",
      "Epoch 333 completed in 0.43 seconds, Total Training Loss: 0.00039636379813423335\n",
      "\n",
      "Epoch 334/500\n",
      "Epoch 334, Batch 41/41, Batch loss: 0.0007555778720416129\n",
      "Memory Usage: 258.69 MB\n",
      "Epoch 334 completed in 0.44 seconds, Total Training Loss: 0.0004171882779124511\n",
      "\n",
      "Epoch 335/500\n",
      "Epoch 335, Batch 41/41, Batch loss: 0.0007491152500733733\n",
      "Memory Usage: 259.61 MB\n",
      "Epoch 335 completed in 0.43 seconds, Total Training Loss: 0.00038616276589625447\n",
      "\n",
      "Epoch 336/500\n",
      "Epoch 336, Batch 41/41, Batch loss: 0.0007717194966971874\n",
      "Memory Usage: 261.44 MB\n",
      "Epoch 336 completed in 0.45 seconds, Total Training Loss: 0.0004598453458861402\n",
      "\n",
      "Epoch 337/500\n",
      "Epoch 337, Batch 41/41, Batch loss: 0.0007459946791641414\n",
      "Memory Usage: 261.44 MB\n",
      "Epoch 337 completed in 0.43 seconds, Total Training Loss: 0.0003976291821057136\n",
      "\n",
      "Epoch 338/500\n",
      "Epoch 338, Batch 41/41, Batch loss: 0.0007505128742195666\n",
      "Memory Usage: 261.45 MB\n",
      "Epoch 338 completed in 0.42 seconds, Total Training Loss: 0.00042297472295680695\n",
      "\n",
      "Epoch 339/500\n",
      "Epoch 339, Batch 41/41, Batch loss: 0.0007437211461365223\n",
      "Memory Usage: 261.45 MB\n",
      "Epoch 339 completed in 0.43 seconds, Total Training Loss: 0.0003867533420661192\n",
      "\n",
      "Epoch 340/500\n",
      "Epoch 340, Batch 41/41, Batch loss: 0.0007830678368918598\n",
      "Memory Usage: 262.08 MB\n",
      "Epoch 340 completed in 0.42 seconds, Total Training Loss: 0.00046839603441540243\n",
      "\n",
      "Epoch 341/500\n",
      "Epoch 341, Batch 41/41, Batch loss: 0.000753703759983182\n",
      "Memory Usage: 262.08 MB\n",
      "Epoch 341 completed in 0.42 seconds, Total Training Loss: 0.00040606078787610255\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.001701812306419015\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 342/500\n",
      "Epoch 342, Batch 41/41, Batch loss: 0.000756419321987778\n",
      "Memory Usage: 262.30 MB\n",
      "Epoch 342 completed in 0.44 seconds, Total Training Loss: 0.0004478300908841647\n",
      "\n",
      "Epoch 343/500\n",
      "Epoch 343, Batch 41/41, Batch loss: 0.0007461196510121226\n",
      "Memory Usage: 262.30 MB\n",
      "Epoch 343 completed in 0.43 seconds, Total Training Loss: 0.0003897952937688565\n",
      "\n",
      "Epoch 344/500\n",
      "Epoch 344, Batch 41/41, Batch loss: 0.0007557752542197704\n",
      "Memory Usage: 262.30 MB\n",
      "Epoch 344 completed in 0.45 seconds, Total Training Loss: 0.00045411253618445567\n",
      "\n",
      "Epoch 345/500\n",
      "Epoch 345, Batch 41/41, Batch loss: 0.0007374439155682921\n",
      "Memory Usage: 262.30 MB\n",
      "Epoch 345 completed in 0.42 seconds, Total Training Loss: 0.0003920051660715239\n",
      "\n",
      "Epoch 346/500\n",
      "Epoch 346, Batch 41/41, Batch loss: 0.0007414382416754961\n",
      "Memory Usage: 262.30 MB\n",
      "Epoch 346 completed in 0.44 seconds, Total Training Loss: 0.00043140852713116965\n",
      "\n",
      "Epoch 347/500\n",
      "Epoch 347, Batch 41/41, Batch loss: 0.0007289631757885218\n",
      "Memory Usage: 262.30 MB\n",
      "Epoch 347 completed in 0.41 seconds, Total Training Loss: 0.00038479169012223393\n",
      "\n",
      "Epoch 348/500\n",
      "Epoch 348, Batch 41/41, Batch loss: 0.0007600912940688431\n",
      "Memory Usage: 262.30 MB\n",
      "Epoch 348 completed in 0.50 seconds, Total Training Loss: 0.0004545867580795524\n",
      "\n",
      "Epoch 349/500\n",
      "Epoch 349, Batch 41/41, Batch loss: 0.0007450769771821797\n",
      "Memory Usage: 263.81 MB\n",
      "Epoch 349 completed in 0.45 seconds, Total Training Loss: 0.00039822470805630453\n",
      "\n",
      "Epoch 350/500\n",
      "Epoch 350, Batch 41/41, Batch loss: 0.0007433207938447595\n",
      "Memory Usage: 263.81 MB\n",
      "Epoch 350 completed in 0.44 seconds, Total Training Loss: 0.00044098999028281514\n",
      "\n",
      "Epoch 351/500\n",
      "Epoch 351, Batch 41/41, Batch loss: 0.0007311941008083522\n",
      "Memory Usage: 263.81 MB\n",
      "Epoch 351 completed in 0.46 seconds, Total Training Loss: 0.000390918201890422\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0017007482005283237\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 352/500\n",
      "Epoch 352, Batch 41/41, Batch loss: 0.0007512665470130742\n",
      "Memory Usage: 264.39 MB\n",
      "Epoch 352 completed in 0.45 seconds, Total Training Loss: 0.0004631401163528719\n",
      "\n",
      "Epoch 353/500\n",
      "Epoch 353, Batch 41/41, Batch loss: 0.000750678707845509\n",
      "Memory Usage: 264.39 MB\n",
      "Epoch 353 completed in 0.41 seconds, Total Training Loss: 0.0004012902455742289\n",
      "\n",
      "Epoch 354/500\n",
      "Epoch 354, Batch 41/41, Batch loss: 0.0007361809257417917\n",
      "Memory Usage: 264.42 MB\n",
      "Epoch 354 completed in 0.43 seconds, Total Training Loss: 0.00043659454347173935\n",
      "\n",
      "Epoch 355/500\n",
      "Epoch 355, Batch 41/41, Batch loss: 0.0007330072112381458\n",
      "Memory Usage: 265.23 MB\n",
      "Epoch 355 completed in 0.44 seconds, Total Training Loss: 0.00039402972771956517\n",
      "\n",
      "Epoch 356/500\n",
      "Epoch 356, Batch 41/41, Batch loss: 0.0007495509926229715\n",
      "Memory Usage: 265.23 MB\n",
      "Epoch 356 completed in 0.46 seconds, Total Training Loss: 0.00047458450735132096\n",
      "\n",
      "Epoch 357/500\n",
      "Epoch 357, Batch 41/41, Batch loss: 0.0007488328265026212\n",
      "Memory Usage: 265.23 MB\n",
      "Epoch 357 completed in 0.44 seconds, Total Training Loss: 0.00039737954329012145\n",
      "\n",
      "Epoch 358/500\n",
      "Epoch 358, Batch 41/41, Batch loss: 0.0007355935522355139\n",
      "Memory Usage: 265.23 MB\n",
      "Epoch 358 completed in 0.43 seconds, Total Training Loss: 0.00043150318130600923\n",
      "\n",
      "Epoch 359/500\n",
      "Epoch 359, Batch 41/41, Batch loss: 0.0007283866289071739\n",
      "Memory Usage: 265.23 MB\n",
      "Epoch 359 completed in 0.45 seconds, Total Training Loss: 0.00038911397930904766\n",
      "\n",
      "Epoch 360/500\n",
      "Epoch 360, Batch 41/41, Batch loss: 0.0007378475274890661\n",
      "Memory Usage: 265.23 MB\n",
      "Epoch 360 completed in 0.42 seconds, Total Training Loss: 0.00046729352825644946\n",
      "\n",
      "Epoch 361/500\n",
      "Epoch 361, Batch 41/41, Batch loss: 0.000725087127648294\n",
      "Memory Usage: 265.23 MB\n",
      "Epoch 361 completed in 0.47 seconds, Total Training Loss: 0.00038295930177654797\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.001710132835432887\n",
      "No improvement in model.\n",
      "\n",
      "Epoch 362/500\n",
      "Epoch 362, Batch 41/41, Batch loss: 0.0007175248465500772\n",
      "Memory Usage: 265.25 MB\n",
      "Epoch 362 completed in 0.46 seconds, Total Training Loss: 0.0004182373164306826\n",
      "\n",
      "Epoch 363/500\n",
      "Epoch 363, Batch 41/41, Batch loss: 0.0007121011731214821\n",
      "Memory Usage: 265.25 MB\n",
      "Epoch 363 completed in 0.42 seconds, Total Training Loss: 0.00037257702135276506\n",
      "\n",
      "Epoch 364/500\n",
      "Epoch 364, Batch 41/41, Batch loss: 0.0007186434231698513\n",
      "Memory Usage: 265.25 MB\n",
      "Epoch 364 completed in 0.99 seconds, Total Training Loss: 0.00042606208533071347\n",
      "\n",
      "Epoch 365/500\n",
      "Epoch 365, Batch 41/41, Batch loss: 0.0007069860585033894\n",
      "Memory Usage: 265.25 MB\n",
      "Epoch 365 completed in 0.51 seconds, Total Training Loss: 0.0003717894161548806\n",
      "\n",
      "Epoch 366/500\n",
      "Epoch 366, Batch 41/41, Batch loss: 0.0007098993519321084\n",
      "Memory Usage: 265.25 MB\n",
      "Epoch 366 completed in 1.01 seconds, Total Training Loss: 0.0004089706736721848\n",
      "\n",
      "Epoch 367/500\n",
      "Epoch 367, Batch 41/41, Batch loss: 0.0007021395140327513\n",
      "Memory Usage: 265.28 MB\n",
      "Epoch 367 completed in 0.94 seconds, Total Training Loss: 0.0003790948914417929\n",
      "\n",
      "Epoch 368/500\n",
      "Epoch 368, Batch 41/41, Batch loss: 0.0007376227877102792\n",
      "Memory Usage: 265.28 MB\n",
      "Epoch 368 completed in 0.47 seconds, Total Training Loss: 0.0004511724872849654\n",
      "\n",
      "Epoch 369/500\n",
      "Epoch 369, Batch 41/41, Batch loss: 0.0007050153217278421\n",
      "Memory Usage: 265.30 MB\n",
      "Epoch 369 completed in 0.43 seconds, Total Training Loss: 0.0003875208533760842\n",
      "\n",
      "Epoch 370/500\n",
      "Epoch 370, Batch 41/41, Batch loss: 0.0007118973298929632\n",
      "Memory Usage: 265.30 MB\n",
      "Epoch 370 completed in 0.41 seconds, Total Training Loss: 0.0004246413662302785\n",
      "\n",
      "Epoch 371/500\n",
      "Epoch 371, Batch 41/41, Batch loss: 0.0006994826253503561\n",
      "Memory Usage: 266.53 MB\n",
      "Epoch 371 completed in 0.44 seconds, Total Training Loss: 0.0003775391303905763\n",
      "Validation completed in 0.02 seconds, Average Validation Loss: 0.0017820132430642842\n",
      "No improvement in model.\n",
      "Stopping early at epoch 371 due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import psutil  # To monitor system resources\n",
    "\n",
    "model = NBeatsNet(input_size=X_train_combined.shape[1], forecast_length=1, layer_size=128)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()\n",
    "patience = 10\n",
    "epochs = 500\n",
    "batch_size = 1000\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "total_batches = (X_train_tensor.size(0) + batch_size - 1) // batch_size  # Total number of batches\n",
    "print(\"Starting the model training\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "        batch_num = i // batch_size + 1\n",
    "        X_batch = X_train_tensor[i:i + batch_size]\n",
    "        y_batch = y_train_tensor[i:i + batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_num % 100 == 0 or batch_num == total_batches:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_num}/{total_batches}, Batch loss: {loss.item()}\")\n",
    "            print(f\"Memory Usage: {psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1} completed in {epoch_time:.2f} seconds, Total Training Loss: {total_loss / total_batches}\")\n",
    "\n",
    "    if epoch % 10 == 0 or epochs_no_improve >= patience:\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_start_time = time.time()\n",
    "\n",
    "        for i in range(0, X_val_tensor.size(0), batch_size):\n",
    "            X_val_batch = X_val_tensor[i:i + batch_size]\n",
    "            y_val_batch = y_val_tensor[i:i + batch_size]\n",
    "\n",
    "            val_output = model(X_val_batch)\n",
    "            val_loss = criterion(val_output, y_val_batch)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        val_time = time.time() - val_start_time\n",
    "        print(f'Validation completed in {val_time:.2f} seconds, Average Validation Loss: {avg_val_loss}')\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the model checkpoint\n",
    "            torch.save(model.state_dict(), 'model_checkpoint.pth')\n",
    "            print(\"Model improved and saved.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(\"No improvement in model.\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'Stopping early at epoch {epoch+1} due to no improvement in validation loss.')\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:14:09.659196Z",
     "start_time": "2024-06-06T11:11:16.797290Z"
    }
   },
   "id": "489c766a8e105c83",
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitcoin_Price: RMSE=0.10038602352142334, MAE=0.07804664224386215, MAPE=44.64976489543915%\n",
      "ethereum_Price: RMSE=0.09456729143857956, MAE=0.06419152021408081, MAPE=117.66083240509033%\n",
      "avax_Price: RMSE=0.04713231697678566, MAE=0.030489366501569748, MAPE=4.953066259622574%\n",
      "binance_Price: RMSE=0.10522855818271637, MAE=0.07513738423585892, MAPE=354.55596446990967%\n",
      "doge_Price: RMSE=0.13388843834400177, MAE=0.07945636659860611, MAPE=94.41903233528137%\n",
      "cardano_Price: RMSE=0.0835476741194725, MAE=0.05828559026122093, MAPE=138.3745789527893%\n",
      "polkadot_Price: RMSE=0.11017443239688873, MAE=0.07918719947338104, MAPE=75.85225701332092%\n",
      "ripple_Price: RMSE=0.12305860221385956, MAE=0.08177193999290466, MAPE=92.7228331565857%\n",
      "solana_Price: RMSE=0.025387097150087357, MAE=0.018805069848895073, MAPE=2.430073916912079%\n",
      "tron_Price: RMSE=0.11465929448604584, MAE=0.07504935562610626, MAPE=22.519634664058685%\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "model.eval()  # Switch to evaluation mode\n",
    "\n",
    "for idx, company in enumerate(df.columns):\n",
    "    # Find indices for the current company in the test dataset\n",
    "    company_test_indices = labels_test_combined == idx\n",
    "    if not np.any(company_test_indices):\n",
    "        print(f\"Skipping {company} due to no test data.\")\n",
    "        continue\n",
    "\n",
    "    X_test_company = X_test_combined[company_test_indices]\n",
    "    y_test_company = y_test_combined[company_test_indices]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_test_tensor = torch.tensor(X_test_company, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test_company, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test_tensor)\n",
    "        test_loss = criterion(test_output, y_test_tensor.view(-1, 1))\n",
    "        rmse = torch.sqrt(test_loss)\n",
    "\n",
    "        # Convert tensors to numpy arrays for MAE and MAPE calculations\n",
    "        test_output_np = test_output.numpy()\n",
    "        y_test_np = y_test_tensor.numpy()\n",
    "\n",
    "        mae = np.mean(np.abs(test_output_np - y_test_np))\n",
    "        mape = np.mean(np.abs((test_output_np - y_test_np) / y_test_np)) * 100 if np.any(y_test_np) else float('nan')\n",
    "\n",
    "    print(f\"{company}: RMSE={rmse.item()}, MAE={mae}, MAPE={mape}%\")\n",
    "    results.append({'Company': company, 'RMSE': rmse.item(), 'MAE': mae, 'MAPE': mape})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:14:20.372270Z",
     "start_time": "2024-06-06T11:14:20.357693Z"
    }
   },
   "id": "9febf34ba36919a9",
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, columns=['Company Name', 'RMSE', 'MAE', 'MAPE'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:14:20.932083Z",
     "start_time": "2024-06-06T11:14:20.926473Z"
    }
   },
   "id": "78f0bdcf56c64c1d",
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract MAPE values from results list\n",
    "mapes = [result['MAPE'] for result in results]\n",
    "\n",
    "# Define ranges\n",
    "ranges = [(0, 10), (10, 25), (25, 60), (60, 100), (100,1000), (1000, float('inf'))]\n",
    "\n",
    "# Initialize count for each range\n",
    "counts = [0] * len(ranges)\n",
    "\n",
    "# Count the number of data points in each range\n",
    "for mape in mapes:\n",
    "    for i, (start, end) in enumerate(ranges):\n",
    "        if start <= mape < end:\n",
    "            counts[i] += 1\n",
    "            break\n",
    "    else:\n",
    "        counts[-1] += 1  # For values above 100\n",
    "\n",
    "# Define range labels\n",
    "range_labels = ['0-10', '10-25', '25-60', '60-100', '100-1000', '1000+']\n",
    "\n",
    "# Plotting bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(ranges)), counts, color='skyblue', edgecolor='black')\n",
    "plt.xticks(range(len(ranges)), range_labels)\n",
    "plt.title('Distribution of MAPE Values')\n",
    "plt.xlabel('MAPE Range')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:14:21.393080Z",
     "start_time": "2024-06-06T11:14:21.324480Z"
    }
   },
   "id": "996a5197c82889e3",
   "execution_count": 97
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "results_df.to_excel('/Users/keremsmacbook/DataspellProjects/MultivariateForecasting/OutputData/Crypto_NBeats_V0.2.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T11:14:55.502521Z",
     "start_time": "2024-06-06T11:14:55.214418Z"
    }
   },
   "id": "8144fda1fd083729",
   "execution_count": 98
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "99de23e6f0d0ec72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
